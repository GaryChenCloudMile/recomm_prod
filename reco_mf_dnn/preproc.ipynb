{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "from pprint import pprint\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath('..')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.decode_csv + tf.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./movielens.tr', names=loader.schema.cols)\n",
    "vl = pd.read_csv('./movielens.vl', names=loader.schema.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69717, 30287)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr), len(vl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Read JSON Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to parse ./movielens.yaml (user supplied) ...\n",
      "try to transform ./tr.raw.movielens.csv ... \n",
      "[./tr.raw.movielens.csv]: process take time 0:00:38.276189\n",
      "try to transform ./te.raw.movielens.csv ... \n",
      "[./te.raw.movielens.csv]: process take time 0:00:16.452941\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "utils = reload('utils.utils')\n",
    "reco = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "loader = reco.Loader(conf_path='./movielens.yaml',\n",
    "                     parsed_conf_path='./parsed.yaml',\n",
    "                     raw_paths=['./tr.raw.movielens.csv', './te.raw.movielens.csv'])\n",
    "loader.tansform('./tr.raw.movielens.csv', './movielens.tr', chunksize=20000, reset=True)\\\n",
    "      .tansform('./te.raw.movielens.csv', './movielens.vl', chunksize=20000, reset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## MF + DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('query_movie_ids',\n",
       "              CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                    name='query_movie_ids', sep=',', vocabs=None,\n",
       "                    vocabs_path='item.vocab')),\n",
       "             ('genres',\n",
       "              CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                    sep='|', vocabs=None, vocabs_path='genres.vocab')),\n",
       "             ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "             ('year', NumericMapper(default=None, name='year')),\n",
       "             ('candidate_movie_id',\n",
       "              CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                    name='candidate_movie_id', sep=None, vocabs=None,\n",
       "                    vocabs_path='item.vocab')),\n",
       "             ('rating',\n",
       "              CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                    sep=None, vocabs=None, vocabs_path=None))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = reload('utils.utils')\n",
    "reco = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "\n",
    "schema = reco.Schema(conf_path='./movielens.yaml',\n",
    "                     parsed_conf_path='./parsed.yaml',\n",
    "                     raw_paths=['./tr.raw.movielens.csv', './te.raw.movielens.csv']).init()\n",
    "schema.col_states_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>m_dtype</th>\n",
       "      <th>date_format</th>\n",
       "      <th>default</th>\n",
       "      <th>is_multi</th>\n",
       "      <th>sep</th>\n",
       "      <th>vocabs</th>\n",
       "      <th>vocabs_path</th>\n",
       "      <th>aux</th>\n",
       "      <th>type</th>\n",
       "      <th>col_state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>user_id</td>\n",
       "      <td>catg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_movie_ids</th>\n",
       "      <td>query_movie_ids</td>\n",
       "      <td>catg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "      <td>item.vocab</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>allow_null: true\\nclasses_: ['1', '2', '3', '4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genres</th>\n",
       "      <td>genres</td>\n",
       "      <td>catg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>|</td>\n",
       "      <td>None</td>\n",
       "      <td>genres.vocab</td>\n",
       "      <td>False</td>\n",
       "      <td>item</td>\n",
       "      <td>allow_null: true\\nclasses_: [(no genres listed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_rating</th>\n",
       "      <td>avg_rating</td>\n",
       "      <td>cont</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>item</td>\n",
       "      <td>{cumsum_: 354374.99999999965, default: null, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>year</td>\n",
       "      <td>cont</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>item</td>\n",
       "      <td>{cumsum_: 199176755.0, default: null, max_: 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <td>candidate_movie_id</td>\n",
       "      <td>catg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>item.vocab</td>\n",
       "      <td>False</td>\n",
       "      <td>item</td>\n",
       "      <td>allow_null: true\\nclasses_: ['1', '2', '3', '4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>datetime</td>\n",
       "      <td>%Y-%m-%d %H:%M:%S</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>rating</td>\n",
       "      <td>catg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>label</td>\n",
       "      <td>allow_null: false\\nclasses_: ['0', '1']\\ndefau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id   m_dtype        date_format default is_multi   sep vocabs  \\\n",
       "id                                                                                                  \n",
       "user_id                        user_id      catg               None    None    False  None   None   \n",
       "query_movie_ids        query_movie_ids      catg               None    None     True     ,   None   \n",
       "genres                          genres      catg               None    None     True     |   None   \n",
       "avg_rating                  avg_rating      cont               None    None    False  None   None   \n",
       "year                              year      cont               None    None    False  None   None   \n",
       "candidate_movie_id  candidate_movie_id      catg               None    None    False  None   None   \n",
       "timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None    False  None   None   \n",
       "rating                          rating      catg               None    None    False  None   None   \n",
       "\n",
       "                     vocabs_path    aux   type                                          col_state  \n",
       "id                                                                                                 \n",
       "user_id                     None  False   None                                               None  \n",
       "query_movie_ids       item.vocab  False   user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       "genres              genres.vocab  False   item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       "avg_rating                  None  False   item  {cumsum_: 354374.99999999965, default: null, m...  \n",
       "year                        None  False   item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       "candidate_movie_id    item.vocab  False   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       "timestamp                   None  False   None                                               None  \n",
       "rating                      None  False  label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.df_conf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/reco_mf_dnn\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_is_chief': True, '_model_dir': './model/reco_mf_dnn', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000000021D01C18>, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_tf_random_seed': 88, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_task_id': 0, '_task_type': 'worker', '_master': '', '_save_checkpoints_steps': 545, '_session_config': None}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.5711223\n",
      "INFO:tensorflow:global_step/sec: 71.9901\n",
      "INFO:tensorflow:step = 101, loss = 1.0577768 (1.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.227\n",
      "INFO:tensorflow:step = 201, loss = 1.53901 (0.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3623\n",
      "INFO:tensorflow:step = 301, loss = 0.69855785 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.244\n",
      "INFO:tensorflow:step = 401, loss = 0.63636434 (0.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.6\n",
      "INFO:tensorflow:step = 501, loss = 0.6289816 (0.865 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 546 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.6766\n",
      "INFO:tensorflow:step = 601, loss = 0.9026178 (5.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7056\n",
      "INFO:tensorflow:step = 701, loss = 0.5915097 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.089\n",
      "INFO:tensorflow:step = 801, loss = 0.71379364 (0.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.981\n",
      "INFO:tensorflow:step = 901, loss = 0.659325 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.601\n",
      "INFO:tensorflow:step = 1001, loss = 0.38056087 (0.897 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1091 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.5346\n",
      "INFO:tensorflow:step = 1101, loss = 0.69056183 (5.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.571\n",
      "INFO:tensorflow:step = 1201, loss = 0.4763707 (0.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.058\n",
      "INFO:tensorflow:step = 1301, loss = 0.5007939 (0.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.918\n",
      "INFO:tensorflow:step = 1401, loss = 0.75323683 (0.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.467\n",
      "INFO:tensorflow:step = 1501, loss = 0.57218 (0.837 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.403\n",
      "INFO:tensorflow:step = 1601, loss = 0.66859674 (0.914 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1636 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.9491\n",
      "INFO:tensorflow:step = 1701, loss = 0.5523828 (5.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.689\n",
      "INFO:tensorflow:step = 1801, loss = 0.63991666 (0.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.476\n",
      "INFO:tensorflow:step = 1901, loss = 0.5458547 (0.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.352\n",
      "INFO:tensorflow:step = 2001, loss = 0.76072073 (0.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.126\n",
      "INFO:tensorflow:step = 2101, loss = 0.68498874 (0.912 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2181 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 16.3016\n",
      "INFO:tensorflow:step = 2201, loss = 0.53930205 (6.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9315\n",
      "INFO:tensorflow:step = 2301, loss = 1.1860671 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.8166\n",
      "INFO:tensorflow:step = 2401, loss = 0.12236534 (1.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.197\n",
      "INFO:tensorflow:step = 2501, loss = 0.547717 (0.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.804\n",
      "INFO:tensorflow:step = 2601, loss = 0.53970754 (0.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3604\n",
      "INFO:tensorflow:step = 2701, loss = 0.39873067 (1.118 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2726 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.4084\n",
      "INFO:tensorflow:step = 2801, loss = 0.68201876 (5.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.766\n",
      "INFO:tensorflow:step = 2901, loss = 0.61848724 (0.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.868\n",
      "INFO:tensorflow:step = 3001, loss = 0.621135 (0.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.396\n",
      "INFO:tensorflow:step = 3101, loss = 0.16933735 (0.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.981\n",
      "INFO:tensorflow:step = 3201, loss = 0.6734189 (0.902 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3271 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.0105\n",
      "INFO:tensorflow:step = 3301, loss = 0.28073353 (5.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.613\n",
      "INFO:tensorflow:step = 3401, loss = 0.38971382 (0.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5244\n",
      "INFO:tensorflow:step = 3501, loss = 0.77822345 (1.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.763\n",
      "INFO:tensorflow:step = 3601, loss = 0.90063715 (0.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.126\n",
      "INFO:tensorflow:step = 3701, loss = 1.3948071 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.7416\n",
      "INFO:tensorflow:step = 3801, loss = 0.28424436 (1.304 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3816 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.0887\n",
      "INFO:tensorflow:step = 3901, loss = 0.5802001 (5.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.353\n",
      "INFO:tensorflow:step = 4001, loss = 0.35444236 (0.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9015\n",
      "INFO:tensorflow:step = 4101, loss = 0.69065714 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.137\n",
      "INFO:tensorflow:step = 4201, loss = 0.16751274 (0.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.041\n",
      "INFO:tensorflow:step = 4301, loss = 0.51198286 (0.832 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4361 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.7712\n",
      "INFO:tensorflow:step = 4401, loss = 0.6392863 (5.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0984\n",
      "INFO:tensorflow:step = 4501, loss = 0.29921696 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3605\n",
      "INFO:tensorflow:step = 4601, loss = 0.40148035 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.283\n",
      "INFO:tensorflow:step = 4701, loss = 0.48821294 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.372\n",
      "INFO:tensorflow:step = 4801, loss = 0.65191334 (0.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8595\n",
      "INFO:tensorflow:step = 4901, loss = 0.5917644 (1.301 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4906 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.4935\n",
      "INFO:tensorflow:step = 5001, loss = 0.18731038 (5.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.247\n",
      "INFO:tensorflow:step = 5101, loss = 0.5707618 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.952\n",
      "INFO:tensorflow:step = 5201, loss = 0.7314781 (0.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.889\n",
      "INFO:tensorflow:step = 5301, loss = 0.41812885 (0.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.364\n",
      "INFO:tensorflow:step = 5401, loss = 0.62429535 (0.852 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5450 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5740137.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-08-14:03:06\n",
      "INFO:tensorflow:Restoring parameters from ./model/reco_mf_dnn\\model.ckpt-5450\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-08-14:03:08\n",
      "INFO:tensorflow:Saving dict for global step 5450: global_step = 5450, loss = 0.6803791\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./model/reco_mf_dnn\\model.ckpt-5450\n",
      "INFO:tensorflow:Saving checkpoints for 5451 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:step = 5451, loss = 0.42578667\n",
      "INFO:tensorflow:global_step/sec: 104.816\n",
      "INFO:tensorflow:step = 5551, loss = 0.5003849 (0.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.336\n",
      "INFO:tensorflow:step = 5651, loss = 0.05938237 (0.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.295\n",
      "INFO:tensorflow:step = 5751, loss = 0.6737044 (0.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.068\n",
      "INFO:tensorflow:step = 5851, loss = 8.246668 (0.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.6\n",
      "INFO:tensorflow:step = 5951, loss = 0.9063791 (0.865 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5996 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.7995\n",
      "INFO:tensorflow:step = 6051, loss = 0.8381015 (5.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.946\n",
      "INFO:tensorflow:step = 6151, loss = 0.78321886 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.467\n",
      "INFO:tensorflow:step = 6251, loss = 0.6418671 (0.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.392\n",
      "INFO:tensorflow:step = 6351, loss = 0.5686705 (0.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.279\n",
      "INFO:tensorflow:step = 6451, loss = 0.44610527 (0.875 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6541 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.569\n",
      "INFO:tensorflow:step = 6551, loss = 0.47562563 (5.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.279\n",
      "INFO:tensorflow:step = 6651, loss = 0.64866894 (0.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.197\n",
      "INFO:tensorflow:step = 6751, loss = 0.5036558 (0.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.352\n",
      "INFO:tensorflow:step = 6851, loss = 1.4614539 (0.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.61\n",
      "INFO:tensorflow:step = 6951, loss = 0.5829401 (0.837 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.1272\n",
      "INFO:tensorflow:step = 7051, loss = 0.6586213 (1.331 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7086 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 17.2285\n",
      "INFO:tensorflow:step = 7151, loss = 0.96388614 (5.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.816\n",
      "INFO:tensorflow:step = 7251, loss = 0.6473571 (0.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.759\n",
      "INFO:tensorflow:step = 7351, loss = 0.31312633 (0.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.272\n",
      "INFO:tensorflow:step = 7451, loss = 0.7910564 (0.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.733\n",
      "INFO:tensorflow:step = 7551, loss = 0.5588624 (0.888 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7631 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.0008\n",
      "INFO:tensorflow:step = 7651, loss = 0.5394144 (5.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.151\n",
      "INFO:tensorflow:step = 7751, loss = 0.71159786 (0.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.6\n",
      "INFO:tensorflow:step = 7851, loss = 0.027768206 (0.865 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.197\n",
      "INFO:tensorflow:step = 7951, loss = 0.5055089 (0.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0595\n",
      "INFO:tensorflow:step = 8051, loss = 0.5031488 (1.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.348\n",
      "INFO:tensorflow:step = 8151, loss = 0.93112856 (0.978 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8176 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.6417\n",
      "INFO:tensorflow:step = 8251, loss = 0.61617166 (5.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.912\n",
      "INFO:tensorflow:step = 8351, loss = 0.6845438 (0.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.185\n",
      "INFO:tensorflow:step = 8451, loss = 0.62456715 (0.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.228\n",
      "INFO:tensorflow:step = 8551, loss = 0.08711268 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.479\n",
      "INFO:tensorflow:step = 8651, loss = 0.5074189 (0.947 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8721 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.405\n",
      "INFO:tensorflow:step = 8751, loss = 0.39453512 (5.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.9258\n",
      "INFO:tensorflow:step = 8851, loss = 0.28058657 (1.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8039\n",
      "INFO:tensorflow:step = 8951, loss = 0.48063013 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.86\n",
      "INFO:tensorflow:step = 9051, loss = 0.7913381 (0.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.325\n",
      "INFO:tensorflow:step = 9151, loss = 1.051392 (0.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.897\n",
      "INFO:tensorflow:step = 9251, loss = 0.70226526 (0.834 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9266 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.729\n",
      "INFO:tensorflow:step = 9351, loss = 0.64212966 (5.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.526\n",
      "INFO:tensorflow:step = 9451, loss = 0.3199551 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.981\n",
      "INFO:tensorflow:step = 9551, loss = 0.700945 (0.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.491\n",
      "INFO:tensorflow:step = 9651, loss = 0.4176485 (0.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3167\n",
      "INFO:tensorflow:step = 9751, loss = 0.41644484 (1.245 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9811 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.4628\n",
      "INFO:tensorflow:step = 9851, loss = 0.7039177 (5.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.918\n",
      "INFO:tensorflow:step = 9951, loss = 0.13408059 (0.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step = 10051, loss = 0.09192287 (0.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.672\n",
      "INFO:tensorflow:step = 10151, loss = 0.49231997 (0.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.467\n",
      "INFO:tensorflow:step = 10251, loss = 0.56248546 (0.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5614\n",
      "INFO:tensorflow:step = 10351, loss = 0.69551355 (1.225 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10356 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.4764\n",
      "INFO:tensorflow:step = 10451, loss = 0.09225416 (5.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.988\n",
      "INFO:tensorflow:step = 10551, loss = 0.95989263 (0.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1236\n",
      "INFO:tensorflow:step = 10651, loss = 0.91996026 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.228\n",
      "INFO:tensorflow:step = 10751, loss = 0.42750683 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.672\n",
      "INFO:tensorflow:step = 10851, loss = 0.7104304 (0.871 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10900 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.49342775.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-08-14:04:53\n",
      "INFO:tensorflow:Restoring parameters from ./model/reco_mf_dnn\\model.ckpt-10900\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-08-14:04:55\n",
      "INFO:tensorflow:Saving dict for global step 10900: global_step = 10900, loss = 0.80151045\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./model/reco_mf_dnn\\model.ckpt-10900\n",
      "INFO:tensorflow:Saving checkpoints for 10901 into ./model/reco_mf_dnn\\model.ckpt.\n",
      "INFO:tensorflow:step = 10901, loss = 0.994957\n",
      "INFO:tensorflow:global_step/sec: 105.368\n",
      "INFO:tensorflow:step = 11001, loss = 0.65220565 (0.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.899\n",
      "INFO:tensorflow:step = 11101, loss = 0.053582154 (0.841 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-88f3956c130b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mreco_mf_dnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reco_mf_dnn.reco_mf_dnn_flex_shema'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreco_mf_dnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelMfDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_genres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mH:\\workspace\\notebook\\recomm_prod\\reco_mf_dnn\\reco_mf_dnn_flex_shema.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_input, valid_input, reset)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[0;32m    428\u001b[0m       config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[0;32m    429\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m     \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mrun_local\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m           hooks=train_hooks)\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m       \u001b[1;31m# Final export signal: For any eval result with global_step >= train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m           \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    893\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "utils = reload('utils.utils')\n",
    "reco = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "\n",
    "model_dir='./model/reco_mf_dnn'\n",
    "dim = 16\n",
    "lr = 0.005\n",
    "n_batch = 128\n",
    "n_movies = len(schema.col_states_['candidate_movie_id'].classes_)\n",
    "n_genres = len(schema.col_states_['genres'].classes_)\n",
    "\n",
    "reco_mf_dnn = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "model = reco_mf_dnn.ModelMfDNN(schema, n_movies, n_genres, dim=dim, model_dir=model_dir)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(30287 // 128) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding_size', 8), ('eval_batch_size', 40), ('eval_files', ['../data/adult.test.csv']), ('eval_steps', 100), ('export_format', 'JSON'), ('first_layer_size', 100), ('job_dir', 'census_model'), ('learning_rate', 0.1), ('num_epochs', None), ('num_layers', 4), ('scale_factor', 0.7), ('train_batch_size', 40), ('train_files', ['../data/adult.data.csv']), ('train_steps', 1000), ('verbosity', 'INFO')]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\envs\\ml_dl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n",
      "INFO:tensorflow:Using default config.\r\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_master': '', '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001D217DA0>, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_model_dir': 'census_model', '_save_summary_steps': 100, '_service': None, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600}\r\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n",
      "WARNING:tensorflow:From H:\\workspace\\notebook\\tensorflow_estimator\\cloudml-samples\\census\\customestimator\\trainer\\model.py:193: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please switch to tf.train.get_or_create_global_step\r\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\r\n",
      "INFO:tensorflow:Saving checkpoints for 1 into census_model\\model.ckpt.\r\n",
      "INFO:tensorflow:loss = 34.8261, step = 1\r\n",
      "INFO:tensorflow:global_step/sec: 87.103\r\n",
      "INFO:tensorflow:loss = 0.66805506, step = 101 (1.149 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.5016\r\n",
      "INFO:tensorflow:loss = 0.56469536, step = 201 (1.081 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.2457\r\n",
      "INFO:tensorflow:loss = 0.5067726, step = 301 (1.084 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.8452\r\n",
      "INFO:tensorflow:loss = 0.6182569, step = 401 (1.077 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 93.1045\r\n",
      "INFO:tensorflow:loss = 0.53060496, step = 501 (1.074 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.9315\r\n",
      "INFO:tensorflow:loss = 0.6219691, step = 601 (1.076 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.1606\r\n",
      "INFO:tensorflow:loss = 0.47979227, step = 701 (1.085 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.3308\r\n",
      "INFO:tensorflow:loss = 0.53743964, step = 801 (1.083 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 92.7591\r\n",
      "INFO:tensorflow:loss = 0.47474313, step = 901 (1.078 sec)\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into census_model\\model.ckpt.\r\n",
      "INFO:tensorflow:Loss for final step: 0.57198966.\r\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-08-13:28:53\r\n",
      "INFO:tensorflow:Restoring parameters from census_model\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Evaluation [1/100]\r\n",
      "INFO:tensorflow:Evaluation [2/100]\r\n",
      "INFO:tensorflow:Evaluation [3/100]\r\n",
      "INFO:tensorflow:Evaluation [4/100]\r\n",
      "INFO:tensorflow:Evaluation [5/100]\r\n",
      "INFO:tensorflow:Evaluation [6/100]\r\n",
      "INFO:tensorflow:Evaluation [7/100]\r\n",
      "INFO:tensorflow:Evaluation [8/100]\r\n",
      "INFO:tensorflow:Evaluation [9/100]\r\n",
      "INFO:tensorflow:Evaluation [10/100]\r\n",
      "INFO:tensorflow:Evaluation [11/100]\r\n",
      "INFO:tensorflow:Evaluation [12/100]\r\n",
      "INFO:tensorflow:Evaluation [13/100]\r\n",
      "INFO:tensorflow:Evaluation [14/100]\r\n",
      "INFO:tensorflow:Evaluation [15/100]\r\n",
      "INFO:tensorflow:Evaluation [16/100]\r\n",
      "INFO:tensorflow:Evaluation [17/100]\r\n",
      "INFO:tensorflow:Evaluation [18/100]\r\n",
      "INFO:tensorflow:Evaluation [19/100]\r\n",
      "INFO:tensorflow:Evaluation [20/100]\r\n",
      "INFO:tensorflow:Evaluation [21/100]\r\n",
      "INFO:tensorflow:Evaluation [22/100]\r\n",
      "INFO:tensorflow:Evaluation [23/100]\r\n",
      "INFO:tensorflow:Evaluation [24/100]\r\n",
      "INFO:tensorflow:Evaluation [25/100]\r\n",
      "INFO:tensorflow:Evaluation [26/100]\r\n",
      "INFO:tensorflow:Evaluation [27/100]\r\n",
      "INFO:tensorflow:Evaluation [28/100]\r\n",
      "INFO:tensorflow:Evaluation [29/100]\r\n",
      "INFO:tensorflow:Evaluation [30/100]\r\n",
      "INFO:tensorflow:Evaluation [31/100]\r\n",
      "INFO:tensorflow:Evaluation [32/100]\r\n",
      "INFO:tensorflow:Evaluation [33/100]\r\n",
      "INFO:tensorflow:Evaluation [34/100]\r\n",
      "INFO:tensorflow:Evaluation [35/100]\r\n",
      "INFO:tensorflow:Evaluation [36/100]\r\n",
      "INFO:tensorflow:Evaluation [37/100]\r\n",
      "INFO:tensorflow:Evaluation [38/100]\r\n",
      "INFO:tensorflow:Evaluation [39/100]\r\n",
      "INFO:tensorflow:Evaluation [40/100]\r\n",
      "INFO:tensorflow:Evaluation [41/100]\r\n",
      "INFO:tensorflow:Evaluation [42/100]\r\n",
      "INFO:tensorflow:Evaluation [43/100]\r\n",
      "INFO:tensorflow:Evaluation [44/100]\r\n",
      "INFO:tensorflow:Evaluation [45/100]\r\n",
      "INFO:tensorflow:Evaluation [46/100]\r\n",
      "INFO:tensorflow:Evaluation [47/100]\r\n",
      "INFO:tensorflow:Evaluation [48/100]\r\n",
      "INFO:tensorflow:Evaluation [49/100]\r\n",
      "INFO:tensorflow:Evaluation [50/100]\r\n",
      "INFO:tensorflow:Evaluation [51/100]\r\n",
      "INFO:tensorflow:Evaluation [52/100]\r\n",
      "INFO:tensorflow:Evaluation [53/100]\r\n",
      "INFO:tensorflow:Evaluation [54/100]\r\n",
      "INFO:tensorflow:Evaluation [55/100]\r\n",
      "INFO:tensorflow:Evaluation [56/100]\r\n",
      "INFO:tensorflow:Evaluation [57/100]\r\n",
      "INFO:tensorflow:Evaluation [58/100]\r\n",
      "INFO:tensorflow:Evaluation [59/100]\r\n",
      "INFO:tensorflow:Evaluation [60/100]\r\n",
      "INFO:tensorflow:Evaluation [61/100]\r\n",
      "INFO:tensorflow:Evaluation [62/100]\r\n",
      "INFO:tensorflow:Evaluation [63/100]\r\n",
      "INFO:tensorflow:Evaluation [64/100]\r\n",
      "INFO:tensorflow:Evaluation [65/100]\r\n",
      "INFO:tensorflow:Evaluation [66/100]\r\n",
      "INFO:tensorflow:Evaluation [67/100]\r\n",
      "INFO:tensorflow:Evaluation [68/100]\r\n",
      "INFO:tensorflow:Evaluation [69/100]\r\n",
      "INFO:tensorflow:Evaluation [70/100]\r\n",
      "INFO:tensorflow:Evaluation [71/100]\r\n",
      "INFO:tensorflow:Evaluation [72/100]\r\n",
      "INFO:tensorflow:Evaluation [73/100]\r\n",
      "INFO:tensorflow:Evaluation [74/100]\r\n",
      "INFO:tensorflow:Evaluation [75/100]\r\n",
      "INFO:tensorflow:Evaluation [76/100]\r\n",
      "INFO:tensorflow:Evaluation [77/100]\r\n",
      "INFO:tensorflow:Evaluation [78/100]\r\n",
      "INFO:tensorflow:Evaluation [79/100]\r\n",
      "INFO:tensorflow:Evaluation [80/100]\r\n",
      "INFO:tensorflow:Evaluation [81/100]\r\n",
      "INFO:tensorflow:Evaluation [82/100]\r\n",
      "INFO:tensorflow:Evaluation [83/100]\r\n",
      "INFO:tensorflow:Evaluation [84/100]\r\n",
      "INFO:tensorflow:Evaluation [85/100]\r\n",
      "INFO:tensorflow:Evaluation [86/100]\r\n",
      "INFO:tensorflow:Evaluation [87/100]\r\n",
      "INFO:tensorflow:Evaluation [88/100]\r\n",
      "INFO:tensorflow:Evaluation [89/100]\r\n",
      "INFO:tensorflow:Evaluation [90/100]\r\n",
      "INFO:tensorflow:Evaluation [91/100]\r\n",
      "INFO:tensorflow:Evaluation [92/100]\r\n",
      "INFO:tensorflow:Evaluation [93/100]\r\n",
      "INFO:tensorflow:Evaluation [94/100]\r\n",
      "INFO:tensorflow:Evaluation [95/100]\r\n",
      "INFO:tensorflow:Evaluation [96/100]\r\n",
      "INFO:tensorflow:Evaluation [97/100]\r\n",
      "INFO:tensorflow:Evaluation [98/100]\r\n",
      "INFO:tensorflow:Evaluation [99/100]\r\n",
      "INFO:tensorflow:Evaluation [100/100]\r\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-08-13:28:54\r\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.0, auroc = 1.0, global_step = 1000, loss = nan\r\n",
      "INFO:tensorflow:Performing the final export in the end of training.\r\n",
      "INFO:tensorflow:Restoring parameters from census_model\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Assets added to graph.\r\n",
      "INFO:tensorflow:No assets to write.\r\n",
      "INFO:tensorflow:SavedModel written to: b\"census_model\\\\export\\\\census\\\\temp-b'1518096536'\\\\saved_model.pb\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd H:/workspace/notebook/tensorflow_estimator/cloudml-samples/census/customestimator\n",
    "export TRAIN_STEPS=1000\n",
    "DATE=`date '+%Y%m%d_%H%M%S'`\n",
    "export OUTPUT_DIR=census_model\n",
    "rm -rf $OUTPUT_DIR\n",
    "\n",
    "TRAIN_FILE=../data/adult.data.csv\n",
    "EVAL_FILE=../data/adult.test.csv\n",
    "python -m trainer.task --train-files $TRAIN_FILE \\\n",
    "                       --eval-files $EVAL_FILE \\\n",
    "                       --job-dir $OUTPUT_DIR \\\n",
    "                       --train-steps $TRAIN_STEPS \\\n",
    "                       --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./movielens.tr', names=schema.cols)\n",
    "vl = pd.read_csv('./movielens.vl', names=schema.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516,1666,31,834,860,907,1018,1042,1048,1084,1...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.835749</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>932</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>932,1666,31,834,860,907,1018,1042,1048,1084,10...</td>\n",
       "      <td>2,7,18</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>932,1516,31,834,860,907,1018,1042,1048,1084,10...</td>\n",
       "      <td>2,3,17</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>1666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>932,1516,1666,834,860,907,1018,1042,1048,1084,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>932,1516,1666,31,860,907,1018,1042,1048,1084,1...</td>\n",
       "      <td>4,5,9,14</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     query_movie_ids    genres  avg_rating      year  \\\n",
       "0  1516,1666,31,834,860,907,1018,1042,1048,1084,1...         9    0.835749  0.763158   \n",
       "1  932,1666,31,834,860,907,1018,1042,1048,1084,10...    2,7,18    0.782609  0.605263   \n",
       "2  932,1516,31,834,860,907,1018,1042,1048,1084,10...    2,3,17    0.661939  0.701754   \n",
       "3  932,1516,1666,834,860,907,1018,1042,1048,1084,...         9    0.595238  0.815789   \n",
       "4  932,1516,1666,31,860,907,1018,1042,1048,1084,1...  4,5,9,14    0.711640  0.342105   \n",
       "\n",
       "   candidate_movie_id  rating  \n",
       "0                 932       0  \n",
       "1                1516       0  \n",
       "2                1666       0  \n",
       "3                  31       1  \n",
       "4                 834       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(filenames, schema, n_batch=128, n_epoch=1):\n",
    "    cols = ['query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "    defaults = [[''], [''], [], [], [0], [0]]\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        for col in ('query_movie_ids', 'genres'):\n",
    "            features[col] = tf.string_to_number(tf.string_split([features[col]], ',').values, out_type=tf.int32)\n",
    "            # features[col] = utils.to_dense(features[col])\n",
    "        return features\n",
    "\n",
    "    def add_seq_cols(feat):\n",
    "        multi_cols = ('query_movie_ids', 'genres')\n",
    "        for m_col in multi_cols:\n",
    "            name = '{}_lens'.format(m_col)\n",
    "            feat['{}_lens'.format(m_col)] = tf.size(feat[m_col])\n",
    "            cols.append(name)\n",
    "        return feat\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(filenames)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "    dataset = dataset.padded_batch(n_batch, OrderedDict(zip(cols, ([None], [None], [], [], [], [], [], []))) )\n",
    "    dataset = dataset.shuffle(n_batch * 1000, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop('rating')\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=5)\n",
    "    query_movie_ids = inputs[0]['query_movie_ids']\n",
    "    print(query_movie_ids.get_shape().as_list())\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    labels = tf.constant(np.ones([10, 8]))\n",
    "    pred = tf.concat([tf.Variable(tf.ones(shape=[1, 8]), trainable=False), tf.Variable(tf.truncated_normal([9, 8]))], 0)\n",
    "    loss = tf.losses.mean_squared_error(predictions=pred, labels=labels)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(pred.eval())\n",
    "        for i in range(1000):\n",
    "            sess.run([train_op])\n",
    "        print()\n",
    "        print(pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
