{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "from pprint import pprint\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath(os.path.abspath('..')).replace('\\\\', '/')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.decode_csv + tf.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Transform Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to parse D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'] ... \n",
      "[D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\merged_movielens.csv]: process take time 0:00:59.478155\n",
      "2018-02-13 11:02:02,449 - ctrl - INFO - foo: gen_data take time 0:01:03.828361\n",
      "{'err_cde': 0}\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "reload('service')\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "print(ctrl.gen_data(hparam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-13 14:37:53,430 - ctrl - INFO - foo: try to unserialize D:/Python/notebook/recomm_prod/repo\\foo\\data\\parsed_movielens.yaml\n",
      "2018-02-13 14:37:54,992 - service - INFO - hparam: {'conf_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml',\n",
      " 'data_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data',\n",
      " 'dim': 16,\n",
      " 'eval_name': 'foo',\n",
      " 'eval_steps': 233,\n",
      " 'export_name': 'export_foo',\n",
      " 'job_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model',\n",
      " 'n_batch': 128,\n",
      " 'override': True,\n",
      " 'parsed_conf_path': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\parsed_movielens.yaml',\n",
      " 'pid': 'foo',\n",
      " 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
      " 'repo': 'D:/Python/notebook/recomm_prod/repo\\\\foo',\n",
      " 'save_every_steps': None,\n",
      " 'train_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.tr',\n",
      " 'train_steps': 1647,\n",
      " 'valid_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.vl'}\n",
      "\n",
      "BestScoreExporter init\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 88, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002650E491048>, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086', '_num_worker_replicas': 1, '_service': None, '_task_id': 0, '_num_ps_replicas': 0, '_is_chief': True, '_task_type': 'worker', '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5}\n",
      "2018-02-13 14:37:54,997 - tensorflow - INFO - Using config: {'_save_checkpoints_secs': 600, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 88, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002650E491048>, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086', '_num_worker_replicas': 1, '_service': None, '_task_id': 0, '_num_ps_replicas': 0, '_is_chief': True, '_task_type': 'worker', '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "2018-02-13 14:37:55,000 - tensorflow - INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 20 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2018-02-13 14:37:55,003 - tensorflow - INFO - Start train and evaluate loop. The evaluate will happen after 20 secs (eval_spec.throttle_secs) or training is finished.\n",
      "WARNING:tensorflow:From D:/Python/notebook/recomm_prod\\reco_mf_dnn\\reco_mf_dnn_est.py:72: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2018-02-13 14:37:55,631 - tensorflow - WARNING - From D:/Python/notebook/recomm_prod\\reco_mf_dnn\\reco_mf_dnn_est.py:72: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-13 14:37:56,563 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:38:03,952 - tensorflow - INFO - Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.0883753\n",
      "2018-02-13 14:38:05,493 - tensorflow - INFO - step = 1, loss = 1.0883753\n",
      "INFO:tensorflow:global_step/sec: 30.1135\n",
      "2018-02-13 14:38:08,814 - tensorflow - INFO - global_step/sec: 30.1135\n",
      "INFO:tensorflow:step = 101, loss = 0.7336418 (3.327 sec)\n",
      "2018-02-13 14:38:08,820 - tensorflow - INFO - step = 101, loss = 0.7336418 (3.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.5932\n",
      "2018-02-13 14:38:11,056 - tensorflow - INFO - global_step/sec: 44.5932\n",
      "INFO:tensorflow:step = 201, loss = 0.61427903 (2.239 sec)\n",
      "2018-02-13 14:38:11,059 - tensorflow - INFO - step = 201, loss = 0.61427903 (2.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5803\n",
      "2018-02-13 14:38:14,034 - tensorflow - INFO - global_step/sec: 33.5803\n",
      "INFO:tensorflow:step = 301, loss = 0.678261 (2.978 sec)\n",
      "2018-02-13 14:38:14,037 - tensorflow - INFO - step = 301, loss = 0.678261 (2.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.8926\n",
      "2018-02-13 14:38:17,895 - tensorflow - INFO - global_step/sec: 25.8926\n",
      "INFO:tensorflow:step = 401, loss = 0.49780095 (3.863 sec)\n",
      "2018-02-13 14:38:17,899 - tensorflow - INFO - step = 401, loss = 0.49780095 (3.863 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 403 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:38:17,975 - tensorflow - INFO - Saving checkpoints for 403 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5983266.\n",
      "2018-02-13 14:38:19,374 - tensorflow - INFO - Loss for final step: 0.5983266.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-13-06:38:19\n",
      "2018-02-13 14:38:19,944 - tensorflow - INFO - Starting evaluation at 2018-02-13-06:38:19\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "2018-02-13 14:38:20,140 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-13 14:38:20,983 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-13 14:38:21,178 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-13 14:38:21,387 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-13 14:38:21,596 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-13 14:38:21,833 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-13 14:38:22,112 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-13 14:38:22,328 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-13 14:38:22,543 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-13 14:38:22,794 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-13 14:38:23,104 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-13 14:38:23,134 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-13-06:38:23\n",
      "2018-02-13 14:38:23,157 - tensorflow - INFO - Finished evaluation at 2018-02-13-06:38:23\n",
      "INFO:tensorflow:Saving dict for global step 403: auc = 0.77104956, global_step = 403, loss = 0.5719776\n",
      "2018-02-13 14:38:23,158 - tensorflow - INFO - Saving dict for global step 403: auc = 0.77104956, global_step = 403, loss = 0.5719776\n",
      "<tensorflow.python.estimator.estimator.Estimator object at 0x000002650E491FD0>\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\export\\export_foo\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "{'auc': 0.77104956, 'loss': 0.5719776, 'global_step': 403}\n",
      "False\n",
      "nice eval loss: 0.5719776153564453\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "2018-02-13 14:38:24,322 - tensorflow - INFO - Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-02-13 14:38:24,324 - tensorflow - INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-02-13 14:38:24,326 - tensorflow - INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "2018-02-13 14:38:24,419 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-02-13 14:38:24,607 - tensorflow - INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-02-13 14:38:24,609 - tensorflow - INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086\\\\export\\\\export_foo\\\\temp-b'1518503904'\\\\saved_model.pb\"\n",
      "2018-02-13 14:38:25,171 - tensorflow - INFO - SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086\\\\export\\\\export_foo\\\\temp-b'1518503904'\\\\saved_model.pb\"\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-13 14:38:26,533 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "2018-02-13 14:38:27,021 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-403\n",
      "INFO:tensorflow:Saving checkpoints for 404 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:38:30,763 - tensorflow - INFO - Saving checkpoints for 404 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:step = 404, loss = 0.52940273\n",
      "2018-02-13 14:38:32,033 - tensorflow - INFO - step = 404, loss = 0.52940273\n",
      "INFO:tensorflow:global_step/sec: 32.2515\n",
      "2018-02-13 14:38:35,133 - tensorflow - INFO - global_step/sec: 32.2515\n",
      "INFO:tensorflow:step = 504, loss = 0.75297713 (3.105 sec)\n",
      "2018-02-13 14:38:35,138 - tensorflow - INFO - step = 504, loss = 0.75297713 (3.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0034\n",
      "2018-02-13 14:38:37,407 - tensorflow - INFO - global_step/sec: 44.0034\n",
      "INFO:tensorflow:step = 604, loss = 0.47203583 (2.275 sec)\n",
      "2018-02-13 14:38:37,413 - tensorflow - INFO - step = 604, loss = 0.47203583 (2.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8367\n",
      "2018-02-13 14:38:40,196 - tensorflow - INFO - global_step/sec: 35.8367\n",
      "INFO:tensorflow:step = 704, loss = 0.67464596 (2.787 sec)\n",
      "2018-02-13 14:38:40,199 - tensorflow - INFO - step = 704, loss = 0.67464596 (2.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.3162\n",
      "2018-02-13 14:38:44,147 - tensorflow - INFO - global_step/sec: 25.3162\n",
      "INFO:tensorflow:step = 804, loss = 0.40794346 (3.953 sec)\n",
      "2018-02-13 14:38:44,152 - tensorflow - INFO - step = 804, loss = 0.40794346 (3.953 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 894 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:38:46,553 - tensorflow - INFO - Saving checkpoints for 894 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.49548.\n",
      "2018-02-13 14:38:47,993 - tensorflow - INFO - Loss for final step: 0.49548.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-13-06:38:48\n",
      "2018-02-13 14:38:48,502 - tensorflow - INFO - Starting evaluation at 2018-02-13-06:38:48\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "2018-02-13 14:38:48,700 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-13 14:38:49,360 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-13 14:38:49,587 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-13 14:38:49,795 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-13 14:38:50,000 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-13 14:38:50,234 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-13 14:38:50,604 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-13 14:38:50,849 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-13 14:38:51,073 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-13 14:38:51,296 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-13 14:38:51,630 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-13 14:38:51,666 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-13-06:38:51\n",
      "2018-02-13 14:38:51,700 - tensorflow - INFO - Finished evaluation at 2018-02-13-06:38:51\n",
      "INFO:tensorflow:Saving dict for global step 894: auc = 0.79723305, global_step = 894, loss = 0.5428071\n",
      "2018-02-13 14:38:51,703 - tensorflow - INFO - Saving dict for global step 894: auc = 0.79723305, global_step = 894, loss = 0.5428071\n",
      "<tensorflow.python.estimator.estimator.Estimator object at 0x000002650E491FD0>\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\export\\export_foo\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "{'auc': 0.79723305, 'loss': 0.5428071, 'global_step': 894}\n",
      "False\n",
      "nice eval loss: 0.5428071022033691\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "2018-02-13 14:38:52,044 - tensorflow - INFO - Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-02-13 14:38:52,047 - tensorflow - INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-02-13 14:38:52,049 - tensorflow - INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "2018-02-13 14:38:52,402 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-02-13 14:38:52,588 - tensorflow - INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-02-13 14:38:52,590 - tensorflow - INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086\\\\export\\\\export_foo\\\\temp-b'1518503932'\\\\saved_model.pb\"\n",
      "2018-02-13 14:38:53,176 - tensorflow - INFO - SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518503874.9942086\\\\export\\\\export_foo\\\\temp-b'1518503932'\\\\saved_model.pb\"\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-13 14:38:54,342 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "2018-02-13 14:38:54,899 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-894\n",
      "INFO:tensorflow:Saving checkpoints for 895 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:38:58,457 - tensorflow - INFO - Saving checkpoints for 895 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:step = 895, loss = 0.51960963\n",
      "2018-02-13 14:38:59,670 - tensorflow - INFO - step = 895, loss = 0.51960963\n",
      "INFO:tensorflow:global_step/sec: 32.4594\n",
      "2018-02-13 14:39:02,749 - tensorflow - INFO - global_step/sec: 32.4594\n",
      "INFO:tensorflow:step = 995, loss = 0.6568143 (3.082 sec)\n",
      "2018-02-13 14:39:02,752 - tensorflow - INFO - step = 995, loss = 0.6568143 (3.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7657\n",
      "2018-02-13 14:39:04,843 - tensorflow - INFO - global_step/sec: 47.7657\n",
      "INFO:tensorflow:step = 1095, loss = 0.40832534 (2.097 sec)\n",
      "2018-02-13 14:39:04,849 - tensorflow - INFO - step = 1095, loss = 0.40832534 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.7338\n",
      "2018-02-13 14:39:07,808 - tensorflow - INFO - global_step/sec: 33.7338\n",
      "INFO:tensorflow:step = 1195, loss = 0.4031521 (2.962 sec)\n",
      "2018-02-13 14:39:07,812 - tensorflow - INFO - step = 1195, loss = 0.4031521 (2.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.5205\n",
      "2018-02-13 14:39:11,726 - tensorflow - INFO - global_step/sec: 25.5205\n",
      "INFO:tensorflow:step = 1295, loss = 0.3138136 (3.920 sec)\n",
      "2018-02-13 14:39:11,732 - tensorflow - INFO - step = 1295, loss = 0.3138136 (3.920 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1394 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:39:14,351 - tensorflow - INFO - Saving checkpoints for 1394 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5853273.\n",
      "2018-02-13 14:39:15,803 - tensorflow - INFO - Loss for final step: 0.5853273.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-13-06:39:16\n",
      "2018-02-13 14:39:16,339 - tensorflow - INFO - Starting evaluation at 2018-02-13-06:39:16\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1394\n",
      "2018-02-13 14:39:16,543 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1394\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-13 14:39:17,410 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-13 14:39:17,608 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-13 14:39:17,822 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-13 14:39:18,059 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-13 14:39:18,285 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-13 14:39:18,681 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-13 14:39:18,962 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-13 14:39:19,177 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-13 14:39:19,377 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-13 14:39:19,713 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-13 14:39:19,746 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-13-06:39:19\n",
      "2018-02-13 14:39:19,782 - tensorflow - INFO - Finished evaluation at 2018-02-13-06:39:19\n",
      "INFO:tensorflow:Saving dict for global step 1394: auc = 0.79769087, global_step = 1394, loss = 0.5476228\n",
      "2018-02-13 14:39:19,784 - tensorflow - INFO - Saving dict for global step 1394: auc = 0.79769087, global_step = 1394, loss = 0.5476228\n",
      "<tensorflow.python.estimator.estimator.Estimator object at 0x000002650E491FD0>\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\export\\export_foo\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1394\n",
      "{'auc': 0.79769087, 'loss': 0.5476228, 'global_step': 1394}\n",
      "False\n",
      "bad eval loss: 0.547622799873352\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-13 14:39:21,298 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1394\n",
      "2018-02-13 14:39:21,692 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1394\n",
      "INFO:tensorflow:Saving checkpoints for 1395 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:39:25,491 - tensorflow - INFO - Saving checkpoints for 1395 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:step = 1395, loss = 0.47161388\n",
      "2018-02-13 14:39:26,600 - tensorflow - INFO - step = 1395, loss = 0.47161388\n",
      "INFO:tensorflow:global_step/sec: 32.4579\n",
      "2018-02-13 14:39:29,681 - tensorflow - INFO - global_step/sec: 32.4579\n",
      "INFO:tensorflow:step = 1495, loss = 0.62468755 (3.083 sec)\n",
      "2018-02-13 14:39:29,683 - tensorflow - INFO - step = 1495, loss = 0.62468755 (3.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9002\n",
      "2018-02-13 14:39:31,813 - tensorflow - INFO - global_step/sec: 46.9002\n",
      "INFO:tensorflow:step = 1595, loss = 0.36930346 (2.134 sec)\n",
      "2018-02-13 14:39:31,817 - tensorflow - INFO - step = 1595, loss = 0.36930346 (2.134 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "2018-02-13 14:39:33,189 - tensorflow - INFO - Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40475243.\n",
      "2018-02-13 14:39:34,491 - tensorflow - INFO - Loss for final step: 0.40475243.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-13-06:39:35\n",
      "2018-02-13 14:39:35,000 - tensorflow - INFO - Starting evaluation at 2018-02-13-06:39:35\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1647\n",
      "2018-02-13 14:39:35,316 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1647\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-13 14:39:35,987 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-13 14:39:36,170 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-13 14:39:36,366 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-13 14:39:36,583 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-13 14:39:36,836 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-13 14:39:37,192 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-13 14:39:37,452 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-13 14:39:37,758 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-13 14:39:37,970 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-13 14:39:38,238 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-13 14:39:38,267 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-13-06:39:38\n",
      "2018-02-13 14:39:38,294 - tensorflow - INFO - Finished evaluation at 2018-02-13-06:39:38\n",
      "INFO:tensorflow:Saving dict for global step 1647: auc = 0.76288104, global_step = 1647, loss = 0.6787882\n",
      "2018-02-13 14:39:38,296 - tensorflow - INFO - Saving dict for global step 1647: auc = 0.76288104, global_step = 1647, loss = 0.6787882\n",
      "<tensorflow.python.estimator.estimator.Estimator object at 0x000002650E491FD0>\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\export\\export_foo\n",
      "D:/Python/notebook/recomm_prod/repo\\foo\\model_1518503874.9942086\\model.ckpt-1647\n",
      "{'auc': 0.76288104, 'loss': 0.6787882, 'global_step': 1647}\n",
      "True\n",
      "bad eval loss: 0.6787881851196289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "reload('env')\n",
    "reload('service')\n",
    "est = reload('reco_mf_dnn.reco_mf_dnn_est')\n",
    "reload('service')\n",
    "\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "ctrl.train(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.read_csv('D:/Python/notebook/recomm_prod/repo/foo/data/data.tr', names=schema.cols)\n",
    "vl = pd.read_csv('D:/Python/notebook/recomm_prod/repo/foo/data/data.vl', names=schema.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "utils = reload('utils.utils')\n",
    "reco = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "\n",
    "model_dir='./model/reco_mf_dnn'\n",
    "n_movies = len(schema.col_states_['candidate_movie_id'].classes_)\n",
    "n_genres = len(schema.col_states_['genres'].classes_)\n",
    "\n",
    "reco_mf_dnn = reload('reco_mf_dnn.reco_mf_dnn_flex_shema')\n",
    "model = reco_mf_dnn.ModelMfDNN(schema, n_movies, n_genres, model_dir, hparam)\n",
    "train_fn = model.input_fn(filenames=[p.train_files], n_batch=p.batch_size)\n",
    "valid_fn = model.input_fn(filenames=[p.train_files], n_batch=p.batch_size, shuffle=False)\n",
    "model.fit(train_fn, valid_fn, reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./movielens.tr', names=schema.cols)\n",
    "vl = pd.read_csv('./movielens.vl', names=schema.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516,1666,31,834,860,907,1018,1042,1048,1084,1...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.835749</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>932</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>932,1666,31,834,860,907,1018,1042,1048,1084,10...</td>\n",
       "      <td>2,7,18</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>932,1516,31,834,860,907,1018,1042,1048,1084,10...</td>\n",
       "      <td>2,3,17</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>1666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>932,1516,1666,834,860,907,1018,1042,1048,1084,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>932,1516,1666,31,860,907,1018,1042,1048,1084,1...</td>\n",
       "      <td>4,5,9,14</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     query_movie_ids    genres  avg_rating      year  \\\n",
       "0  1516,1666,31,834,860,907,1018,1042,1048,1084,1...         9    0.835749  0.763158   \n",
       "1  932,1666,31,834,860,907,1018,1042,1048,1084,10...    2,7,18    0.782609  0.605263   \n",
       "2  932,1516,31,834,860,907,1018,1042,1048,1084,10...    2,3,17    0.661939  0.701754   \n",
       "3  932,1516,1666,834,860,907,1018,1042,1048,1084,...         9    0.595238  0.815789   \n",
       "4  932,1516,1666,31,860,907,1018,1042,1048,1084,1...  4,5,9,14    0.711640  0.342105   \n",
       "\n",
       "   candidate_movie_id  rating  \n",
       "0                 932       0  \n",
       "1                1516       0  \n",
       "2                1666       0  \n",
       "3                  31       1  \n",
       "4                 834       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir model/census\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n",
      "INFO:tensorflow:Using config: {'_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F871BA2860>, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_is_chief': True, '_model_dir': 'model/census', '_master': '', '_num_ps_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_task_type': 'worker', '_session_config': None, '_num_worker_replicas': 1, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5}\r\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1\r\n",
      "INFO:tensorflow:Saving checkpoints for 2 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:step = 2, loss = 23.01487\r\n",
      "INFO:tensorflow:global_step/sec: 40.1861\r\n",
      "INFO:tensorflow:step = 102, loss = 16.039494 (2.499 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5457\r\n",
      "INFO:tensorflow:step = 202, loss = 18.856514 (2.139 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.3414\r\n",
      "INFO:tensorflow:step = 302, loss = 19.34407 (2.258 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.8991\r\n",
      "INFO:tensorflow:step = 402, loss = 14.1604185 (2.224 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.0182\r\n",
      "INFO:tensorflow:step = 502, loss = 32.08555 (2.174 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 48.1635\r\n",
      "INFO:tensorflow:step = 602, loss = 14.999886 (2.076 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 47.889\r\n",
      "INFO:tensorflow:step = 702, loss = 15.736288 (2.089 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5455\r\n",
      "INFO:tensorflow:step = 802, loss = 12.467914 (2.149 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 45.2024\r\n",
      "INFO:tensorflow:step = 902, loss = 16.449446 (2.211 sec)\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:Loss for final step: 16.14082.\r\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-03:12:37\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Evaluation [10/100]\r\n",
      "INFO:tensorflow:Evaluation [20/100]\r\n",
      "INFO:tensorflow:Evaluation [30/100]\r\n",
      "INFO:tensorflow:Evaluation [40/100]\r\n",
      "INFO:tensorflow:Evaluation [50/100]\r\n",
      "INFO:tensorflow:Evaluation [60/100]\r\n",
      "INFO:tensorflow:Evaluation [70/100]\r\n",
      "INFO:tensorflow:Evaluation [80/100]\r\n",
      "INFO:tensorflow:Evaluation [90/100]\r\n",
      "INFO:tensorflow:Evaluation [100/100]\r\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-03:12:41\r\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.80275, accuracy_baseline = 0.76325, auc = 0.86648417, auc_precision_recall = 0.64984035, average_loss = 0.5127518, global_step = 1000, label/mean = 0.23675, loss = 20.510073, prediction/mean = 0.26840857\r\n",
      "INFO:tensorflow:Performing the final export in the end of training.\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\r\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "WARNING:tensorflow:Export includes no default signature!\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 170, in <module>\r\n",
      "    run_experiment(hparams)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 56, in run_experiment\r\n",
      "    eval_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 432, in train_and_evaluate\r\n",
      "    executor.run_local()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 618, in run_local\r\n",
      "    metrics = evaluator.evaluate_and_export()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 789, in evaluate_and_export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 821, in _export_eval_result\r\n",
      "    is_the_final_export=is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 168, in export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 115, in export\r\n",
      "    checkpoint_path=checkpoint_path)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 560, in export_savedmodel\r\n",
      "    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 90, in __init__\r\n",
      "    file_io.recursive_create_dir(self._export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 368, in recursive_create_dir\r\n",
      "    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n",
      "    c_api.TF_GetCode(self.status.status))\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: model/census\\export\\census\\temp-b'1518145966'; No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/tensorflow_estimator/census-demo\n",
    "TRAIN_FILE=data/adult.data.csv\n",
    "EVAL_FILE=data/adult.test.csv\n",
    "OUTPUT_DIR=model/census\n",
    "python -m trainer.task --train-files $TRAIN_FILE \\\n",
    "                       --eval-files $EVAL_FILE \\\n",
    "                       --job-dir $OUTPUT_DIR \\\n",
    "                       --train-steps 1000 \\\n",
    "                       --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\python.exe\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:\\Python\\Anaconda3\\envs\\py2_7\\python.exe: not found\n",
      "bash: line 10: --: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/recomm_prod\n",
    "TRAIN_FILE=trainer/data/movielens.tr\n",
    "EVAL_FILE=trainer/data/movielens.vl\n",
    "OUTPUT_DIR=trainer/model/reco_mf_dnn\n",
    "\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --job-dir $OUTPUT_DIR\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    labels = tf.constant(np.ones([10, 8]))\n",
    "    pred = tf.concat([tf.Variable(tf.ones(shape=[1, 8]), trainable=False), tf.Variable(tf.truncated_normal([9, 8]))], 0)\n",
    "    loss = tf.losses.mean_squared_error(predictions=pred, labels=labels)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(pred.eval())\n",
    "        for i in range(1000):\n",
    "            sess.run([train_op])\n",
    "        print()\n",
    "        print(pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
