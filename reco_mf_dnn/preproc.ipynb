{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "from pprint import pprint\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath(os.path.abspath('..')).replace('\\\\', '/')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.decode_csv + tf.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.flex.Schema at 0x23a85f7c1d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flex = reload('utils.flex')\n",
    "with codecs.open('../repo/foo/data/parsed.yaml', 'r', 'utf-8') as r:\n",
    "    schema = flex.Schema.unserialize(r)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.read_csv('../data/foo/user_supplied/raws/merged_movielens.csv', names=schema.raw_cols)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_id', 'candidate_movie_id', 'avg_rating', 'genres', 'year', 'query_movie_ids'])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'user_id': merged.query('user_id == 22').iloc[[0]].user_id.tolist(),\n",
    "    'query_movie_ids': merged.query('user_id == 22').iloc[[0]].query_movie_ids.tolist(),\n",
    "}\n",
    "\n",
    "items = movies.rename(index=str, columns={\"movieId\": \"candidate_movie_id\"})\\\n",
    "              .drop('title', 1) \\\n",
    "              .to_dict('list')\n",
    "data.update(items)\n",
    "with codecs.open('predic.data.json', 'w', 'utf-8') as w:\n",
    "    json.dump(data, w)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Tensor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-151bc15fc3a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_seq_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cols'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m    780\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1620\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.map()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m    484\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;31m# Adds this function into 'g'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;31m# Call func and gather the output tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[1;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1554\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m       \u001b[1;31m# If `map_func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-151bc15fc3a0>\u001b[0m in \u001b[0;36mtrans\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# features = OrderedDict(zip(cols, data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_states_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_movie_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_movie_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# for col in multi_cols:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#     features[col] = tf.string_to_number(tf.string_split(features[col], ',').values, out_type=tf.int32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/Python/notebook/recomm_prod\\utils\\utils.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0msplited\u001b[0m \u001b[0mby\u001b[0m \u001b[0mcomma\u001b[0m \u001b[0msign\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \"\"\"\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_null\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[{}]: null value detected'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# create/copy the manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Tensor' has no len()"
     ]
    }
   ],
   "source": [
    "cols = list(data.keys())\n",
    "multi_cols = ('query_movie_ids', 'genres')\n",
    "\n",
    "def trans(features):\n",
    "    # features = OrderedDict(zip(cols, data))\n",
    "    print( schema.col_states_['query_movie_ids'].transform( features['query_movie_ids'] ) )\n",
    "    # for col in multi_cols:\n",
    "    #     features[col] = tf.string_to_number(tf.string_split(features[col], ',').values, out_type=tf.int32)\n",
    "    return features\n",
    "\n",
    "def add_seq_cols(feat):\n",
    "    for m_col in multi_cols:\n",
    "        name = '{}_len'.format(m_col)\n",
    "        feat[name] = tf.size(feat[m_col])\n",
    "        cols.append(name)\n",
    "    return feat\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.Dataset.from_tensors(data)\n",
    "    dataset = dataset.map(trans, num_parallel_calls=4)\n",
    "    dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "    print('cols', cols)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.padded_batch(5, OrderedDict(zip(cols, ([], [], [], [None], [], [None], [], []))))\n",
    "    inputs = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Transform Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to parse D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'] ... \n",
      "[D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\merged_movielens.csv]: process take time 0:00:59.478155\n",
      "2018-02-13 11:02:02,449 - ctrl - INFO - foo: gen_data take time 0:01:03.828361\n",
      "{'err_cde': 0}\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "reload('service')\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "print(ctrl.gen_data(hparam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:04:00,539 - Ctrl - WARNING - parsed config [D:/Python/notebook/recomm_prod/repo\\foo\\data\\parsed.yaml] not found\n",
      "2018-02-14 12:04:00,541 - Ctrl - INFO - foo: try to generate training data...\n",
      "2018-02-14 12:04:00,543 - Loader - INFO - try to parse D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "2018-02-14 12:04:03,130 - Loader - INFO - try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'] ... \n",
      "[D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\merged_movielens.csv]: process take time 0:01:02.248726\n",
      "2018-02-14 12:05:06,191 - Service - INFO - hparam: {'conf_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml',\n",
      " 'data_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data',\n",
      " 'dim': 16,\n",
      " 'eval_name': 'foo',\n",
      " 'eval_steps': 233,\n",
      " 'export_name': 'export_foo',\n",
      " 'job_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model',\n",
      " 'n_batch': 128,\n",
      " 'override': True,\n",
      " 'parsed_conf_path': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\parsed.yaml',\n",
      " 'pid': 'foo',\n",
      " 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
      " 'raw_paths': ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'],\n",
      " 'repo': 'D:/Python/notebook/recomm_prod/repo\\\\foo',\n",
      " 'save_every_steps': None,\n",
      " 'train_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.tr',\n",
      " 'train_steps': 1647,\n",
      " 'valid_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.vl'}\n",
      "\n",
      "BestScoreExporter init\n",
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_num_worker_replicas': 1, '_task_id': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': 88, '_service': None, '_save_checkpoints_steps': None, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258', '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_save_checkpoints_secs': 600, '_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023AA1F767F0>, '_keep_checkpoint_every_n_hours': 10000}\n",
      "2018-02-14 12:05:06,197 - tensorflow - INFO - Using config: {'_task_type': 'worker', '_num_worker_replicas': 1, '_task_id': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': 88, '_service': None, '_save_checkpoints_steps': None, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258', '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_save_checkpoints_secs': 600, '_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023AA1F767F0>, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "2018-02-14 12:05:06,200 - tensorflow - INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2018-02-14 12:05:06,203 - tensorflow - INFO - Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:05:07,647 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:14,191 - tensorflow - INFO - Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.0883753\n",
      "2018-02-14 12:05:15,376 - tensorflow - INFO - step = 1, loss = 1.0883753\n",
      "INFO:tensorflow:global_step/sec: 30.5558\n",
      "2018-02-14 12:05:18,656 - tensorflow - INFO - global_step/sec: 30.5558\n",
      "INFO:tensorflow:step = 101, loss = 0.7336418 (3.299 sec)\n",
      "2018-02-14 12:05:18,674 - tensorflow - INFO - step = 101, loss = 0.7336418 (3.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6867\n",
      "2018-02-14 12:05:20,836 - tensorflow - INFO - global_step/sec: 45.6867\n",
      "INFO:tensorflow:step = 201, loss = 0.61456645 (2.166 sec)\n",
      "2018-02-14 12:05:20,840 - tensorflow - INFO - step = 201, loss = 0.61456645 (2.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3551\n",
      "2018-02-14 12:05:23,748 - tensorflow - INFO - global_step/sec: 34.3551\n",
      "INFO:tensorflow:step = 301, loss = 0.67162496 (2.916 sec)\n",
      "2018-02-14 12:05:23,756 - tensorflow - INFO - step = 301, loss = 0.67162496 (2.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9186\n",
      "2018-02-14 12:05:27,605 - tensorflow - INFO - global_step/sec: 25.9186\n",
      "INFO:tensorflow:step = 401, loss = 0.4971138 (3.853 sec)\n",
      "2018-02-14 12:05:27,609 - tensorflow - INFO - step = 401, loss = 0.4971138 (3.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.7209\n",
      "2018-02-14 12:05:30,328 - tensorflow - INFO - global_step/sec: 36.7209\n",
      "INFO:tensorflow:step = 501, loss = 0.6030406 (2.722 sec)\n",
      "2018-02-14 12:05:30,331 - tensorflow - INFO - step = 501, loss = 0.6030406 (2.722 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 549 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:32,330 - tensorflow - INFO - Saving checkpoints for 549 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.55781287.\n",
      "2018-02-14 12:05:33,417 - tensorflow - INFO - Loss for final step: 0.55781287.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:05:34\n",
      "2018-02-14 12:05:34,500 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:05:34\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:34,687 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:05:35,362 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:05:35,560 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:05:35,771 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:05:35,975 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:05:36,226 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:05:36,515 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:05:36,747 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:05:36,961 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:05:37,162 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:05:37,453 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:05:37,478 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:05:37\n",
      "2018-02-14 12:05:37,500 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:05:37\n",
      "INFO:tensorflow:Saving dict for global step 549: auc = 0.7915665, global_step = 549, loss = 0.57296175\n",
      "2018-02-14 12:05:37,502 - tensorflow - INFO - Saving dict for global step 549: auc = 0.7915665, global_step = 549, loss = 0.57296175\n",
      "2018-02-14 12:05:38,531 - BestScoreExporter - INFO - nice eval loss: 0.5729617476463318, export to pb\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-02-14 12:05:38,794 - tensorflow - INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-02-14 12:05:38,797 - tensorflow - INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "2018-02-14 12:05:38,799 - tensorflow - INFO - Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:38,895 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-02-14 12:05:39,079 - tensorflow - INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-02-14 12:05:39,081 - tensorflow - INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258\\\\export\\\\export_foo\\\\temp-b'1518581138'\\\\saved_model.pb\"\n",
      "2018-02-14 12:05:39,729 - tensorflow - INFO - SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258\\\\export\\\\export_foo\\\\temp-b'1518581138'\\\\saved_model.pb\"\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:05:41,137 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:41,536 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "INFO:tensorflow:Saving checkpoints for 550 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:45,134 - tensorflow - INFO - Saving checkpoints for 550 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 550, loss = 0.5548116\n",
      "2018-02-14 12:05:46,575 - tensorflow - INFO - step = 550, loss = 0.5548116\n",
      "INFO:tensorflow:global_step/sec: 32.4444\n",
      "2018-02-14 12:05:49,653 - tensorflow - INFO - global_step/sec: 32.4444\n",
      "INFO:tensorflow:step = 650, loss = 0.71449625 (3.083 sec)\n",
      "2018-02-14 12:05:49,658 - tensorflow - INFO - step = 650, loss = 0.71449625 (3.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3018\n",
      "2018-02-14 12:05:51,814 - tensorflow - INFO - global_step/sec: 46.3018\n",
      "INFO:tensorflow:step = 750, loss = 0.45504737 (2.161 sec)\n",
      "2018-02-14 12:05:51,819 - tensorflow - INFO - step = 750, loss = 0.45504737 (2.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6446\n",
      "2018-02-14 12:05:54,619 - tensorflow - INFO - global_step/sec: 35.6446\n",
      "INFO:tensorflow:step = 850, loss = 0.47156 (2.803 sec)\n",
      "2018-02-14 12:05:54,623 - tensorflow - INFO - step = 850, loss = 0.47156 (2.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.6189\n",
      "2018-02-14 12:05:58,522 - tensorflow - INFO - global_step/sec: 25.6189\n",
      "INFO:tensorflow:step = 950, loss = 0.43427843 (3.903 sec)\n",
      "2018-02-14 12:05:58,526 - tensorflow - INFO - step = 950, loss = 0.43427843 (3.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.2669\n",
      "2018-02-14 12:06:01,280 - tensorflow - INFO - global_step/sec: 36.2669\n",
      "INFO:tensorflow:step = 1050, loss = 0.5861499 (2.759 sec)\n",
      "2018-02-14 12:06:01,285 - tensorflow - INFO - step = 1050, loss = 0.5861499 (2.759 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1098 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:03,369 - tensorflow - INFO - Saving checkpoints for 1098 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5433118.\n",
      "2018-02-14 12:06:05,289 - tensorflow - INFO - Loss for final step: 0.5433118.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:06:05\n",
      "2018-02-14 12:06:05,883 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:06:05\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "2018-02-14 12:06:06,070 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:06:06,759 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:06:06,940 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:06:07,159 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:06:07,365 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:06:07,597 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:06:07,939 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:06:08,176 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:06:08,415 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:06:08,636 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:06:08,954 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:06:08,990 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:06:09\n",
      "2018-02-14 12:06:09,025 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:06:09\n",
      "INFO:tensorflow:Saving dict for global step 1098: auc = 0.7877959, global_step = 1098, loss = 0.59375995\n",
      "2018-02-14 12:06:09,027 - tensorflow - INFO - Saving dict for global step 1098: auc = 0.7877959, global_step = 1098, loss = 0.59375995\n",
      "2018-02-14 12:06:09,033 - BestScoreExporter - INFO - bad eval loss: 0.5937599539756775\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:06:10,863 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "2018-02-14 12:06:11,245 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "INFO:tensorflow:Saving checkpoints for 1099 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:15,225 - tensorflow - INFO - Saving checkpoints for 1099 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 1099, loss = 0.47767633\n",
      "2018-02-14 12:06:16,563 - tensorflow - INFO - step = 1099, loss = 0.47767633\n",
      "INFO:tensorflow:global_step/sec: 31.884\n",
      "2018-02-14 12:06:19,699 - tensorflow - INFO - global_step/sec: 31.884\n",
      "INFO:tensorflow:step = 1199, loss = 0.6478784 (3.140 sec)\n",
      "2018-02-14 12:06:19,703 - tensorflow - INFO - step = 1199, loss = 0.6478784 (3.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3961\n",
      "2018-02-14 12:06:21,913 - tensorflow - INFO - global_step/sec: 45.3961\n",
      "INFO:tensorflow:step = 1299, loss = 0.4191215 (2.229 sec)\n",
      "2018-02-14 12:06:21,932 - tensorflow - INFO - step = 1299, loss = 0.4191215 (2.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2044\n",
      "2018-02-14 12:06:24,742 - tensorflow - INFO - global_step/sec: 35.2044\n",
      "INFO:tensorflow:step = 1399, loss = 0.44278777 (2.813 sec)\n",
      "2018-02-14 12:06:24,746 - tensorflow - INFO - step = 1399, loss = 0.44278777 (2.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.5597\n",
      "2018-02-14 12:06:28,655 - tensorflow - INFO - global_step/sec: 25.5597\n",
      "INFO:tensorflow:step = 1499, loss = 0.35131794 (3.915 sec)\n",
      "2018-02-14 12:06:28,662 - tensorflow - INFO - step = 1499, loss = 0.35131794 (3.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3197\n",
      "2018-02-14 12:06:31,407 - tensorflow - INFO - global_step/sec: 36.3197\n",
      "INFO:tensorflow:step = 1599, loss = 0.50131756 (2.754 sec)\n",
      "2018-02-14 12:06:31,415 - tensorflow - INFO - step = 1599, loss = 0.50131756 (2.754 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:33,399 - tensorflow - INFO - Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40266097.\n",
      "2018-02-14 12:06:35,243 - tensorflow - INFO - Loss for final step: 0.40266097.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:06:35\n",
      "2018-02-14 12:06:35,758 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:06:35\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1647\n",
      "2018-02-14 12:06:35,945 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1647\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:06:36,634 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:06:36,818 - tensorflow - INFO - Evaluation [46/233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:06:37,018 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:06:37,237 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:06:37,477 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:06:37,784 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:06:38,053 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:06:38,327 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:06:38,581 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:06:38,865 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:06:38,893 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:06:38\n",
      "2018-02-14 12:06:38,914 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:06:38\n",
      "INFO:tensorflow:Saving dict for global step 1647: auc = 0.75802153, global_step = 1647, loss = 0.65210867\n",
      "2018-02-14 12:06:38,916 - tensorflow - INFO - Saving dict for global step 1647: auc = 0.75802153, global_step = 1647, loss = 0.65210867\n",
      "2018-02-14 12:06:38,920 - BestScoreExporter - INFO - bad eval loss: 0.6521086692810059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_mf_dnn.reco_mf_dnn_est.ModelMfDNN at 0x23ae3759860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "reload('env')\n",
    "reload('utils.flex')\n",
    "reload('service')\n",
    "est = reload('reco_mf_dnn.reco_mf_dnn_est')\n",
    "reload('service')\n",
    "\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "ctrl.train(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "reload('env')\n",
    "reload('service')\n",
    "est = reload('reco_mf_dnn.reco_mf_dnn_est')\n",
    "reload('service')\n",
    "\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "ctrl.train(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "    --package-path recomm_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.read_csv('D:/Python/notebook/recomm_prod/repo/foo/data/data.tr', names=schema.cols)\n",
    "vl = pd.read_csv('D:/Python/notebook/recomm_prod/repo/foo/data/data.vl', names=schema.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir model/census\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n",
      "INFO:tensorflow:Using config: {'_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F871BA2860>, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_is_chief': True, '_model_dir': 'model/census', '_master': '', '_num_ps_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_task_type': 'worker', '_session_config': None, '_num_worker_replicas': 1, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5}\r\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1\r\n",
      "INFO:tensorflow:Saving checkpoints for 2 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:step = 2, loss = 23.01487\r\n",
      "INFO:tensorflow:global_step/sec: 40.1861\r\n",
      "INFO:tensorflow:step = 102, loss = 16.039494 (2.499 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5457\r\n",
      "INFO:tensorflow:step = 202, loss = 18.856514 (2.139 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.3414\r\n",
      "INFO:tensorflow:step = 302, loss = 19.34407 (2.258 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.8991\r\n",
      "INFO:tensorflow:step = 402, loss = 14.1604185 (2.224 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.0182\r\n",
      "INFO:tensorflow:step = 502, loss = 32.08555 (2.174 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 48.1635\r\n",
      "INFO:tensorflow:step = 602, loss = 14.999886 (2.076 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 47.889\r\n",
      "INFO:tensorflow:step = 702, loss = 15.736288 (2.089 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5455\r\n",
      "INFO:tensorflow:step = 802, loss = 12.467914 (2.149 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 45.2024\r\n",
      "INFO:tensorflow:step = 902, loss = 16.449446 (2.211 sec)\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:Loss for final step: 16.14082.\r\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-03:12:37\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Evaluation [10/100]\r\n",
      "INFO:tensorflow:Evaluation [20/100]\r\n",
      "INFO:tensorflow:Evaluation [30/100]\r\n",
      "INFO:tensorflow:Evaluation [40/100]\r\n",
      "INFO:tensorflow:Evaluation [50/100]\r\n",
      "INFO:tensorflow:Evaluation [60/100]\r\n",
      "INFO:tensorflow:Evaluation [70/100]\r\n",
      "INFO:tensorflow:Evaluation [80/100]\r\n",
      "INFO:tensorflow:Evaluation [90/100]\r\n",
      "INFO:tensorflow:Evaluation [100/100]\r\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-03:12:41\r\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.80275, accuracy_baseline = 0.76325, auc = 0.86648417, auc_precision_recall = 0.64984035, average_loss = 0.5127518, global_step = 1000, label/mean = 0.23675, loss = 20.510073, prediction/mean = 0.26840857\r\n",
      "INFO:tensorflow:Performing the final export in the end of training.\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\r\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "WARNING:tensorflow:Export includes no default signature!\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 170, in <module>\r\n",
      "    run_experiment(hparams)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 56, in run_experiment\r\n",
      "    eval_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 432, in train_and_evaluate\r\n",
      "    executor.run_local()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 618, in run_local\r\n",
      "    metrics = evaluator.evaluate_and_export()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 789, in evaluate_and_export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 821, in _export_eval_result\r\n",
      "    is_the_final_export=is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 168, in export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 115, in export\r\n",
      "    checkpoint_path=checkpoint_path)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 560, in export_savedmodel\r\n",
      "    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 90, in __init__\r\n",
      "    file_io.recursive_create_dir(self._export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 368, in recursive_create_dir\r\n",
      "    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n",
      "    c_api.TF_GetCode(self.status.status))\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: model/census\\export\\census\\temp-b'1518145966'; No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/tensorflow_estimator/census-demo\n",
    "TRAIN_FILE=data/adult.data.csv\n",
    "EVAL_FILE=data/adult.test.csv\n",
    "OUTPUT_DIR=model/census\n",
    "python -m trainer.task --train-files $TRAIN_FILE \\\n",
    "                       --eval-files $EVAL_FILE \\\n",
    "                       --job-dir $OUTPUT_DIR \\\n",
    "                       --train-steps 1000 \\\n",
    "                       --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:\\Python\\Anaconda3\\envs\\py2_7\\python.exe: not found\n",
      "bash: line 10: --: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/recomm_prod\n",
    "TRAIN_FILE=trainer/data/movielens.tr\n",
    "EVAL_FILE=trainer/data/movielens.vl\n",
    "OUTPUT_DIR=trainer/model/reco_mf_dnn\n",
    "\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --job-dir $OUTPUT_DIR\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    labels = tf.constant(np.ones([10, 8]))\n",
    "    pred = tf.concat([tf.Variable(tf.ones(shape=[1, 8]), trainable=False), tf.Variable(tf.truncated_normal([9, 8]))], 0)\n",
    "    loss = tf.losses.mean_squared_error(predictions=pred, labels=labels)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(pred.eval())\n",
    "        for i in range(1000):\n",
    "            sess.run([train_op])\n",
    "        print()\n",
    "        print(pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
