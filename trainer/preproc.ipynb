{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "\n",
    "from pprint import pprint\n",
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath('..').replace('\\\\', '/')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('trainer.utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Cmd Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd D:/Python/notebook/recomm_prod && \\\n",
    "gcloud ml-engine jobs submit training recomm_movielens_15 \\\n",
    "    --job-dir gs://recomm-job/foo/model \\\n",
    "    --runtime-version 1.4 \\\n",
    "    --module-name trainer.ctrl \\\n",
    "    --package-path trainer \\\n",
    "    --region asia-east1 \\\n",
    "    --config config.yaml \\\n",
    "    -- \\\n",
    "    --method train \\\n",
    "    --conf-path gs://recomm-job/foo/data/user_supplied/movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gcloud ml-engine jobs describe recomm_movielens_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd .. && python setup.py build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Client API Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "hparam.add_hparam('is_local', False)\n",
    "ctrl.gen_data(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-06 15:53:38,551 - Loader - INFO [line:363] - try to unserialize from gs://recomm-job/foo-bar/movielens_recommendation/data/parsed.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'col_states_': OrderedDict([('query_movie_ids',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                     name='query_movie_ids', sep=',', vocabs=None, vocabs_path=None)),\n",
       "              ('genres',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                     sep='|', vocabs=None, vocabs_path=None)),\n",
       "              ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "              ('year', NumericMapper(default=None, name='year')),\n",
       "              ('candidate_movie_id',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                     name='candidate_movie_id', sep=None, vocabs=None, vocabs_path=None)),\n",
       "              ('rating',\n",
       "               CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                     sep=None, vocabs=None, vocabs_path=None))]),\n",
       " 'conf_': {'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "   {'id': 'query_movie_ids',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': ',',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "   {'id': 'genres',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': '|',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/genres.vocab'},\n",
       "   {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "   {'id': 'year', 'm_dtype': 'cont'},\n",
       "   {'id': 'candidate_movie_id',\n",
       "    'm_dtype': 'catg',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "   {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "    'id': 'timestamp',\n",
       "    'm_dtype': 'datetime'},\n",
       "   {'id': 'rating', 'm_dtype': 'catg'}],\n",
       "  'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       "  'label': ['rating'],\n",
       "  'model_id': 'movielens_recommendation',\n",
       "  'project_id': 'foo-bar',\n",
       "  'raw_dir': 'gs://movielens-foo/user_supplied/raws',\n",
       "  'user': ['query_movie_ids']},\n",
       " 'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml',\n",
       " 'count_': 100004,\n",
       " 'df_conf_':                                     id   m_dtype        date_format default  \\\n",
       " id                                                                            \n",
       " user_id                        user_id      catg               None    None   \n",
       " query_movie_ids        query_movie_ids      catg               None    None   \n",
       " genres                          genres      catg               None    None   \n",
       " avg_rating                  avg_rating      cont               None    None   \n",
       " year                              year      cont               None    None   \n",
       " candidate_movie_id  candidate_movie_id      catg               None    None   \n",
       " timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None   \n",
       " rating                          rating      catg               None    None   \n",
       " \n",
       "                     is_multi   sep vocabs  \\\n",
       " id                                          \n",
       " user_id                False  None   None   \n",
       " query_movie_ids         True     ,   None   \n",
       " genres                  True     |   None   \n",
       " avg_rating             False  None   None   \n",
       " year                   False  None   None   \n",
       " candidate_movie_id     False  None   None   \n",
       " timestamp              False  None   None   \n",
       " rating                 False  None   None   \n",
       " \n",
       "                                                       vocabs_path    aux  \\\n",
       " id                                                                         \n",
       " user_id                                                      None  False   \n",
       " query_movie_ids       gs://movielens-foo/user_supplied/item.vocab  False   \n",
       " genres              gs://movielens-foo/user_supplied/genres.vocab  False   \n",
       " avg_rating                                                   None  False   \n",
       " year                                                         None  False   \n",
       " candidate_movie_id    gs://movielens-foo/user_supplied/item.vocab  False   \n",
       " timestamp                                                    None  False   \n",
       " rating                                                       None  False   \n",
       " \n",
       "                      type                                          col_state  \n",
       " id                                                                            \n",
       " user_id              None                                               None  \n",
       " query_movie_ids      user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " genres               item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       " avg_rating           item  {cumsum_: 354374.99999999907, default: null, m...  \n",
       " year                 item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       " candidate_movie_id   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " timestamp            None                                               None  \n",
       " rating              label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  ,\n",
       " 'raw_paths': ['gs://movielens-foo/user_supplied/raws/merged_movielens.csv'],\n",
       " 'tr_count_': 70182,\n",
       " 'vl_count_': 29822}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml'}\n",
    "loader = ctrl.load_schema(params)\n",
    "\n",
    "vars(loader.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 0], [1, 2, 3], [1, 0, 0]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "col = [[1, 2], [1, 2, 3], [1]]\n",
    "maxlen = max(map(len, col))\n",
    "pad(col, padding=\"post\", maxlen=maxlen).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Python Client API Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 15:22:27,661 - Ctrl - INFO [line:139] - foo-bar: gen_data take time 0:00:06.690555\n",
      "jobId: foo_bar_movielens_recommendation_20180308152221665827\n",
      "state: QUEUED\n",
      "D:\\Google\\Cloud SDK\\google-cloud-sdk\\lib\\googlecloudsdk\\core\\util\\files.py:622: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  for chunk in iter(lambda: fp.read(4096), ''):\n",
      "Job [foo_bar_movielens_recommendation_20180308152221665827] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe foo_bar_movielens_recommendation_20180308152221665827\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs foo_bar_movielens_recommendation_20180308152221665827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml',\n",
    "          'runtime_version': '1.4'}\n",
    "ret = ctrl.train_submit(params)\n",
    "job_id = ret.get('job_id')\n",
    "print( ret.get('response') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Describe Job States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 15:22:28,470 - googleapiclient.discovery - INFO [line:274] - URL being requested: GET https://www.googleapis.com/discovery/v1/apis/ml/v1/rest\n",
      "2018-03-08 15:22:29,763 - googleapiclient.discovery - INFO [line:868] - URL being requested: GET https://ml.googleapis.com/v1/projects/training-recommendation-engine/jobs/foo_bar_movielens_recommendation_20180308152221665827?alt=json\n",
      "2018-03-08 15:22:29,765 - oauth2client.transport - INFO [line:151] - Attempting refresh to obtain initial access_token\n",
      "2018-03-08 15:22:29,798 - oauth2client.client - INFO [line:795] - Refreshing access_token\n",
      "2018-03-08 15:22:31,101 - Ctrl - INFO [line:157] - foo-bar: describe take time 0:00:02.643986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'createTime': '2018-03-08T07:22:26Z',\n",
       " 'jobId': 'foo_bar_movielens_recommendation_20180308152221665827',\n",
       " 'state': 'PREPARING',\n",
       " 'trainingInput': {'args': ['--train-steps',\n",
       "   '1000',\n",
       "   '--method',\n",
       "   'train',\n",
       "   '--conf-path',\n",
       "   'gs://movielens-foo/user_supplied/movielens.yaml',\n",
       "   '--job-id',\n",
       "   'foo_bar_movielens_recommendation_20180308152221665827'],\n",
       "  'jobDir': 'gs://recomm-job/foo-bar/movielens_recommendation/model',\n",
       "  'packageUris': ['gs://recomm-job/foo-bar/movielens_recommendation/model/packages/6a7f7097e2b3e0e19f97e673596db54eaeeeebba2e5e65a7c1bd959a20ed6dc7/trainer-0.1.tar.gz'],\n",
       "  'pythonModule': 'trainer.ctrl',\n",
       "  'pythonVersion': '3.5',\n",
       "  'region': 'asia-east1',\n",
       "  'runtimeVersion': '1.4'},\n",
       " 'trainingOutput': {}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "env = reload('trainer.env')\n",
    "utils = reload('trainer.utils.utils')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml', 'job_id': job_id}\n",
    "ret = ctrl.describe(params)\n",
    "ret.get('response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 15:43:40,636 - Service - INFO [line:98] - try to create model [foo_bar_movielens_recommendation] ...\n",
      "2018-03-08 15:43:40,643 - googleapiclient.discovery - INFO [line:868] - URL being requested: POST https://ml.googleapis.com/v1/projects/training-recommendation-engine/models?alt=json\n",
      "2018-03-08 15:43:40,645 - oauth2client.transport - INFO [line:151] - Attempting refresh to obtain initial access_token\n",
      "2018-03-08 15:43:40,682 - oauth2client.client - INFO [line:795] - Refreshing access_token\n",
      "2018-03-08 15:43:41,968 - Service - WARNING [line:135] - <HttpError 409 when requesting https://ml.googleapis.com/v1/projects/training-recommendation-engine/models?alt=json returned \"Field: model.name Error: A model with the same name already exists.\">\n",
      "2018-03-08 15:43:41,969 - Service - INFO [line:101] - try to clean old version ...\n",
      "2018-03-08 15:43:41,974 - googleapiclient.discovery - INFO [line:868] - URL being requested: GET https://ml.googleapis.com/v1/projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions?alt=json\n",
      "2018-03-08 15:43:42,245 - Service - INFO [line:142] - delete model version [projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions/v20180308153622527903]\n",
      "2018-03-08 15:43:42,250 - googleapiclient.discovery - INFO [line:868] - URL being requested: DELETE https://ml.googleapis.com/v1/projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions/v20180308153622527903?alt=json\n",
      "2018-03-08 15:43:43,298 - googleapiclient.discovery - INFO [line:868] - URL being requested: POST https://ml.googleapis.com/v1/projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions?alt=json\n",
      "2018-03-08 15:43:45,518 - Service - INFO [line:120] - [foo_bar_movielens_recommendation] write deploy version name: v20180308154343291994\n",
      "2018-03-08 15:43:45,648 - FlexIO - INFO [line:580] - upload to [gs://recomm-job/foo-bar/movielens_recommendation/data/deploy.yaml]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00',\n",
       " 'response': {'metadata': {'@type': 'type.googleapis.com/google.cloud.ml.v1.OperationMetadata',\n",
       "   'createTime': '2018-03-08T07:43:44Z',\n",
       "   'modelName': 'projects/training-recommendation-engine/models/foo_bar_movielens_recommendation',\n",
       "   'operationType': 'CREATE_VERSION',\n",
       "   'version': {'createTime': '2018-03-08T07:43:43Z',\n",
       "    'deploymentUri': 'gs://recomm-job/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520494135',\n",
       "    'description': '[foo-bar] recommendation model',\n",
       "    'name': 'projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions/v20180308154343291994',\n",
       "    'runtimeVersion': '1.4'}},\n",
       "  'name': 'projects/training-recommendation-engine/operations/create_foo_bar_movielens_recommendation_v20180308154343291994-1520495023263'}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml'}\n",
    "ret = ctrl.deploy(params)\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Information From Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 15:45:11,616 - googleapiclient.discovery - INFO [line:868] - URL being requested: GET https://ml.googleapis.com/v1/projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions/v20180308154343291994?alt=json\n",
      "2018-03-08 15:45:11,617 - oauth2client.transport - INFO [line:151] - Attempting refresh to obtain initial access_token\n",
      "2018-03-08 15:45:11,651 - oauth2client.client - INFO [line:795] - Refreshing access_token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00',\n",
       " 'response': {'createTime': '2018-03-08T07:43:43Z',\n",
       "  'deploymentUri': 'gs://recomm-job/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520494135',\n",
       "  'description': '[foo-bar] recommendation model',\n",
       "  'isDefault': True,\n",
       "  'name': 'projects/training-recommendation-engine/models/foo_bar_movielens_recommendation/versions/v20180308154343291994',\n",
       "  'runtimeVersion': '1.4',\n",
       "  'state': 'READY'}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml'}\n",
    "ret = ctrl.model_info(params)\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restful predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 17:51:45,153 - Loader - INFO [line:363] - try to unserialize from gs://recomm-job/foo-bar/movielens_recommendation/data/parsed.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': '../data/foo/user_supplied/movielens.yaml'}\n",
    "loader = ctrl.load_schema(params)\n",
    "merged = pd.read_csv('../data/foo/user_supplied/raws/merged_movielens.csv', names=loader.schema.raw_cols)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_rating': [3.8724696356275303,\n",
       "  3.4018691588785046,\n",
       "  3.1610169491525424,\n",
       "  2.3846153846153846,\n",
       "  3.267857142857143],\n",
       " 'candidate_movie_id': ['1', '2', '3', '4', '5'],\n",
       " 'genres': ['Adventure|Animation|Children|Comedy|Fantasy',\n",
       "  'Adventure|Children|Fantasy',\n",
       "  'Comedy|Romance',\n",
       "  'Comedy|Drama|Romance',\n",
       "  'Comedy'],\n",
       " 'query_movie_ids': ['32,1884,1580,1527,1387,1377,1376,1375,1372,1356,1339,1291,1270,1255,1240,1215,1210,1200,2174,2288,2291,3081,4011,3868,3793,3535,3527,3355,3213,3033,2459,2987,2959,2858,2762,2712,2571,2542,1198,1799,457,648,555,480,551,589,541,592,593,253,260,163,296,858,1089,2881,235,2990,2953,231,3052,208,2985,1148,2763,3147,267,2723,2717,2716,2710,2701,2700,2683,2672,3082,3300,3176,3751,4015,47,3999,3996,3994,3977,48,3826,3809,70,3697,173,3623,3578,153,158,3438,3408,3354,2617,3285,3253,2657,356,2616,588,1693,1682,1645,1641,1625,1608,1544,1391,552,586,1374,2605,1371,1320,784,1263,785,1080,1214,1208,1201,1097,1721,1722,1769,44,315,355,2502,1101,2431,2402,2340,2301,442,2232,2115,2081,2023,2011,2006,1997,1923,1917,1909,485,1876,4027'],\n",
       " 'year': [1995.0, 1995.0, 1995.0, 1995.0, 1995.0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def restful_data(user_ids):\n",
    "    data = {\n",
    "        'query_movie_ids': merged.query('user_id in {}'.format(user_ids)).groupby('user_id').query_movie_ids.max().tolist(),\n",
    "    }\n",
    "    items = movies.rename(index=str, columns={\"movieId\": \"candidate_movie_id\"}).drop('title', 1)\n",
    "    items.loc[:, 'candidate_movie_id'] = items.candidate_movie_id.astype(str)\n",
    "    # reduce to 5 records\n",
    "    items = items[:5].to_dict('list')\n",
    "    data.update(items)\n",
    "    return data\n",
    "    \n",
    "restful_data((22,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml',\n",
    "          'json_data': restful_data((22,))}\n",
    "ret = ctrl.predict(params)\n",
    "ret.get('response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'D:/Python/notebook/recomm_prod/repo/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520485856\\\\variables\\\\variables'\n",
      "2018-03-08 13:12:33,378 - tensorflow - INFO [line:116] - Restoring parameters from b'D:/Python/notebook/recomm_prod/repo/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520485856\\\\variables\\\\variables'\n",
      "global_step/Initializer/zeros/shape_as_tensor\n",
      "global_step/Initializer/zeros/Const\n",
      "global_step/Initializer/zeros\n",
      "global_step\n",
      "global_step/Assign\n",
      "global_step/read\n",
      "query_movie_ids\n",
      "genres\n",
      "avg_rating\n",
      "year\n",
      "candidate_movie_id\n",
      "query_movie_ids_len\n",
      "genres_len\n",
      "init/random_uniform/shape\n",
      "init/random_uniform/min\n",
      "init/random_uniform/max\n",
      "init/random_uniform/RandomUniform\n",
      "init/random_uniform/sub\n",
      "init/random_uniform/mul\n",
      "init/random_uniform\n",
      "init/b_global\n",
      "init/b_global/Assign\n",
      "init/b_global/read\n",
      "init/embedding/random_uniform/shape\n",
      "init/embedding/random_uniform/min\n",
      "init/embedding/random_uniform/max\n",
      "init/embedding/random_uniform/RandomUniform\n",
      "init/embedding/random_uniform/sub\n",
      "init/embedding/random_uniform/mul\n",
      "init/embedding/random_uniform\n",
      "init/embedding/w_query_movie_ids\n",
      "init/embedding/w_query_movie_ids/Assign\n",
      "init/embedding/w_query_movie_ids/read\n",
      "init/embedding/random_uniform_1/shape\n",
      "init/embedding/random_uniform_1/min\n",
      "init/embedding/random_uniform_1/max\n",
      "init/embedding/random_uniform_1/RandomUniform\n",
      "init/embedding/random_uniform_1/sub\n",
      "init/embedding/random_uniform_1/mul\n",
      "init/embedding/random_uniform_1\n",
      "init/embedding/b_query_movie_ids\n",
      "init/embedding/b_query_movie_ids/Assign\n",
      "init/embedding/b_query_movie_ids/read\n",
      "init/embedding/truncated_normal/shape\n",
      "init/embedding/truncated_normal/mean\n",
      "init/embedding/truncated_normal/stddev\n",
      "init/embedding/truncated_normal/TruncatedNormal\n",
      "init/embedding/truncated_normal/mul\n",
      "init/embedding/truncated_normal\n",
      "init/embedding/w_candidate_movie_id\n",
      "init/embedding/w_candidate_movie_id/Assign\n",
      "init/embedding/w_candidate_movie_id/read\n",
      "init/embedding/truncated_normal_1/shape\n",
      "init/embedding/truncated_normal_1/mean\n",
      "init/embedding/truncated_normal_1/stddev\n",
      "init/embedding/truncated_normal_1/TruncatedNormal\n",
      "init/embedding/truncated_normal_1/mul\n",
      "init/embedding/truncated_normal_1\n",
      "init/embedding/b_candidate_movie_id\n",
      "init/embedding/b_candidate_movie_id/Assign\n",
      "init/embedding/b_candidate_movie_id/read\n",
      "init/embedding/random_uniform_2/shape\n",
      "init/embedding/random_uniform_2/min\n",
      "init/embedding/random_uniform_2/max\n",
      "init/embedding/random_uniform_2/RandomUniform\n",
      "init/embedding/random_uniform_2/sub\n",
      "init/embedding/random_uniform_2/mul\n",
      "init/embedding/random_uniform_2\n",
      "init/embedding/w_genres\n",
      "init/embedding/w_genres/Assign\n",
      "init/embedding/w_genres/read\n",
      "user_encoding/embedding_lookup\n",
      "user_encoding/SequenceMask/Const\n",
      "user_encoding/SequenceMask/Max\n",
      "user_encoding/SequenceMask/Const_1\n",
      "user_encoding/SequenceMask/Const_2\n",
      "user_encoding/SequenceMask/Range\n",
      "user_encoding/SequenceMask/ExpandDims/dim\n",
      "user_encoding/SequenceMask/ExpandDims\n",
      "user_encoding/SequenceMask/Cast\n",
      "user_encoding/SequenceMask/Less\n",
      "user_encoding/ToFloat\n",
      "user_encoding/l2_normalize/Square\n",
      "user_encoding/l2_normalize/Sum/reduction_indices\n",
      "user_encoding/l2_normalize/Sum\n",
      "user_encoding/l2_normalize/Maximum/y\n",
      "user_encoding/l2_normalize/Maximum\n",
      "user_encoding/l2_normalize/Rsqrt\n",
      "user_encoding/l2_normalize\n",
      "user_encoding/ExpandDims/dim\n",
      "user_encoding/ExpandDims\n",
      "user_encoding/mul\n",
      "user_encoding/Sum/reduction_indices\n",
      "user_encoding/Sum\n",
      "user_encoding/strided_slice/stack\n",
      "user_encoding/strided_slice/stack_1\n",
      "user_encoding/strided_slice/stack_2\n",
      "user_encoding/strided_slice\n",
      "user_encoding/MatMul\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal/shape\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal/mean\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal/stddev\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal/mul\n",
      "user_encoding/dense/kernel/Initializer/truncated_normal\n",
      "user_encoding/dense/kernel\n",
      "user_encoding/dense/kernel/Assign\n",
      "user_encoding/dense/kernel/read\n",
      "user_encoding/dense/bias/Initializer/zeros/shape_as_tensor\n",
      "user_encoding/dense/bias/Initializer/zeros/Const\n",
      "user_encoding/dense/bias/Initializer/zeros\n",
      "user_encoding/dense/bias\n",
      "user_encoding/dense/bias/Assign\n",
      "user_encoding/dense/bias/read\n",
      "user_encoding/dense/MatMul\n",
      "user_encoding/dense/BiasAdd\n",
      "user_encoding/dense/Selu\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal/shape\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal/mean\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal/stddev\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal/mul\n",
      "user_encoding/dense_1/kernel/Initializer/truncated_normal\n",
      "user_encoding/dense_1/kernel\n",
      "user_encoding/dense_1/kernel/Assign\n",
      "user_encoding/dense_1/kernel/read\n",
      "user_encoding/dense_1/bias/Initializer/zeros/shape_as_tensor\n",
      "user_encoding/dense_1/bias/Initializer/zeros/Const\n",
      "user_encoding/dense_1/bias/Initializer/zeros\n",
      "user_encoding/dense_1/bias\n",
      "user_encoding/dense_1/bias/Assign\n",
      "user_encoding/dense_1/bias/read\n",
      "user_encoding/dense_1/MatMul\n",
      "user_encoding/dense_1/BiasAdd\n",
      "user_encoding/dense_1/Selu\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal/shape\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal/mean\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal/stddev\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal/mul\n",
      "user_encoding/dense_2/kernel/Initializer/truncated_normal\n",
      "user_encoding/dense_2/kernel\n",
      "user_encoding/dense_2/kernel/Assign\n",
      "user_encoding/dense_2/kernel/read\n",
      "user_encoding/dense_2/bias/Initializer/zeros/shape_as_tensor\n",
      "user_encoding/dense_2/bias/Initializer/zeros/Const\n",
      "user_encoding/dense_2/bias/Initializer/zeros\n",
      "user_encoding/dense_2/bias\n",
      "user_encoding/dense_2/bias/Assign\n",
      "user_encoding/dense_2/bias/read\n",
      "user_encoding/dense_2/MatMul\n",
      "user_encoding/dense_2/BiasAdd\n",
      "user_encoding/dense_2/Selu\n",
      "item_encoding/embedding_lookup\n",
      "item_encoding/embedding_lookup_1\n",
      "item_encoding/SequenceMask/Const\n",
      "item_encoding/SequenceMask/Max\n",
      "item_encoding/SequenceMask/Const_1\n",
      "item_encoding/SequenceMask/Const_2\n",
      "item_encoding/SequenceMask/Range\n",
      "item_encoding/SequenceMask/ExpandDims/dim\n",
      "item_encoding/SequenceMask/ExpandDims\n",
      "item_encoding/SequenceMask/Cast\n",
      "item_encoding/SequenceMask/Less\n",
      "item_encoding/ToFloat\n",
      "item_encoding/l2_normalize/Square\n",
      "item_encoding/l2_normalize/Sum/reduction_indices\n",
      "item_encoding/l2_normalize/Sum\n",
      "item_encoding/l2_normalize/Maximum/y\n",
      "item_encoding/l2_normalize/Maximum\n",
      "item_encoding/l2_normalize/Rsqrt\n",
      "item_encoding/l2_normalize\n",
      "item_encoding/ExpandDims/dim\n",
      "item_encoding/ExpandDims\n",
      "item_encoding/mul\n",
      "item_encoding/Sum/reduction_indices\n",
      "item_encoding/Sum\n",
      "item_encoding/strided_slice/stack\n",
      "item_encoding/strided_slice/stack_1\n",
      "item_encoding/strided_slice/stack_2\n",
      "item_encoding/strided_slice\n",
      "item_encoding/strided_slice_1/stack\n",
      "item_encoding/strided_slice_1/stack_1\n",
      "item_encoding/strided_slice_1/stack_2\n",
      "item_encoding/strided_slice_1\n",
      "item_encoding/concat/axis\n",
      "item_encoding/concat\n",
      "item_encoding/strided_slice_2/stack\n",
      "item_encoding/strided_slice_2/stack_1\n",
      "item_encoding/strided_slice_2/stack_2\n",
      "item_encoding/strided_slice_2\n",
      "item_encoding/MatMul\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal/shape\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal/mean\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal/stddev\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal/mul\n",
      "item_encoding/dense/kernel/Initializer/truncated_normal\n",
      "item_encoding/dense/kernel\n",
      "item_encoding/dense/kernel/Assign\n",
      "item_encoding/dense/kernel/read\n",
      "item_encoding/dense/bias/Initializer/zeros/shape_as_tensor\n",
      "item_encoding/dense/bias/Initializer/zeros/Const\n",
      "item_encoding/dense/bias/Initializer/zeros\n",
      "item_encoding/dense/bias\n",
      "item_encoding/dense/bias/Assign\n",
      "item_encoding/dense/bias/read\n",
      "item_encoding/dense/MatMul\n",
      "item_encoding/dense/BiasAdd\n",
      "item_encoding/dense/Selu\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal/shape\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal/mean\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal/stddev\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal/mul\n",
      "item_encoding/dense_1/kernel/Initializer/truncated_normal\n",
      "item_encoding/dense_1/kernel\n",
      "item_encoding/dense_1/kernel/Assign\n",
      "item_encoding/dense_1/kernel/read\n",
      "item_encoding/dense_1/bias/Initializer/zeros/shape_as_tensor\n",
      "item_encoding/dense_1/bias/Initializer/zeros/Const\n",
      "item_encoding/dense_1/bias/Initializer/zeros\n",
      "item_encoding/dense_1/bias\n",
      "item_encoding/dense_1/bias/Assign\n",
      "item_encoding/dense_1/bias/read\n",
      "item_encoding/dense_1/MatMul\n",
      "item_encoding/dense_1/BiasAdd\n",
      "item_encoding/dense_1/Selu\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal/shape\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal/mean\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal/stddev\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal/mul\n",
      "item_encoding/dense_2/kernel/Initializer/truncated_normal\n",
      "item_encoding/dense_2/kernel\n",
      "item_encoding/dense_2/kernel/Assign\n",
      "item_encoding/dense_2/kernel/read\n",
      "item_encoding/dense_2/bias/Initializer/zeros/shape_as_tensor\n",
      "item_encoding/dense_2/bias/Initializer/zeros/Const\n",
      "item_encoding/dense_2/bias/Initializer/zeros\n",
      "item_encoding/dense_2/bias\n",
      "item_encoding/dense_2/bias/Assign\n",
      "item_encoding/dense_2/bias/read\n",
      "item_encoding/dense_2/MatMul\n",
      "item_encoding/dense_2/BiasAdd\n",
      "item_encoding/dense_2/Selu\n",
      "gmf/mul\n",
      "gmf/Sum/reduction_indices\n",
      "gmf/Sum\n",
      "gmf/Add\n",
      "gmf/Add_1\n",
      "gmf/infer\n",
      "gmf/transpose/Rank\n",
      "gmf/transpose/sub/y\n",
      "gmf/transpose/sub\n",
      "gmf/transpose/Range/start\n",
      "gmf/transpose/Range/delta\n",
      "gmf/transpose/Range\n",
      "gmf/transpose/sub_1\n",
      "gmf/transpose\n",
      "gmf/MatMul\n",
      "gmf/Reshape/shape\n",
      "gmf/Reshape\n",
      "gmf/add\n",
      "gmf/add_1\n",
      "gmf/add_2\n",
      "gmf/pred\n",
      "save/Const\n",
      "save/StringJoin/inputs_1\n",
      "save/StringJoin\n",
      "save/num_shards\n",
      "save/ShardedFilename/shard\n",
      "save/ShardedFilename\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/MergeV2Checkpoints/checkpoint_prefixes\n",
      "save/MergeV2Checkpoints\n",
      "save/Identity\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/Assign_12\n",
      "save/Assign_13\n",
      "save/Assign_14\n",
      "save/Assign_15\n",
      "save/Assign_16\n",
      "save/Assign_17\n",
      "save/Assign_18\n",
      "save/restore_shard\n",
      "save/restore_all\n",
      "init_1\n",
      "init_all_tables\n",
      "init_2\n",
      "group_deps\n",
      "save_1/Const\n",
      "save_1/StringJoin/inputs_1\n",
      "save_1/StringJoin\n",
      "save_1/num_shards\n",
      "save_1/ShardedFilename/shard\n",
      "save_1/ShardedFilename\n",
      "save_1/SaveV2/tensor_names\n",
      "save_1/SaveV2/shape_and_slices\n",
      "save_1/SaveV2\n",
      "save_1/control_dependency\n",
      "save_1/MergeV2Checkpoints/checkpoint_prefixes\n",
      "save_1/MergeV2Checkpoints\n",
      "save_1/Identity\n",
      "save_1/RestoreV2/tensor_names\n",
      "save_1/RestoreV2/shape_and_slices\n",
      "save_1/RestoreV2\n",
      "save_1/Assign\n",
      "save_1/Assign_1\n",
      "save_1/Assign_2\n",
      "save_1/Assign_3\n",
      "save_1/Assign_4\n",
      "save_1/Assign_5\n",
      "save_1/Assign_6\n",
      "save_1/Assign_7\n",
      "save_1/Assign_8\n",
      "save_1/Assign_9\n",
      "save_1/Assign_10\n",
      "save_1/Assign_11\n",
      "save_1/Assign_12\n",
      "save_1/Assign_13\n",
      "save_1/Assign_14\n",
      "save_1/Assign_15\n",
      "save_1/Assign_16\n",
      "save_1/Assign_17\n",
      "save_1/Assign_18\n",
      "save_1/restore_shard\n",
      "save_1/restore_all\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "query_movie_ids\n",
    "genres\n",
    "avg_rating\n",
    "year\n",
    "candidate_movie_id\n",
    "query_movie_ids_len\n",
    "genres_len\n",
    "\"\"\"\n",
    "# path = 'D:/Python/notebook/recomm_prod/repo/foo-bar/movielens_recommendation/1520474347'\n",
    "path = 'D:/Python/notebook/recomm_prod/repo/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520485856'\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], path)\n",
    "        # graph = sess.graph\n",
    "        # print(sess.run(graph.get_tensor_by_name('gmf/pred:0'), feed_dict={\n",
    "        #     graph.get_tensor_by_name('query_movie_ids:0'): data_for_model.get('query_movie_ids'),\n",
    "        #     graph.get_tensor_by_name('genres:0'): data_for_model.get('genres'),\n",
    "        #     graph.get_tensor_by_name('avg_rating:0'): data_for_model.get('avg_rating'),\n",
    "        #     graph.get_tensor_by_name('year:0'): data_for_model.get('year'),\n",
    "        #     graph.get_tensor_by_name('candidate_movie_id:0'): data_for_model.get('candidate_movie_id'),\n",
    "        #     graph.get_tensor_by_name('query_movie_ids_len:0'): data_for_model.get('query_movie_ids_len'),\n",
    "        #     graph.get_tensor_by_name('genres_len:0'): data_for_model.get('genres_len')\n",
    "        # }))\n",
    "        for node in sess.graph.get_operations():\n",
    "            shape = node.node_def.attr.get('shape')\n",
    "            print(node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 13:07:29,937 - Loader - INFO [line:363] - try to unserialize from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/data/parsed.yaml\n",
      "2018-03-08 13:07:31,455 - Loader - INFO [line:381] - try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv'] ... \n",
      "2018-03-08 13:08:21,425 - Loader - INFO [line:440] - [D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv]: process take time 0:00:47.925426\n",
      "2018-03-08 13:08:21,431 - Ctrl - INFO [line:87] - foo-bar: gen_data take time 0:00:51.494970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "service = reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'is_local': True}\n",
    "ctrl.gen_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local View Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_states_': OrderedDict([('query_movie_ids',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                     name='query_movie_ids', sep=',', vocabs=None, vocabs_path=None)),\n",
       "              ('genres',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                     sep='|', vocabs=None, vocabs_path=None)),\n",
       "              ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "              ('year', NumericMapper(default=None, name='year')),\n",
       "              ('candidate_movie_id',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                     name='candidate_movie_id', sep=None, vocabs=None, vocabs_path=None)),\n",
       "              ('rating',\n",
       "               CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                     sep=None, vocabs=None, vocabs_path=None))]),\n",
       " 'conf_': {'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "   {'id': 'query_movie_ids',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': ',',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'id': 'genres',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': '|',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/genres.vocab'},\n",
       "   {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "   {'id': 'year', 'm_dtype': 'cont'},\n",
       "   {'id': 'candidate_movie_id',\n",
       "    'm_dtype': 'catg',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "    'id': 'timestamp',\n",
       "    'm_dtype': 'datetime'},\n",
       "   {'id': 'rating', 'm_dtype': 'catg'}],\n",
       "  'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       "  'label': ['rating'],\n",
       "  'model_id': 'movielens_recommendation',\n",
       "  'project_id': 'foo-bar',\n",
       "  'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
       "  'user': ['query_movie_ids']},\n",
       " 'conf_path': '../data/foo/user_supplied/movielens.local.yaml',\n",
       " 'count_': 100004,\n",
       " 'df_conf_':                                     id   m_dtype        date_format default  \\\n",
       " id                                                                            \n",
       " user_id                        user_id      catg               None    None   \n",
       " query_movie_ids        query_movie_ids      catg               None    None   \n",
       " genres                          genres      catg               None    None   \n",
       " avg_rating                  avg_rating      cont               None    None   \n",
       " year                              year      cont               None    None   \n",
       " candidate_movie_id  candidate_movie_id      catg               None    None   \n",
       " timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None   \n",
       " rating                          rating      catg               None    None   \n",
       " \n",
       "                     is_multi   sep vocabs  \\\n",
       " id                                          \n",
       " user_id                False  None   None   \n",
       " query_movie_ids         True     ,   None   \n",
       " genres                  True     |   None   \n",
       " avg_rating             False  None   None   \n",
       " year                   False  None   None   \n",
       " candidate_movie_id     False  None   None   \n",
       " timestamp              False  None   None   \n",
       " rating                 False  None   None   \n",
       " \n",
       "                                                           vocabs_path    aux  \\\n",
       " id                                                                             \n",
       " user_id                                                          None  False   \n",
       " query_movie_ids     D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " genres              D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " avg_rating                                                       None  False   \n",
       " year                                                             None  False   \n",
       " candidate_movie_id  D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " timestamp                                                        None  False   \n",
       " rating                                                           None  False   \n",
       " \n",
       "                      type                                          col_state  \n",
       " id                                                                            \n",
       " user_id              None                                               None  \n",
       " query_movie_ids      user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " genres               item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       " avg_rating           item  {cumsum_: 354374.99999999907, default: null, m...  \n",
       " year                 item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       " candidate_movie_id   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " timestamp            None                                               None  \n",
       " rating              label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  ,\n",
       " 'raw_paths': ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv'],\n",
       " 'tr_count_': 70182,\n",
       " 'vl_count_': 29822}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "service = reload('trainer.service')\n",
    "\n",
    "with flex.io('../repo/foo-bar/movielens_recommendation/data/parsed.yaml').as_reader() as f:\n",
    "    schema = flex.Schema.unserialize(f.stream)\n",
    "vars(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 17:12:59,158 - Ctrl - INFO [line:168] - received params: {'is_local': True, 'train_steps': 600, 'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'runtime_version': '1.4'}\n",
      "2018-03-08 17:12:59,160 - Ctrl - INFO [line:174] - do local training\n",
      "2018-03-08 17:12:59,190 - Ctrl - INFO [line:194] - foo-bar: try to unserialize D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/data/parsed.yaml\n",
      "2018-03-08 17:13:01,264 - Service - INFO [line:41] - received params: {'save_every_steps': None, 'train_steps': 600, 'n_batch': 128, 'model_id': 'movielens_recommendation', 'valid_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/data/data.vl', 'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'job_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model', 'dim': 16, 'data_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/data', 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws', 'repo': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation', 'eval_name': 'foo-bar', 'eval_steps': 233, 'export_name': 'export_foo-bar', 'deploy_path': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/data/deploy.yaml', 'is_local': True, 'parsed_conf_path': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/data/parsed.yaml', 'train_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/data/data.tr', 'job_id': 'foo_bar_movielens_recommendation_20180308171301264705', 'pid': 'foo-bar', 'runtime_version': '1.4'}\n",
      "2018-03-08 17:13:01,278 - BestScoreExporter - INFO [line:223] - BestScoreExporter init\n",
      "INFO:tensorflow:Using config: {'_global_id_in_cluster': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_service': None, '_save_checkpoints_steps': None, '_log_step_count_steps': 300, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019F865F1550>, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 88, '_master': '', '_is_chief': True, '_task_type': 'worker', '_task_id': 0, '_session_config': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model', '_save_summary_steps': 100}\n",
      "2018-03-08 17:13:01,280 - tensorflow - INFO [line:116] - Using config: {'_global_id_in_cluster': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_service': None, '_save_checkpoints_steps': None, '_log_step_count_steps': 300, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019F865F1550>, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 88, '_master': '', '_is_chief': True, '_task_type': 'worker', '_task_id': 0, '_session_config': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "2018-03-08 17:13:01,283 - tensorflow - INFO [line:116] - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2018-03-08 17:13:01,285 - tensorflow - INFO [line:116] - Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:13:01,401 - tensorflow - INFO [line:116] - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "2018-03-08 17:13:02,809 - tensorflow - INFO [line:116] - Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-03-08 17:13:02,812 - tensorflow - INFO [line:116] - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-03-08 17:13:03,098 - tensorflow - INFO [line:116] - Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2018-03-08 17:13:04,147 - tensorflow - INFO [line:116] - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-03-08 17:13:04,161 - tensorflow - INFO [line:116] - Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "2018-03-08 17:13:07,014 - tensorflow - INFO [line:116] - Saving checkpoints for 1 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7007617, step = 1\n",
      "2018-03-08 17:13:08,093 - tensorflow - INFO [line:116] - loss = 0.7007617, step = 1\n",
      "INFO:tensorflow:loss = 0.542557, step = 101 (2.456 sec)\n",
      "2018-03-08 17:13:10,549 - tensorflow - INFO [line:116] - loss = 0.542557, step = 101 (2.456 sec)\n",
      "INFO:tensorflow:loss = 0.58175415, step = 201 (1.848 sec)\n",
      "2018-03-08 17:13:12,396 - tensorflow - INFO [line:116] - loss = 0.58175415, step = 201 (1.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3817\n",
      "2018-03-08 17:13:14,701 - tensorflow - INFO [line:116] - global_step/sec: 45.3817\n",
      "INFO:tensorflow:loss = 0.5847647, step = 301 (2.308 sec)\n",
      "2018-03-08 17:13:14,705 - tensorflow - INFO [line:116] - loss = 0.5847647, step = 301 (2.308 sec)\n",
      "INFO:tensorflow:loss = 0.5579722, step = 401 (3.040 sec)\n",
      "2018-03-08 17:13:17,745 - tensorflow - INFO [line:116] - loss = 0.5579722, step = 401 (3.040 sec)\n",
      "INFO:tensorflow:loss = 0.52537924, step = 501 (2.138 sec)\n",
      "2018-03-08 17:13:19,882 - tensorflow - INFO [line:116] - loss = 0.52537924, step = 501 (2.138 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 549 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "2018-03-08 17:13:21,424 - tensorflow - INFO [line:116] - Saving checkpoints for 549 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.46075484.\n",
      "2018-03-08 17:13:22,950 - tensorflow - INFO [line:116] - Loss for final step: 0.46075484.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:13:23,039 - tensorflow - INFO [line:116] - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "2018-03-08 17:13:23,625 - tensorflow - INFO [line:116] - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-08-09:13:23\n",
      "2018-03-08 17:13:23,662 - tensorflow - INFO [line:116] - Starting evaluation at 2018-03-08-09:13:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-03-08 17:13:23,810 - tensorflow - INFO [line:116] - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "2018-03-08 17:13:23,823 - tensorflow - INFO [line:116] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2018-03-08 17:13:24,139 - tensorflow - INFO [line:116] - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-03-08 17:13:24,155 - tensorflow - INFO [line:116] - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-03-08 17:13:24,484 - tensorflow - INFO [line:116] - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-03-08 17:13:24,664 - tensorflow - INFO [line:116] - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-03-08 17:13:24,868 - tensorflow - INFO [line:116] - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-03-08 17:13:25,055 - tensorflow - INFO [line:116] - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-03-08 17:13:25,304 - tensorflow - INFO [line:116] - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-03-08 17:13:25,605 - tensorflow - INFO [line:116] - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-03-08 17:13:25,813 - tensorflow - INFO [line:116] - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-03-08 17:13:26,035 - tensorflow - INFO [line:116] - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-03-08 17:13:26,236 - tensorflow - INFO [line:116] - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 17:13:26,497 - tensorflow - INFO [line:116] - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-03-08 17:13:26,523 - tensorflow - INFO [line:116] - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-08-09:13:26\n",
      "2018-03-08 17:13:26,552 - tensorflow - INFO [line:116] - Finished evaluation at 2018-03-08-09:13:26\n",
      "INFO:tensorflow:Saving dict for global step 549: auc = 0.79645175, global_step = 549, loss = 0.550726\n",
      "2018-03-08 17:13:26,554 - tensorflow - INFO [line:116] - Saving dict for global step 549: auc = 0.79645175, global_step = 549, loss = 0.550726\n",
      "2018-03-08 17:13:26,570 - BestScoreExporter - INFO [line:235] - clean export_path: D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\export\\export_foo-bar\n",
      "2018-03-08 17:13:26,574 - BestScoreExporter - INFO [line:240] - nice eval loss: 0.5507259964942932, export to pb\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:13:26,598 - tensorflow - INFO [line:116] - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "2018-03-08 17:13:26,892 - tensorflow - INFO [line:116] - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-03-08 17:13:26,896 - tensorflow - INFO [line:116] - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "2018-03-08 17:13:26,898 - tensorflow - INFO [line:116] - Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-03-08 17:13:26,900 - tensorflow - INFO [line:116] - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "2018-03-08 17:13:26,964 - tensorflow - INFO [line:116] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-03-08 17:13:27,123 - tensorflow - INFO [line:116] - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-03-08 17:13:27,125 - tensorflow - INFO [line:116] - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model\\\\export\\\\export_foo-bar\\\\temp-b'1520500406'\\\\saved_model.pb\"\n",
      "2018-03-08 17:13:27,831 - tensorflow - INFO [line:116] - SavedModel written to: b\"D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model\\\\export\\\\export_foo-bar\\\\temp-b'1520500406'\\\\saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:13:27,956 - tensorflow - INFO [line:116] - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "2018-03-08 17:13:29,270 - tensorflow - INFO [line:116] - Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-03-08 17:13:29,273 - tensorflow - INFO [line:116] - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-03-08 17:13:29,512 - tensorflow - INFO [line:116] - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "2018-03-08 17:13:29,531 - tensorflow - INFO [line:116] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-549\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2018-03-08 17:13:30,245 - tensorflow - INFO [line:116] - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-03-08 17:13:30,265 - tensorflow - INFO [line:116] - Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 550 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "2018-03-08 17:13:33,269 - tensorflow - INFO [line:116] - Saving checkpoints for 550 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.49453592, step = 550\n",
      "2018-03-08 17:13:34,372 - tensorflow - INFO [line:116] - loss = 0.49453592, step = 550\n",
      "INFO:tensorflow:Saving checkpoints for 600 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "2018-03-08 17:13:35,942 - tensorflow - INFO [line:116] - Saving checkpoints for 600 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.56020594.\n",
      "2018-03-08 17:13:37,225 - tensorflow - INFO [line:116] - Loss for final step: 0.56020594.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:13:37,310 - tensorflow - INFO [line:116] - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "2018-03-08 17:13:37,751 - tensorflow - INFO [line:116] - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-08-09:13:37\n",
      "2018-03-08 17:13:37,788 - tensorflow - INFO [line:116] - Starting evaluation at 2018-03-08-09:13:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-03-08 17:13:37,933 - tensorflow - INFO [line:116] - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-600\n",
      "2018-03-08 17:13:37,955 - tensorflow - INFO [line:116] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/movielens_recommendation/model\\model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2018-03-08 17:13:38,269 - tensorflow - INFO [line:116] - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-03-08 17:13:38,285 - tensorflow - INFO [line:116] - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-03-08 17:13:38,621 - tensorflow - INFO [line:116] - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-03-08 17:13:38,813 - tensorflow - INFO [line:116] - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-03-08 17:13:39,003 - tensorflow - INFO [line:116] - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-03-08 17:13:39,236 - tensorflow - INFO [line:116] - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-03-08 17:13:39,450 - tensorflow - INFO [line:116] - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-03-08 17:13:39,741 - tensorflow - INFO [line:116] - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-03-08 17:13:39,963 - tensorflow - INFO [line:116] - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-03-08 17:13:40,226 - tensorflow - INFO [line:116] - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-03-08 17:13:40,421 - tensorflow - INFO [line:116] - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-03-08 17:13:40,693 - tensorflow - INFO [line:116] - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-03-08 17:13:40,720 - tensorflow - INFO [line:116] - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-08-09:13:40\n",
      "2018-03-08 17:13:40,749 - tensorflow - INFO [line:116] - Finished evaluation at 2018-03-08-09:13:40\n",
      "INFO:tensorflow:Saving dict for global step 600: auc = 0.7766894, global_step = 600, loss = 0.57157254\n",
      "2018-03-08 17:13:40,751 - tensorflow - INFO [line:116] - Saving dict for global step 600: auc = 0.7766894, global_step = 600, loss = 0.57157254\n",
      "2018-03-08 17:13:40,764 - BestScoreExporter - INFO [line:248] - bad eval loss: 0.5715725421905518\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.reco_mf_dnn_est')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': '../data/foo/user_supplied/movielens.local.yaml', \n",
    "          'is_local': True,\n",
    "          'runtime_version': '1.4',\n",
    "          'train_steps': 600}\n",
    "result = ctrl.train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ans': <tf.Tensor 'loss/strided_slice:0' shape=(?, 1) dtype=float32>,\n",
       " 'auc': (<tf.Tensor 'metrics/auc/value:0' shape=() dtype=float32>,\n",
       "  <tf.Tensor 'metrics/auc/update_op:0' shape=() dtype=float32>),\n",
       " 'avg_rating': <tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=float32>,\n",
       " 'b_candidate_movie_id': <tf.Variable 'init/embedding/b_candidate_movie_id:0' shape=(26,) dtype=float32_ref>,\n",
       " 'b_global': <tf.Variable 'init/b_global:0' shape=() dtype=float32_ref>,\n",
       " 'b_query_movie_ids': <tf.Variable 'init/embedding/b_query_movie_ids:0' shape=(16,) dtype=float32_ref>,\n",
       " 'candidate_bias': <tf.Tensor 'item_encoding/MatMul:0' shape=(?, 1) dtype=float32>,\n",
       " 'candidate_emb': <tf.Tensor 'item_encoding/embedding_lookup:0' shape=(?, 16) dtype=float32>,\n",
       " 'candidate_movie_id': <tf.Tensor 'IteratorGetNext:1' shape=(?,) dtype=int32>,\n",
       " 'emb_genres': <tf.Tensor 'item_encoding/Sum:0' shape=(?, 8) dtype=float32>,\n",
       " 'emb_item': <tf.Tensor 'item_encoding/dense_2/Selu:0' shape=(?, 32) dtype=float32>,\n",
       " 'emb_query': <tf.Tensor 'user_encoding/dense_2/Selu:0' shape=(?, 32) dtype=float32>,\n",
       " 'estimator_': <tensorflow.python.estimator.estimator.Estimator at 0x19f865f1a58>,\n",
       " 'exporter': <trainer.reco_mf_dnn_est.BestScoreExporter at 0x19f865f1400>,\n",
       " 'features': OrderedDict([('query_movie_ids',\n",
       "               <tf.Tensor 'IteratorGetNext:4' shape=(?, ?) dtype=int32>),\n",
       "              ('genres',\n",
       "               <tf.Tensor 'IteratorGetNext:2' shape=(?, ?) dtype=int32>),\n",
       "              ('avg_rating',\n",
       "               <tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=float32>),\n",
       "              ('year',\n",
       "               <tf.Tensor 'IteratorGetNext:7' shape=(?,) dtype=float32>),\n",
       "              ('candidate_movie_id',\n",
       "               <tf.Tensor 'IteratorGetNext:1' shape=(?,) dtype=int32>),\n",
       "              ('query_movie_ids_len',\n",
       "               <tf.Tensor 'IteratorGetNext:5' shape=(?,) dtype=int32>),\n",
       "              ('genres_len',\n",
       "               <tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=int32>)]),\n",
       " 'genres': <tf.Tensor 'IteratorGetNext:2' shape=(?, ?) dtype=int32>,\n",
       " 'genres_len': <tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=int32>,\n",
       " 'global_step': <tf.Variable 'global_step:0' shape=() dtype=int64_ref>,\n",
       " 'gmf': <tf.Tensor 'gmf/infer:0' shape=(?, 1) dtype=float32>,\n",
       " 'hparam': conf_path              ../data/foo/user_supplied/movielens.local.yaml\n",
       " is_local                                                         True\n",
       " runtime_version                                                   1.4\n",
       " train_steps                                                       600\n",
       " pid                                                           foo-bar\n",
       " raw_dir             D:/Python/notebook/recomm_prod/data/foo/user_s...\n",
       " model_id                                     movielens_recommendation\n",
       " repo                D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " job_dir             D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " data_dir            D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " deploy_path         D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " parsed_conf_path    D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " train_file          D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " valid_file          D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/mo...\n",
       " export_name                                            export_foo-bar\n",
       " eval_name                                                     foo-bar\n",
       " n_batch                                                           128\n",
       " eval_steps                                                        233\n",
       " dim                                                                16\n",
       " save_every_steps                                                 None\n",
       " job_id              foo_bar_movielens_recommendation_2018030817130...\n",
       " dtype: object,\n",
       " 'labels': <tf.Tensor 'IteratorGetNext:6' shape=(?,) dtype=int32>,\n",
       " 'loss': <tf.Tensor 'loss/Mean:0' shape=() dtype=float32>,\n",
       " 'model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model',\n",
       " 'n_genres': 20,\n",
       " 'n_items': 9125,\n",
       " 'pred': <tf.Tensor 'gmf/pred:0' shape=(?, ?) dtype=float32>,\n",
       " 'query_bias': <tf.Tensor 'user_encoding/MatMul:0' shape=(?, 1) dtype=float32>,\n",
       " 'query_movie_ids': <tf.Tensor 'IteratorGetNext:4' shape=(?, ?) dtype=int32>,\n",
       " 'query_movie_ids_len': <tf.Tensor 'IteratorGetNext:5' shape=(?,) dtype=int32>,\n",
       " 'schema': <trainer.utils.flex.Schema at 0x19f865cce80>,\n",
       " 'train_op': None,\n",
       " 'w_candidate_movie_id': <tf.Variable 'init/embedding/w_candidate_movie_id:0' shape=(9125, 16) dtype=float32_ref>,\n",
       " 'w_genres': <tf.Variable 'init/embedding/w_genres:0' shape=(20, 8) dtype=float32_ref>,\n",
       " 'w_query_movie_ids': <tf.Variable 'init/embedding/w_query_movie_ids:0' shape=(9125, 16) dtype=float32_ref>,\n",
       " 'year': <tf.Tensor 'IteratorGetNext:7' shape=(?,) dtype=float32>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = result.get('response')\n",
    "vars(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Get Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_save_summary_steps': 100, '_tf_random_seed': 88, '_is_chief': True, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model', '_evaluation_master': '', '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_service': None, '_save_checkpoints_secs': 600, '_task_id': 0, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DFB9519630>, '_global_id_in_cluster': 0, '_log_step_count_steps': 300}\n",
      "2018-03-08 17:57:17,896 - tensorflow - INFO [line:116] - Using config: {'_task_type': 'worker', '_save_summary_steps': 100, '_tf_random_seed': 88, '_is_chief': True, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/movielens_recommendation/model', '_evaluation_master': '', '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_service': None, '_save_checkpoints_secs': 600, '_task_id': 0, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DFB9519630>, '_global_id_in_cluster': 0, '_log_step_count_steps': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: {candidate_movie_id: (5,), genres: (5,), year: (5,), avg_rating: (5,), query_movie_ids: (1,)}, types: {candidate_movie_id: tf.string, genres: tf.string, year: tf.float32, avg_rating: tf.float32, query_movie_ids: tf.string}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "est = reload('trainer.reco_mf_dnn_est')\n",
    "service = reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "model = ctrl.get_model({'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'is_local': True, 'json_data': data})\n",
    "estimator = model.create_est()\n",
    "\n",
    "cols = ['query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "defaults = [[''], [''], [], [], [0], [0]]\n",
    "multi_cols = ('query_movie_ids', 'genres')\n",
    "\n",
    "# def add_seq_cols(feat):\n",
    "#     for m_col in multi_cols:\n",
    "#         name = '{}_len'.format(m_col)\n",
    "#         feat[name] = tf.size(feat[m_col])\n",
    "#         cols.append(name)\n",
    "#     return feat\n",
    "# dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "# dataset = dataset.repeat(1)\n",
    "# dataset = dataset.padded_batch(5, OrderedDict(zip(cols, ([None], [None], [], [], [], [], [], [], []))))\n",
    "# features = dataset.make_one_shot_iterator().get_next()\n",
    "# for e in estimator.predict(lambda: dataset):\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_rating': [3.8724696356275303,\n",
       "  3.4018691588785046,\n",
       "  3.1610169491525424,\n",
       "  2.3846153846153846,\n",
       "  3.267857142857143],\n",
       " 'candidate_movie_id': ['1', '2', '3', '4', '5'],\n",
       " 'genres': ['Adventure|Animation|Children|Comedy|Fantasy',\n",
       "  'Adventure|Children|Fantasy',\n",
       "  'Comedy|Romance',\n",
       "  'Comedy|Drama|Romance',\n",
       "  'Comedy'],\n",
       " 'query_movie_ids': ['32,1884,1580,1527,1387,1377,1376,1375,1372,1356,1339,1291,1270,1255,1240,1215,1210,1200,2174,2288,2291,3081,4011,3868,3793,3535,3527,3355,3213,3033,2459,2987,2959,2858,2762,2712,2571,2542,1198,1799,457,648,555,480,551,589,541,592,593,253,260,163,296,858,1089,2881,235,2990,2953,231,3052,208,2985,1148,2763,3147,267,2723,2717,2716,2710,2701,2700,2683,2672,3082,3300,3176,3751,4015,47,3999,3996,3994,3977,48,3826,3809,70,3697,173,3623,3578,153,158,3438,3408,3354,2617,3285,3253,2657,356,2616,588,1693,1682,1645,1641,1625,1608,1544,1391,552,586,1374,2605,1371,1320,784,1263,785,1080,1214,1208,1201,1097,1721,1722,1769,44,315,355,2502,1101,2431,2402,2340,2301,442,2232,2115,2081,2023,2011,2006,1997,1923,1917,1909,485,1876,4027'],\n",
       " 'year': [1995.0, 1995.0, 1995.0, 1995.0, 1995.0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 16:35:23,760 - Service - INFO [line:98] - try to create model [foo_bar_movielens_recommendation] ...\n",
      "2018-03-08 16:35:23,762 - Ctrl - ERROR [line:255] - create_model_rsc() takes 3 positional arguments but 4 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"D:/Python/notebook/recomm_prod\\trainer\\ctrl.py\", line 249, in deploy\n",
      "    res = self.service.deploy(p, deploy_conf[self.EXPORT_PATH])\n",
      "  File \"D:/Python/notebook/recomm_prod\\trainer\\service.py\", line 99, in deploy\n",
      "    self.create_model_rsc(ml, p, model_name)\n",
      "TypeError: create_model_rsc() takes 3 positional arguments but 4 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '99',\n",
       " 'err_msg': 'create_model_rsc() takes 3 positional arguments but 4 were given'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'is_local': True}\n",
    "ret = ctrl.deploy(params)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-03-08 17:21:54,874 - tensorflow - INFO [line:116] - Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"IteratorGetNext_2:3\", shape=(?, 1), dtype=string) must be from the same graph as Tensor(\"init/embedding/w_query_movie_ids:0\", shape=(9125, 16), dtype=float32_ref).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9da7abd33580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input_fn, predict_keys, hooks, checkpoint_path)\u001b[0m\n\u001b[0;32m    477\u001b[0m           input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[0;32m    478\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[1;32m--> 479\u001b[1;33m           features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[0;32m    480\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m       \u001b[0mall_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/Python/notebook/recomm_prod\\trainer\\reco_mf_dnn_est.py\u001b[0m in \u001b[0;36mgraphing\u001b[1;34m(self, features, labels, mode)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_encoding\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m# query_movie embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_query_movie_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_movie_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             query_movie_mask = tf.expand_dims(\n\u001b[0;32m     48\u001b[0m                 tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[0;32m    325\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m       transform_fn=None)\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[1;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embedding_lookup\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mnp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Number of partitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;31m# Preserve the resource variable status to avoid accidental dense reads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5614\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5615\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5616\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5617\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5618\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5282\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5283\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5284\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5285\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5286\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5218\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5219\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5220\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"IteratorGetNext_2:3\", shape=(?, 1), dtype=string) must be from the same graph as Tensor(\"init/embedding/w_query_movie_ids:0\", shape=(9125, 16), dtype=float32_ref)."
     ]
    }
   ],
   "source": [
    "model = result.get('response')\n",
    "flex = reload('trainer.utils.flex')\n",
    "# with flex.io('./data.json').as_reader('r') as f:\n",
    "#     data = json.load(f.stream)\n",
    "\n",
    "\n",
    "for e in model.estimator_.predict(input_fn=lambda: dataset):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'export_path': 'gs://recomm-job/foo-bar/movielens_recommendation/model/export/export_foo-bar/1520326728', 'model_name': 'foo_bar_movielens_recommendation', 'version': 'v20180306181514380085', 'job_id': 'movielens_recommendation_20180306165443177431'}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.reco_mf_dnn_est')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "params = {'conf_path': '../data/foo/user_supplied/movielens.local.yaml', \n",
    "          'raw_dir': 'gs://recomm-job/foo-bar'}\n",
    "print(ctrl.test(params).get('response'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改GCS movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.cloud.storage.blob import Blob\n",
    "from io import BytesIO\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "flex = reload('trainer.utils.flex')\n",
    "env = reload('trainer.env')\n",
    "\n",
    "with flex.io('../data/foo/user_supplied/movielens.yaml') as r, \\\n",
    "    flex.io('gs://movielens-foo/user_supplied/movielens.yaml') as w:\n",
    "    w.write(r.read())\n",
    "\n",
    "# stream = BytesIO(open('../data/foo/user_supplied/movielens.yaml', mode='rb').read())\n",
    "# utils.gcs_blob('gs://movielens-foo/user_supplied/movielens.yaml').upload_from_file(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(data.keys())\n",
    "multi_cols = ('query_movie_ids', 'genres')\n",
    "\n",
    "def trans(features):\n",
    "    # features = OrderedDict(zip(cols, data))\n",
    "    print( schema.col_states_['query_movie_ids'].transform( features['query_movie_ids'] ) )\n",
    "    # for col in multi_cols:\n",
    "    #     features[col] = tf.string_to_number(tf.string_split(features[col], ',').values, out_type=tf.int32)\n",
    "    return features\n",
    "\n",
    "def add_seq_cols(feat):\n",
    "    for m_col in multi_cols:\n",
    "        name = '{}_len'.format(m_col)\n",
    "        feat[name] = tf.size(feat[m_col])\n",
    "        cols.append(name)\n",
    "    return feat\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.Dataset.from_tensors(data)\n",
    "    dataset = dataset.map(trans, num_parallel_calls=4)\n",
    "    dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "    print('cols', cols)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.padded_batch(5, OrderedDict(zip(cols, ([], [], [], [None], [], [None], [], []))))\n",
    "    inputs = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dimension(None), Dimension(None), Dimension(None)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    a = tf.placeholder(shape=[None, None, None], dtype=tf.float32)\n",
    "    print( a.shape.dims )\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
