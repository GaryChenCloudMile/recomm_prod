{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "from pprint import pprint\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath('..').replace('\\\\', '/')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('trainer.utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_states_': OrderedDict([('query_movie_ids',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                     name='query_movie_ids', sep=',', vocabs=None, vocabs_path=None)),\n",
       "              ('genres',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                     sep='|', vocabs=None, vocabs_path=None)),\n",
       "              ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "              ('year', NumericMapper(default=None, name='year')),\n",
       "              ('candidate_movie_id',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                     name='candidate_movie_id', sep=None, vocabs=None, vocabs_path=None)),\n",
       "              ('rating',\n",
       "               CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                     sep=None, vocabs=None, vocabs_path=None))]),\n",
       " 'conf_': {'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "   {'id': 'query_movie_ids',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': ',',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'id': 'genres',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': '|',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/genres.vocab'},\n",
       "   {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "   {'id': 'year', 'm_dtype': 'cont'},\n",
       "   {'id': 'candidate_movie_id',\n",
       "    'm_dtype': 'catg',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "    'id': 'timestamp',\n",
       "    'm_dtype': 'datetime'},\n",
       "   {'id': 'rating', 'm_dtype': 'catg'}],\n",
       "  'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       "  'label': ['rating'],\n",
       "  'override': True,\n",
       "  'project_id': 'foo-bar',\n",
       "  'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
       "  'user': ['query_movie_ids']},\n",
       " 'conf_path': '../data/foo/user_supplied/movielens.local.yaml',\n",
       " 'count_': 100004,\n",
       " 'df_conf_':                                     id   m_dtype        date_format default  \\\n",
       " id                                                                            \n",
       " user_id                        user_id      catg               None    None   \n",
       " query_movie_ids        query_movie_ids      catg               None    None   \n",
       " genres                          genres      catg               None    None   \n",
       " avg_rating                  avg_rating      cont               None    None   \n",
       " year                              year      cont               None    None   \n",
       " candidate_movie_id  candidate_movie_id      catg               None    None   \n",
       " timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None   \n",
       " rating                          rating      catg               None    None   \n",
       " \n",
       "                     is_multi   sep vocabs  \\\n",
       " id                                          \n",
       " user_id                False  None   None   \n",
       " query_movie_ids         True     ,   None   \n",
       " genres                  True     |   None   \n",
       " avg_rating             False  None   None   \n",
       " year                   False  None   None   \n",
       " candidate_movie_id     False  None   None   \n",
       " timestamp              False  None   None   \n",
       " rating                 False  None   None   \n",
       " \n",
       "                                                           vocabs_path    aux  \\\n",
       " id                                                                             \n",
       " user_id                                                          None  False   \n",
       " query_movie_ids     D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " genres              D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " avg_rating                                                       None  False   \n",
       " year                                                             None  False   \n",
       " candidate_movie_id  D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " timestamp                                                        None  False   \n",
       " rating                                                           None  False   \n",
       " \n",
       "                      type                                          col_state  \n",
       " id                                                                            \n",
       " user_id              None                                               None  \n",
       " query_movie_ids      user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " genres               item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       " avg_rating           item  {cumsum_: 354374.99999999907, default: null, m...  \n",
       " year                 item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       " candidate_movie_id   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " timestamp            None                                               None  \n",
       " rating              label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  ,\n",
       " 'raw_paths': ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv'],\n",
       " 'tr_count_': 70182,\n",
       " 'vl_count_': 29822}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Cmd Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: recomm_movielens_15\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Google\\Cloud SDK\\google-cloud-sdk\\lib\\googlecloudsdk\\core\\util\\files.py:622: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  for chunk in iter(lambda: fp.read(4096), ''):\n",
      "Job [recomm_movielens_15] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe recomm_movielens_15\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs recomm_movielens_15\n"
     ]
    }
   ],
   "source": [
    "!cd D:/Python/notebook/recomm_prod && \\\n",
    "gcloud ml-engine jobs submit training recomm_movielens_15 \\\n",
    "    --job-dir gs://recomm-job/foo/model \\\n",
    "    --runtime-version 1.4 \\\n",
    "    --module-name trainer.ctrl \\\n",
    "    --package-path trainer \\\n",
    "    --region asia-east1 \\\n",
    "    --config config.yaml \\\n",
    "    -- \\\n",
    "    --method train \\\n",
    "    --conf-path gs://recomm-job/foo/data/user_supplied/movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2018-02-22T06:32:40Z'\n",
      "endTime: '2018-02-22T06:33:44Z'\n",
      "jobId: recomm_movielens_15\n",
      "startTime: '2018-02-22T06:33:13Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --method\n",
      "  - train\n",
      "  - --conf-path\n",
      "  - gs://recomm-job/foo/data/user_supplied/movielens.yaml\n",
      "  jobDir: gs://recomm-job/foo/model\n",
      "  packageUris:\n",
      "  - gs://recomm-job/foo/model/packages/0a10348c08fa5c2fcbcd7017d1f01ba18be758689a6f860fc725b64bff1f2955/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.ctrl\n",
      "  pythonVersion: '3.5'\n",
      "  region: asia-east1\n",
      "  runtimeVersion: '1.4'\n",
      "trainingOutput:\n",
      "  consumedMLUnits: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/ml/jobs/recomm_movielens_15?project=training-recommendation-engine\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Frecomm_movielens_15&project=training-recommendation-engine\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs describe recomm_movielens_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build\\lib\n",
      "creating build\\lib\\trainer\n",
      "copying trainer\\ctrl.py -> build\\lib\\trainer\n",
      "copying trainer\\env.py -> build\\lib\\trainer\n",
      "copying trainer\\reco_mf_dnn.py -> build\\lib\\trainer\n",
      "copying trainer\\reco_mf_dnn_est.py -> build\\lib\\trainer\n",
      "copying trainer\\service.py -> build\\lib\\trainer\n",
      "copying trainer\\__init__.py -> build\\lib\\trainer\n",
      "creating build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\flex.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\kkbox_data.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\neg_sampling_data.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\utils.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\__init__.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\logging.yaml -> build\\lib\\trainer\n"
     ]
    }
   ],
   "source": [
    "!cd .. && python setup.py build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Client API Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-01 10:15:30,889 - Loader - INFO [line:367] - try to parse gs://movielens-foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "2018-03-01 10:15:50,094 - CatgMapper - INFO [line:318] - [query_movie_ids] fetch vocab [gs://movielens-foo/user_supplied/item.vocab] \n",
      "2018-03-01 10:15:50,301 - CatgMapper - INFO [line:318] - [genres] fetch vocab [gs://movielens-foo/user_supplied/genres.vocab] \n",
      "2018-03-01 10:15:50,499 - CatgMapper - INFO [line:318] - [candidate_movie_id] fetch vocab [gs://movielens-foo/user_supplied/item.vocab] \n",
      "2018-03-01 10:15:53,042 - Loader - INFO [line:380] - try to transform ['gs://movielens-foo/user_supplied/raws/merged_movielens.csv'] ... \n",
      "2018-03-01 10:15:53,969 - FlexIO - INFO [line:542] - upload to [gs://recomm-job/foo-bar/data/parsed.yaml]\n",
      "2018-03-01 10:16:57,824 - Loader - INFO [line:439] - [gs://movielens-foo/user_supplied/raws/merged_movielens.csv]: process take time 0:01:03.553158\n",
      "2018-03-01 10:16:57,826 - FlexIO - INFO [line:542] - upload to [gs://recomm-job/foo-bar/data/data.tr]\n",
      "2018-03-01 10:17:31,261 - FlexIO - INFO [line:542] - upload to [gs://recomm-job/foo-bar/data/data.vl]\n",
      "2018-03-01 10:17:43,380 - Ctrl - INFO [line:82] - foo-bar: gen_data take time 0:02:13.164194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "'''\n",
    "{'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml',\n",
    " 'data_dir': 'gs://recomm-job/foo/data',\n",
    " 'job_dir': 'gs://recomm-job/foo/model',\n",
    " 'override': True,\n",
    " 'parsed_conf_path': 'gs://recomm-job/foo/data/parsed.yaml',\n",
    " 'pid': 'foo-bar',\n",
    " 'raw_dir': 'gs://movielens-foo/user_supplied/raws',\n",
    " 'repo': 'gs://recomm-job/foo',\n",
    " 'train_file': 'gs://recomm-job/foo/data/data.tr',\n",
    " 'valid_file': 'gs://recomm-job/foo/data/data.vl'}\n",
    "'''\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "ctrl.gen_data(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-01 10:20:04,380 - Loader - INFO [line:362] - try to unserialize from gs://recomm-job/foo-bar/data/parsed.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'col_states_': OrderedDict([('query_movie_ids',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                     name='query_movie_ids', sep=',', vocabs=None, vocabs_path=None)),\n",
       "              ('genres',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                     sep='|', vocabs=None, vocabs_path=None)),\n",
       "              ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "              ('year', NumericMapper(default=None, name='year')),\n",
       "              ('candidate_movie_id',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                     name='candidate_movie_id', sep=None, vocabs=None, vocabs_path=None)),\n",
       "              ('rating',\n",
       "               CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                     sep=None, vocabs=None, vocabs_path=None))]),\n",
       " 'conf_': {'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "   {'id': 'query_movie_ids',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': ',',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "   {'id': 'genres',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': '|',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/genres.vocab'},\n",
       "   {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "   {'id': 'year', 'm_dtype': 'cont'},\n",
       "   {'id': 'candidate_movie_id',\n",
       "    'm_dtype': 'catg',\n",
       "    'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "   {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "    'id': 'timestamp',\n",
       "    'm_dtype': 'datetime'},\n",
       "   {'id': 'rating', 'm_dtype': 'catg'}],\n",
       "  'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       "  'label': ['rating'],\n",
       "  'override': True,\n",
       "  'project_id': 'foo-bar',\n",
       "  'raw_dir': 'gs://movielens-foo/user_supplied/raws',\n",
       "  'user': ['query_movie_ids']},\n",
       " 'conf_path': 'gs://movielens-foo/user_supplied/movielens.yaml',\n",
       " 'count_': 100004,\n",
       " 'df_conf_':                                     id   m_dtype        date_format default  \\\n",
       " id                                                                            \n",
       " user_id                        user_id      catg               None    None   \n",
       " query_movie_ids        query_movie_ids      catg               None    None   \n",
       " genres                          genres      catg               None    None   \n",
       " avg_rating                  avg_rating      cont               None    None   \n",
       " year                              year      cont               None    None   \n",
       " candidate_movie_id  candidate_movie_id      catg               None    None   \n",
       " timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None   \n",
       " rating                          rating      catg               None    None   \n",
       " \n",
       "                     is_multi   sep vocabs  \\\n",
       " id                                          \n",
       " user_id                False  None   None   \n",
       " query_movie_ids         True     ,   None   \n",
       " genres                  True     |   None   \n",
       " avg_rating             False  None   None   \n",
       " year                   False  None   None   \n",
       " candidate_movie_id     False  None   None   \n",
       " timestamp              False  None   None   \n",
       " rating                 False  None   None   \n",
       " \n",
       "                                                       vocabs_path    aux  \\\n",
       " id                                                                         \n",
       " user_id                                                      None  False   \n",
       " query_movie_ids       gs://movielens-foo/user_supplied/item.vocab  False   \n",
       " genres              gs://movielens-foo/user_supplied/genres.vocab  False   \n",
       " avg_rating                                                   None  False   \n",
       " year                                                         None  False   \n",
       " candidate_movie_id    gs://movielens-foo/user_supplied/item.vocab  False   \n",
       " timestamp                                                    None  False   \n",
       " rating                                                       None  False   \n",
       " \n",
       "                      type                                          col_state  \n",
       " id                                                                            \n",
       " user_id              None                                               None  \n",
       " query_movie_ids      user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " genres               item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       " avg_rating           item  {cumsum_: 354374.99999999907, default: null, m...  \n",
       " year                 item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       " candidate_movie_id   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " timestamp            None                                               None  \n",
       " rating              label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  ,\n",
       " 'raw_paths': ['gs://movielens-foo/user_supplied/raws/merged_movielens.csv'],\n",
       " 'tr_count_': 70182,\n",
       " 'vl_count_': 29822}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "loader = ctrl.load_schema(hparam)\n",
    "\n",
    "vars(loader.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Python Client API Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-01 15:52:29,981 - Ctrl - INFO [line:135] - foo-bar: gen_data take time 0:00:05.607160\n",
      "jobId: foo_bar_20180301155224790759\n",
      "state: QUEUED\n",
      "D:\\Google\\Cloud SDK\\google-cloud-sdk\\lib\\googlecloudsdk\\core\\util\\files.py:622: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  for chunk in iter(lambda: fp.read(4096), ''):\n",
      "Job [foo_bar_20180301155224790759] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe foo_bar_20180301155224790759\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs foo_bar_20180301155224790759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "ret = ctrl.train_submit(hparam)\n",
    "jobid = ret.get('jobid')\n",
    "print( ret.get('response') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Describe Job States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-01 17:58:09,424 - googleapiclient.discovery - INFO [line:868] - URL being requested: GET https://ml.googleapis.com/v1/projects/training-recommendation-engine/jobs/foo_bar_20180301155224790759?alt=json\n",
      "2018-03-01 17:58:09,426 - oauth2client.transport - INFO [line:151] - Attempting refresh to obtain initial access_token\n",
      "2018-03-01 17:58:09,460 - oauth2client.client - INFO [line:795] - Refreshing access_token\n",
      "2018-03-01 17:58:10,734 - Ctrl - INFO [line:161] - foo-bar: gen_data take time 0:00:01.854730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'createTime': '2018-03-01T07:52:29Z',\n",
       " 'endTime': '2018-03-01T07:55:34Z',\n",
       " 'jobId': 'foo_bar_20180301155224790759',\n",
       " 'startTime': '2018-03-01T07:52:32Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingInput': {'args': ['--method',\n",
       "   'train',\n",
       "   '--conf-path',\n",
       "   'gs://movielens-foo/user_supplied/movielens.yaml'],\n",
       "  'jobDir': 'gs://recomm-job/foo-bar/model',\n",
       "  'packageUris': ['gs://recomm-job/foo-bar/model/packages/1b765adab33b729d2bbd13118a50fd4f1a558481b796b17c82a054647ff81dbb/trainer-0.1.tar.gz'],\n",
       "  'pythonModule': 'trainer.ctrl',\n",
       "  'pythonVersion': '3.5',\n",
       "  'region': 'asia-east1',\n",
       "  'runtimeVersion': '1.4'},\n",
       " 'trainingOutput': {'consumedMLUnits': 0.1}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "hparam.add_hparam('jobid', jobid)\n",
    "ret = ctrl.describe(hparam)\n",
    "ret['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://recomm-job/foo-bar/model/export/export_foo-bar/', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890799/', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890799/saved_model.pb', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890799/variables/', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890799/variables/variables.data-00000-of-00001', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890799/variables/variables.index', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890882/', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890882/saved_model.pb', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890882/variables/', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890882/variables/variables.data-00000-of-00001', 'gs://recomm-job/foo-bar/model/export/export_foo-bar/1519890882/variables/variables.index']\n"
     ]
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "ret = ctrl.deploy(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_local {'pid': 'foo-bar', 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws', 'repo': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar', 'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'job_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model', 'parsed_conf_path': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/parsed.yaml', 'valid_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.vl', 'data_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data', 'train_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.tr'}\n",
      "2018-03-01 10:52:50,595 - Loader - INFO [line:368] - try to parse ../data/foo/user_supplied/movielens.local.yaml (user supplied) ...\n",
      "2018-03-01 10:52:51,101 - CatgMapper - INFO [line:319] - [query_movie_ids] fetch vocab [D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab] \n",
      "2018-03-01 10:52:51,116 - CatgMapper - INFO [line:319] - [genres] fetch vocab [D:/Python/notebook/recomm_prod/data/foo/user_supplied/genres.vocab] \n",
      "2018-03-01 10:52:51,139 - CatgMapper - INFO [line:319] - [candidate_movie_id] fetch vocab [D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab] \n",
      "2018-03-01 10:52:53,313 - Loader - INFO [line:381] - try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv'] ... \n",
      "2018-03-01 10:52:53,319 - FlexIO - INFO [line:533] - try to mkdirs [D:\\Python\\notebook\\recomm_prod\\repo\\foo-bar\\data]\n",
      "2018-03-01 10:52:53,426 - FlexIO - INFO [line:533] - try to mkdirs [D:\\Python\\notebook\\recomm_prod\\repo\\foo-bar\\data]\n",
      "2018-03-01 10:52:53,875 - FlexIO - INFO [line:533] - try to mkdirs [D:\\Python\\notebook\\recomm_prod\\repo\\foo-bar\\data]\n",
      "2018-03-01 10:53:39,298 - Loader - INFO [line:440] - [D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv]: process take time 0:00:45.155469\n",
      "2018-03-01 10:53:39,303 - Ctrl - INFO [line:84] - foo-bar: gen_data take time 0:00:48.716391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "service = reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='../data/foo/user_supplied/movielens.local.yaml')\n",
    "hparam.add_hparam('is_local', True)\n",
    "ctrl.gen_data(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local View Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_states_': OrderedDict([('query_movie_ids',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True,\n",
       "                     name='query_movie_ids', sep=',', vocabs=None, vocabs_path=None)),\n",
       "              ('genres',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=True, name='genres',\n",
       "                     sep='|', vocabs=None, vocabs_path=None)),\n",
       "              ('avg_rating', NumericMapper(default=None, name='avg_rating')),\n",
       "              ('year', NumericMapper(default=None, name='year')),\n",
       "              ('candidate_movie_id',\n",
       "               CatgMapper(allow_null=True, default=None, is_multi=False,\n",
       "                     name='candidate_movie_id', sep=None, vocabs=None, vocabs_path=None)),\n",
       "              ('rating',\n",
       "               CatgMapper(allow_null=False, default=None, is_multi=False, name='rating',\n",
       "                     sep=None, vocabs=None, vocabs_path=None))]),\n",
       " 'conf_': {'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "   {'id': 'query_movie_ids',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': ',',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'id': 'genres',\n",
       "    'is_multi': True,\n",
       "    'm_dtype': 'catg',\n",
       "    'sep': '|',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/genres.vocab'},\n",
       "   {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "   {'id': 'year', 'm_dtype': 'cont'},\n",
       "   {'id': 'candidate_movie_id',\n",
       "    'm_dtype': 'catg',\n",
       "    'vocabs_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/item.vocab'},\n",
       "   {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "    'id': 'timestamp',\n",
       "    'm_dtype': 'datetime'},\n",
       "   {'id': 'rating', 'm_dtype': 'catg'}],\n",
       "  'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       "  'label': ['rating'],\n",
       "  'override': True,\n",
       "  'project_id': 'foo-bar',\n",
       "  'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
       "  'user': ['query_movie_ids']},\n",
       " 'conf_path': '../data/foo/user_supplied/movielens.local.yaml',\n",
       " 'count_': 100004,\n",
       " 'df_conf_':                                     id   m_dtype        date_format default  \\\n",
       " id                                                                            \n",
       " user_id                        user_id      catg               None    None   \n",
       " query_movie_ids        query_movie_ids      catg               None    None   \n",
       " genres                          genres      catg               None    None   \n",
       " avg_rating                  avg_rating      cont               None    None   \n",
       " year                              year      cont               None    None   \n",
       " candidate_movie_id  candidate_movie_id      catg               None    None   \n",
       " timestamp                    timestamp  datetime  %Y-%m-%d %H:%M:%S    None   \n",
       " rating                          rating      catg               None    None   \n",
       " \n",
       "                     is_multi   sep vocabs  \\\n",
       " id                                          \n",
       " user_id                False  None   None   \n",
       " query_movie_ids         True     ,   None   \n",
       " genres                  True     |   None   \n",
       " avg_rating             False  None   None   \n",
       " year                   False  None   None   \n",
       " candidate_movie_id     False  None   None   \n",
       " timestamp              False  None   None   \n",
       " rating                 False  None   None   \n",
       " \n",
       "                                                           vocabs_path    aux  \\\n",
       " id                                                                             \n",
       " user_id                                                          None  False   \n",
       " query_movie_ids     D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " genres              D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " avg_rating                                                       None  False   \n",
       " year                                                             None  False   \n",
       " candidate_movie_id  D:/Python/notebook/recomm_prod/data/foo/user_s...  False   \n",
       " timestamp                                                        None  False   \n",
       " rating                                                           None  False   \n",
       " \n",
       "                      type                                          col_state  \n",
       " id                                                                            \n",
       " user_id              None                                               None  \n",
       " query_movie_ids      user  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " genres               item  allow_null: true\\nclasses_: [(no genres listed...  \n",
       " avg_rating           item  {cumsum_: 354374.99999999907, default: null, m...  \n",
       " year                 item  {cumsum_: 199176755.0, default: null, max_: 20...  \n",
       " candidate_movie_id   item  allow_null: true\\nclasses_: ['1', '2', '3', '4...  \n",
       " timestamp            None                                               None  \n",
       " rating              label  allow_null: false\\nclasses_: ['0', '1']\\ndefau...  ,\n",
       " 'raw_paths': ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws/merged_movielens.csv'],\n",
       " 'tr_count_': 70182,\n",
       " 'vl_count_': 29822}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "flex = reload('trainer.utils.flex')\n",
    "service = reload('trainer.service')\n",
    "\n",
    "with flex.io('../repo/foo-bar/data/parsed.yaml').as_reader() as f:\n",
    "    schema = flex.Schema.unserialize(f.stream)\n",
    "vars(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-01 17:48:29,680 - Ctrl - INFO [line:178] - do local training ...\n",
      "prepare_local {'data_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data', 'eval_name': 'foo-bar', 'conf_path': '../data/foo/user_supplied/movielens.local.yaml', 'parsed_conf_path': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/parsed.yaml', 'export_name': 'export_foo-bar', 'valid_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.vl', 'train_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.tr', 'job_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model', 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws', 'is_local': True, 'repo': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar', 'pid': 'foo-bar', 'train_steps': 300}\n",
      "2018-03-01 17:48:29,711 - Ctrl - INFO [line:198] - foo-bar: try to unserialize D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/data/parsed.yaml\n",
      "2018-03-01 17:48:31,225 - Service - INFO [line:47] - hparam: {'conf_path': '../data/foo/user_supplied/movielens.local.yaml',\n",
      " 'data_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data',\n",
      " 'dim': 16,\n",
      " 'eval_name': 'foo-bar',\n",
      " 'eval_steps': 233,\n",
      " 'export_name': 'export_foo-bar',\n",
      " 'is_local': True,\n",
      " 'job_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model',\n",
      " 'n_batch': 128,\n",
      " 'parsed_conf_path': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/parsed.yaml',\n",
      " 'pid': 'foo-bar',\n",
      " 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
      " 'repo': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar',\n",
      " 'save_every_steps': None,\n",
      " 'train_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.tr',\n",
      " 'train_steps': 300,\n",
      " 'valid_file': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/data/data.vl'}\n",
      "\n",
      "2018-03-01 17:48:31,233 - Service - ERROR [line:65] - Traceback (most recent call last):\n",
      "  File \"D:/Python/notebook/recomm_prod\\trainer\\service.py\", line 63, in train\n",
      "    os.makedirs(utils.join(p.job_dir, 'export', p.export_name))\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\os.py\", line 241, in makedirs\n",
      "    mkdir(name, mode)\n",
      "FileExistsError: [WinError 183] 當檔案已存在時，無法建立該檔案。: 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model/export/export_foo-bar'\n",
      "\n",
      "2018-03-01 17:48:31,259 - BestScoreExporter - INFO [line:224] - BestScoreExporter init\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model', '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_task_id': 0, '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001ABC1519080>, '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_service': None, '_tf_random_seed': 88, '_is_chief': True, '_log_step_count_steps': 300, '_master': '', '_num_ps_replicas': 0, '_save_checkpoints_secs': 10}\n",
      "2018-03-01 17:48:31,264 - tensorflow - INFO [line:110] - Using config: {'_session_config': None, '_model_dir': 'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model', '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_task_id': 0, '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001ABC1519080>, '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_service': None, '_tf_random_seed': 88, '_is_chief': True, '_log_step_count_steps': 300, '_master': '', '_num_ps_replicas': 0, '_save_checkpoints_secs': 10}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "2018-03-01 17:48:31,269 - tensorflow - INFO [line:110] - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2018-03-01 17:48:31,272 - tensorflow - INFO [line:110] - Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-03-01 17:48:33,487 - tensorflow - INFO [line:110] - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt.\n",
      "2018-03-01 17:48:38,111 - tensorflow - INFO [line:110] - Saving checkpoints for 1 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0883753, step = 1\n",
      "2018-03-01 17:48:39,486 - tensorflow - INFO [line:110] - loss = 1.0883753, step = 1\n",
      "INFO:tensorflow:loss = 0.7336418, step = 101 (3.107 sec)\n",
      "2018-03-01 17:48:42,593 - tensorflow - INFO [line:110] - loss = 0.7336418, step = 101 (3.107 sec)\n",
      "INFO:tensorflow:loss = 0.6148873, step = 201 (2.163 sec)\n",
      "2018-03-01 17:48:44,755 - tensorflow - INFO [line:110] - loss = 0.6148873, step = 201 (2.163 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt.\n",
      "2018-03-01 17:48:47,552 - tensorflow - INFO [line:110] - Saving checkpoints for 300 into D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.56535697.\n",
      "2018-03-01 17:48:48,742 - tensorflow - INFO [line:110] - Loss for final step: 0.56535697.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-01-09:48:49\n",
      "2018-03-01 17:48:49,296 - tensorflow - INFO [line:110] - Starting evaluation at 2018-03-01-09:48:49\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt-300\n",
      "2018-03-01 17:48:49,496 - tensorflow - INFO [line:110] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt-300\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-03-01 17:48:50,833 - tensorflow - INFO [line:110] - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-03-01 17:48:51,016 - tensorflow - INFO [line:110] - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-03-01 17:48:51,217 - tensorflow - INFO [line:110] - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-03-01 17:48:51,452 - tensorflow - INFO [line:110] - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-03-01 17:48:51,693 - tensorflow - INFO [line:110] - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-03-01 17:48:52,027 - tensorflow - INFO [line:110] - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-03-01 17:48:52,268 - tensorflow - INFO [line:110] - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-03-01 17:48:52,501 - tensorflow - INFO [line:110] - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-03-01 17:48:52,717 - tensorflow - INFO [line:110] - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-03-01 17:48:52,983 - tensorflow - INFO [line:110] - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-03-01 17:48:53,019 - tensorflow - INFO [line:110] - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-01-09:48:53\n",
      "2018-03-01 17:48:53,047 - tensorflow - INFO [line:110] - Finished evaluation at 2018-03-01-09:48:53\n",
      "INFO:tensorflow:Saving dict for global step 300: auc = 0.7170216, global_step = 300, loss = 0.7662445\n",
      "2018-03-01 17:48:53,049 - tensorflow - INFO [line:110] - Saving dict for global step 300: auc = 0.7170216, global_step = 300, loss = 0.7662445\n",
      "2018-03-01 17:48:53,069 - BestScoreExporter - INFO [line:237] - nice eval loss: 0.7662444710731506, export to pb\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'outputs']\n",
      "2018-03-01 17:48:53,367 - tensorflow - INFO [line:110] - Signatures INCLUDED in export for Predict: ['serving_default', 'outputs']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-03-01 17:48:53,368 - tensorflow - INFO [line:110] - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-03-01 17:48:53,371 - tensorflow - INFO [line:110] - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt-300\n",
      "2018-03-01 17:48:53,476 - tensorflow - INFO [line:110] - Restoring parameters from D:\\Python\\notebook\\recomm_prod\\repo/foo-bar/model\\model.ckpt-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-03-01 17:48:53,666 - tensorflow - INFO [line:110] - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-03-01 17:48:53,668 - tensorflow - INFO [line:110] - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model\\\\export\\\\export_foo-bar\\\\temp-b'1519897733'\\\\saved_model.pb\"\n",
      "2018-03-01 17:48:54,188 - tensorflow - INFO [line:110] - SavedModel written to: b\"D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model\\\\export\\\\export_foo-bar\\\\temp-b'1519897733'\\\\saved_model.pb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'err_cde': '00'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.reco_mf_dnn_est')\n",
    "flex = reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='../data/foo/user_supplied/movielens.local.yaml')\n",
    "hparam.add_hparam('is_local', True)\n",
    "hparam.add_hparam('train_steps', 300)\n",
    "ctrl.train(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'D:\\\\Python\\\\notebook\\\\recomm_prod\\\\repo/foo-bar/model\\\\export\\\\export_foo-bar\\\\1519897733'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.service.model.exporter.export_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.reco_mf_dnn_est')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "# hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "hparam = HParams(conf_path='../data/foo/user_supplied/movielens.yaml')\n",
    "hparam.add_hparam('raw_dir', 'gs://recomm-job/foo-bar')\n",
    "print(ctrl.test(hparam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://recomm-job/foo-bar/model/checkpoint']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "flex = reload('trainer.utils.flex')\n",
    "env = reload('trainer.env')\n",
    "\n",
    "\n",
    "flex.io('gs://recomm-job/foo-bar/model/checkpoint').list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改GCS movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.cloud.storage.blob import Blob\n",
    "from io import BytesIO\n",
    "\n",
    "env = reload('trainer.env')\n",
    "utils = reload('trainer.utils.utils')\n",
    "stream = BytesIO(open('../data/foo/user_supplied/movielens.yaml', mode='rb').read())\n",
    "utils.gcs_blob('gs://movielens-foo/user_supplied/movielens.yaml').upload_from_file(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Python API Credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import storage\n",
    "\n",
    "authpath = '../auth.json'\n",
    "project = 'training-recommendation-engine'\n",
    "cred = GoogleCredentials.from_stream(authpath)\n",
    "svc = discovery.build('ml', 'v1', credentials=cred)\n",
    "\n",
    "st_client = storage.Client.from_service_account_json(authpath)\n",
    "bucket = st_client.get_bucket('recomm-job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "\n",
    "blob = bucket.get_blob('user_supplied/raws/merged_movielens.csv')\n",
    "sio = BytesIO()\n",
    "blob.download_to_file(sio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required parameter \"body\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-08673c4d57fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# b = a.get(name='projects/{}/jobs/recomm_movielens_13'.format(project))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'projects/{}/jobs/recomm_movielens_13'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\googleapiclient\\discovery.py\u001b[0m in \u001b[0;36mmethod\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m         if name not in _PAGE_TOKEN_NAMES or _findPageTokenName(\n\u001b[0;32m    731\u001b[0m             _methodProperties(methodDesc, schema, 'response')):\n\u001b[1;32m--> 732\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing required parameter \"%s\"'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpattern_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Missing required parameter \"body\""
     ]
    }
   ],
   "source": [
    "# b = a.get(name='projects/{}/jobs/recomm_movielens_13'.format(project))\n",
    "a = svc.projects().jobs()\n",
    "b = a.cancel(name='projects/{}/jobs/recomm_movielens_13'.format(project))\n",
    "b.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc.projects().jobs().get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Restful predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flex = reload('utils.flex')\n",
    "with codecs.open('../repo/foo/data/parsed.yaml', 'r', 'utf-8') as r:\n",
    "    schema = flex.Schema.unserialize(r)\n",
    "\n",
    "merged = pd.read_csv('../data/foo/user_supplied/raws/merged_movielens.csv', names=schema.raw_cols)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_movie_ids', 'year', 'candidate_movie_id', 'avg_rating', 'genres', 'user_id'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'user_id': merged.query('user_id == 22').iloc[[0]].user_id.tolist(),\n",
    "    'query_movie_ids': merged.query('user_id == 22').iloc[[0]].query_movie_ids.tolist(),\n",
    "}\n",
    "\n",
    "items = movies.rename(index=str, columns={\"movieId\": \"candidate_movie_id\"})\\\n",
    "              .drop('title', 1) # .to_dict('list')\n",
    "items.loc[:, 'candidate_movie_id'] = items.candidate_movie_id.astype(str)\n",
    "items = items.to_dict('list')\n",
    "data.update(items)\n",
    "with codecs.open('predic.data.json', 'w', 'utf-8') as w:\n",
    "    json.dump(data, w)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "reload('utils')\n",
    "reload('utils.flex')\n",
    "reload('service')\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "hparam.add_hparam('data', 'predic.data.json')\n",
    "ret = ctrl.predict(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.ml-engine.local.predict) Something has gone really wrong; we can't find a valid Python executable on your PATH.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local predict \\\n",
    "--model-dir D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "--json-instances tmp.20180221155312370200.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\python.exe: can't open file '/cygdrive/d/google-cloud-sdk/lib/gcloud.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local predict \\\n",
    "--model-dir D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "--json-instances tmp.20180221155312370200.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-151bc15fc3a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmulti_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'query_movie_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'genres'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# features = OrderedDict(zip(cols, data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "cols = list(data.keys())\n",
    "multi_cols = ('query_movie_ids', 'genres')\n",
    "\n",
    "def trans(features):\n",
    "    # features = OrderedDict(zip(cols, data))\n",
    "    print( schema.col_states_['query_movie_ids'].transform( features['query_movie_ids'] ) )\n",
    "    # for col in multi_cols:\n",
    "    #     features[col] = tf.string_to_number(tf.string_split(features[col], ',').values, out_type=tf.int32)\n",
    "    return features\n",
    "\n",
    "def add_seq_cols(feat):\n",
    "    for m_col in multi_cols:\n",
    "        name = '{}_len'.format(m_col)\n",
    "        feat[name] = tf.size(feat[m_col])\n",
    "        cols.append(name)\n",
    "    return feat\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.Dataset.from_tensors(data)\n",
    "    dataset = dataset.map(trans, num_parallel_calls=4)\n",
    "    dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "    print('cols', cols)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.padded_batch(5, OrderedDict(zip(cols, ([], [], [], [None], [], [None], [], []))))\n",
    "    inputs = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: D:/Python/Anaconda3/envs/py3_5: Is a directory\n",
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:/Python/Anaconda3/envs/py3_5: cannot execute: Is a directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "    --package-path recomm_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir model/census\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n",
      "INFO:tensorflow:Using config: {'_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F871BA2860>, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_is_chief': True, '_model_dir': 'model/census', '_master': '', '_num_ps_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_task_type': 'worker', '_session_config': None, '_num_worker_replicas': 1, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5}\r\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1\r\n",
      "INFO:tensorflow:Saving checkpoints for 2 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:step = 2, loss = 23.01487\r\n",
      "INFO:tensorflow:global_step/sec: 40.1861\r\n",
      "INFO:tensorflow:step = 102, loss = 16.039494 (2.499 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5457\r\n",
      "INFO:tensorflow:step = 202, loss = 18.856514 (2.139 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.3414\r\n",
      "INFO:tensorflow:step = 302, loss = 19.34407 (2.258 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.8991\r\n",
      "INFO:tensorflow:step = 402, loss = 14.1604185 (2.224 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.0182\r\n",
      "INFO:tensorflow:step = 502, loss = 32.08555 (2.174 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 48.1635\r\n",
      "INFO:tensorflow:step = 602, loss = 14.999886 (2.076 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 47.889\r\n",
      "INFO:tensorflow:step = 702, loss = 15.736288 (2.089 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5455\r\n",
      "INFO:tensorflow:step = 802, loss = 12.467914 (2.149 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 45.2024\r\n",
      "INFO:tensorflow:step = 902, loss = 16.449446 (2.211 sec)\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:Loss for final step: 16.14082.\r\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-03:12:37\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Evaluation [10/100]\r\n",
      "INFO:tensorflow:Evaluation [20/100]\r\n",
      "INFO:tensorflow:Evaluation [30/100]\r\n",
      "INFO:tensorflow:Evaluation [40/100]\r\n",
      "INFO:tensorflow:Evaluation [50/100]\r\n",
      "INFO:tensorflow:Evaluation [60/100]\r\n",
      "INFO:tensorflow:Evaluation [70/100]\r\n",
      "INFO:tensorflow:Evaluation [80/100]\r\n",
      "INFO:tensorflow:Evaluation [90/100]\r\n",
      "INFO:tensorflow:Evaluation [100/100]\r\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-03:12:41\r\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.80275, accuracy_baseline = 0.76325, auc = 0.86648417, auc_precision_recall = 0.64984035, average_loss = 0.5127518, global_step = 1000, label/mean = 0.23675, loss = 20.510073, prediction/mean = 0.26840857\r\n",
      "INFO:tensorflow:Performing the final export in the end of training.\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\r\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "WARNING:tensorflow:Export includes no default signature!\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 170, in <module>\r\n",
      "    run_experiment(hparams)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 56, in run_experiment\r\n",
      "    eval_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 432, in train_and_evaluate\r\n",
      "    executor.run_local()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 618, in run_local\r\n",
      "    metrics = evaluator.evaluate_and_export()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 789, in evaluate_and_export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 821, in _export_eval_result\r\n",
      "    is_the_final_export=is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 168, in export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 115, in export\r\n",
      "    checkpoint_path=checkpoint_path)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 560, in export_savedmodel\r\n",
      "    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 90, in __init__\r\n",
      "    file_io.recursive_create_dir(self._export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 368, in recursive_create_dir\r\n",
      "    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n",
      "    c_api.TF_GetCode(self.status.status))\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: model/census\\export\\census\\temp-b'1518145966'; No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/tensorflow_estimator/census-demo\n",
    "TRAIN_FILE=data/adult.data.csv\n",
    "EVAL_FILE=data/adult.test.csv\n",
    "OUTPUT_DIR=model/census\n",
    "python -m trainer.task --train-files $TRAIN_FILE \\\n",
    "                       --eval-files $EVAL_FILE \\\n",
    "                       --job-dir $OUTPUT_DIR \\\n",
    "                       --train-steps 1000 \\\n",
    "                       --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:\\Python\\Anaconda3\\envs\\py2_7\\python.exe: not found\n",
      "bash: line 10: --: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/recomm_prod\n",
    "TRAIN_FILE=trainer/data/movielens.tr\n",
    "EVAL_FILE=trainer/data/movielens.vl\n",
    "OUTPUT_DIR=trainer/model/reco_mf_dnn\n",
    "\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --job-dir $OUTPUT_DIR\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    labels = tf.constant(np.ones([10, 8]))\n",
    "    pred = tf.concat([tf.Variable(tf.ones(shape=[1, 8]), trainable=False), tf.Variable(tf.truncated_normal([9, 8]))], 0)\n",
    "    loss = tf.losses.mean_squared_error(predictions=pred, labels=labels)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(pred.eval())\n",
    "        for i in range(1000):\n",
    "            sess.run([train_op])\n",
    "        print()\n",
    "        print(pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
