{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, seaborn as sns, json, time, csv, datetime as dt\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, matplotlib.pyplot as plt, scipy.sparse as sp\n",
    "from pprint import pprint\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, minmax_scale\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# classpath\n",
    "ctx = os.path.abspath('..').replace('\\\\', '/')\n",
    "cps = [ctx]\n",
    "_ = [sys.path.insert(0, cp) for cp in cps if cp not in sys.path]\n",
    "\n",
    "# data path\n",
    "datapath = '/'.join([ctx, 'data'])\n",
    "\n",
    "seed = 88\n",
    "utils = reload('trainer.utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "ratings = pd.read_csv(\"{}/ml-latest-small/ratings.csv\".format(datapath))\n",
    "ratings['timestamp'] = ratings.timestamp.map(dt.datetime.fromtimestamp).map(str)\n",
    "ratings['ori_rating'] = ratings['rating']\n",
    "ratings['rating'] = (ratings.rating >= 4).astype(int)\n",
    "tr, te = utils.split_by_ratio(ratings)\n",
    "\n",
    "movies = pd.read_csv(\"{}/ml-latest-small/movies.csv\".format(datapath))\n",
    "avg_rt = ratings.groupby(\"movieId\", as_index=False).ori_rating.mean().rename(index=str, columns={'ori_rating': 'avg_rating'})\n",
    "movies = movies.merge(avg_rt, how='left', on='movieId')\n",
    "# movies.avg_rating.fillna(ratings.rating.mean())\n",
    "movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "# movies[\"year\"] = minmax_scale(movies.year.fillna(movies.year.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\",\n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"timestamp\",\n",
    "             \"rating\"]\n",
    "    \n",
    "    list2str = lambda lst: ','.join(map(str, lst))\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                query_hist = df.movieId[:i].tolist() + df.movieId[i + 1:].tolist()\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "            else:\n",
    "                tr_hist = set(user_movies_hist.tolist())\n",
    "                query_hist = list(tr_hist - set([int(r.movieId)]))\n",
    "                query_hist = list2str(query_hist)\n",
    "                queue.append([int(r.userId), query_hist, r.genres, r.avg_rating, r.year, int(r.movieId), r.timestamp, r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "    \n",
    "tr_merged = preprocess(tr, movies)\n",
    "tr_merged.to_csv('./tr.raw.movielens.csv', index=False, header=None)\n",
    "\n",
    "te_merged = preprocess(te, movies, tr, is_train=False)\n",
    "te_merged.to_csv('./te.raw.movielens.csv', index=False, header=None)\n",
    "# 合併成一個檔案\n",
    "merged = pd.concat([tr_merged, te_merged], ignore_index=True)\n",
    "merged.to_csv('./merged_movielens.csv', index=False, header=None)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Transform Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to parse D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'] ... \n",
      "[D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\merged_movielens.csv]: process take time 0:00:59.478155\n",
      "2018-02-13 11:02:02,449 - ctrl - INFO - foo: gen_data take time 0:01:03.828361\n",
      "{'err_cde': 0}\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "reload('trainer.service')\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "print(ctrl.gen_data(hparam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:04:00,539 - Ctrl - WARNING - parsed config [D:/Python/notebook/recomm_prod/repo\\foo\\data\\parsed.yaml] not found\n",
      "2018-02-14 12:04:00,541 - Ctrl - INFO - foo: try to generate training data...\n",
      "2018-02-14 12:04:00,543 - Loader - INFO - try to parse D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "2018-02-14 12:04:03,130 - Loader - INFO - try to transform ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'] ... \n",
      "[D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\merged_movielens.csv]: process take time 0:01:02.248726\n",
      "2018-02-14 12:05:06,191 - Service - INFO - hparam: {'conf_path': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/movielens.yaml',\n",
      " 'data_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data',\n",
      " 'dim': 16,\n",
      " 'eval_name': 'foo',\n",
      " 'eval_steps': 233,\n",
      " 'export_name': 'export_foo',\n",
      " 'job_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model',\n",
      " 'n_batch': 128,\n",
      " 'override': True,\n",
      " 'parsed_conf_path': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\parsed.yaml',\n",
      " 'pid': 'foo',\n",
      " 'raw_dir': 'D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws',\n",
      " 'raw_paths': ['D:/Python/notebook/recomm_prod/data/foo/user_supplied/raws\\\\merged_movielens.csv'],\n",
      " 'repo': 'D:/Python/notebook/recomm_prod/repo\\\\foo',\n",
      " 'save_every_steps': None,\n",
      " 'train_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.tr',\n",
      " 'train_steps': 1647,\n",
      " 'valid_file': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\data\\\\data.vl'}\n",
      "\n",
      "BestScoreExporter init\n",
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_num_worker_replicas': 1, '_task_id': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': 88, '_service': None, '_save_checkpoints_steps': None, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258', '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_save_checkpoints_secs': 600, '_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023AA1F767F0>, '_keep_checkpoint_every_n_hours': 10000}\n",
      "2018-02-14 12:05:06,197 - tensorflow - INFO - Using config: {'_task_type': 'worker', '_num_worker_replicas': 1, '_task_id': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': 88, '_service': None, '_save_checkpoints_steps': None, '_model_dir': 'D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258', '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_save_checkpoints_secs': 600, '_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023AA1F767F0>, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "2018-02-14 12:05:06,200 - tensorflow - INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2018-02-14 12:05:06,203 - tensorflow - INFO - Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:05:07,647 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:14,191 - tensorflow - INFO - Saving checkpoints for 1 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.0883753\n",
      "2018-02-14 12:05:15,376 - tensorflow - INFO - step = 1, loss = 1.0883753\n",
      "INFO:tensorflow:global_step/sec: 30.5558\n",
      "2018-02-14 12:05:18,656 - tensorflow - INFO - global_step/sec: 30.5558\n",
      "INFO:tensorflow:step = 101, loss = 0.7336418 (3.299 sec)\n",
      "2018-02-14 12:05:18,674 - tensorflow - INFO - step = 101, loss = 0.7336418 (3.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6867\n",
      "2018-02-14 12:05:20,836 - tensorflow - INFO - global_step/sec: 45.6867\n",
      "INFO:tensorflow:step = 201, loss = 0.61456645 (2.166 sec)\n",
      "2018-02-14 12:05:20,840 - tensorflow - INFO - step = 201, loss = 0.61456645 (2.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3551\n",
      "2018-02-14 12:05:23,748 - tensorflow - INFO - global_step/sec: 34.3551\n",
      "INFO:tensorflow:step = 301, loss = 0.67162496 (2.916 sec)\n",
      "2018-02-14 12:05:23,756 - tensorflow - INFO - step = 301, loss = 0.67162496 (2.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9186\n",
      "2018-02-14 12:05:27,605 - tensorflow - INFO - global_step/sec: 25.9186\n",
      "INFO:tensorflow:step = 401, loss = 0.4971138 (3.853 sec)\n",
      "2018-02-14 12:05:27,609 - tensorflow - INFO - step = 401, loss = 0.4971138 (3.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.7209\n",
      "2018-02-14 12:05:30,328 - tensorflow - INFO - global_step/sec: 36.7209\n",
      "INFO:tensorflow:step = 501, loss = 0.6030406 (2.722 sec)\n",
      "2018-02-14 12:05:30,331 - tensorflow - INFO - step = 501, loss = 0.6030406 (2.722 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 549 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:32,330 - tensorflow - INFO - Saving checkpoints for 549 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.55781287.\n",
      "2018-02-14 12:05:33,417 - tensorflow - INFO - Loss for final step: 0.55781287.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:05:34\n",
      "2018-02-14 12:05:34,500 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:05:34\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:34,687 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:05:35,362 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:05:35,560 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:05:35,771 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:05:35,975 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:05:36,226 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:05:36,515 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:05:36,747 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:05:36,961 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:05:37,162 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:05:37,453 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:05:37,478 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:05:37\n",
      "2018-02-14 12:05:37,500 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:05:37\n",
      "INFO:tensorflow:Saving dict for global step 549: auc = 0.7915665, global_step = 549, loss = 0.57296175\n",
      "2018-02-14 12:05:37,502 - tensorflow - INFO - Saving dict for global step 549: auc = 0.7915665, global_step = 549, loss = 0.57296175\n",
      "2018-02-14 12:05:38,531 - BestScoreExporter - INFO - nice eval loss: 0.5729617476463318, export to pb\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "2018-02-14 12:05:38,794 - tensorflow - INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "2018-02-14 12:05:38,797 - tensorflow - INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "2018-02-14 12:05:38,799 - tensorflow - INFO - Signatures INCLUDED in export for Predict: ['outputs', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:38,895 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "2018-02-14 12:05:39,079 - tensorflow - INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2018-02-14 12:05:39,081 - tensorflow - INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258\\\\export\\\\export_foo\\\\temp-b'1518581138'\\\\saved_model.pb\"\n",
      "2018-02-14 12:05:39,729 - tensorflow - INFO - SavedModel written to: b\"D:/Python/notebook/recomm_prod/repo\\\\foo\\\\model_1518581106.1947258\\\\export\\\\export_foo\\\\temp-b'1518581138'\\\\saved_model.pb\"\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:05:41,137 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "2018-02-14 12:05:41,536 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-549\n",
      "INFO:tensorflow:Saving checkpoints for 550 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:05:45,134 - tensorflow - INFO - Saving checkpoints for 550 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 550, loss = 0.5548116\n",
      "2018-02-14 12:05:46,575 - tensorflow - INFO - step = 550, loss = 0.5548116\n",
      "INFO:tensorflow:global_step/sec: 32.4444\n",
      "2018-02-14 12:05:49,653 - tensorflow - INFO - global_step/sec: 32.4444\n",
      "INFO:tensorflow:step = 650, loss = 0.71449625 (3.083 sec)\n",
      "2018-02-14 12:05:49,658 - tensorflow - INFO - step = 650, loss = 0.71449625 (3.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3018\n",
      "2018-02-14 12:05:51,814 - tensorflow - INFO - global_step/sec: 46.3018\n",
      "INFO:tensorflow:step = 750, loss = 0.45504737 (2.161 sec)\n",
      "2018-02-14 12:05:51,819 - tensorflow - INFO - step = 750, loss = 0.45504737 (2.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6446\n",
      "2018-02-14 12:05:54,619 - tensorflow - INFO - global_step/sec: 35.6446\n",
      "INFO:tensorflow:step = 850, loss = 0.47156 (2.803 sec)\n",
      "2018-02-14 12:05:54,623 - tensorflow - INFO - step = 850, loss = 0.47156 (2.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.6189\n",
      "2018-02-14 12:05:58,522 - tensorflow - INFO - global_step/sec: 25.6189\n",
      "INFO:tensorflow:step = 950, loss = 0.43427843 (3.903 sec)\n",
      "2018-02-14 12:05:58,526 - tensorflow - INFO - step = 950, loss = 0.43427843 (3.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.2669\n",
      "2018-02-14 12:06:01,280 - tensorflow - INFO - global_step/sec: 36.2669\n",
      "INFO:tensorflow:step = 1050, loss = 0.5861499 (2.759 sec)\n",
      "2018-02-14 12:06:01,285 - tensorflow - INFO - step = 1050, loss = 0.5861499 (2.759 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1098 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:03,369 - tensorflow - INFO - Saving checkpoints for 1098 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5433118.\n",
      "2018-02-14 12:06:05,289 - tensorflow - INFO - Loss for final step: 0.5433118.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:06:05\n",
      "2018-02-14 12:06:05,883 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:06:05\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "2018-02-14 12:06:06,070 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:06:06,759 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:06:06,940 - tensorflow - INFO - Evaluation [46/233]\n",
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:06:07,159 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:06:07,365 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:06:07,597 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:06:07,939 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:06:08,176 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:06:08,415 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:06:08,636 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:06:08,954 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:06:08,990 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:06:09\n",
      "2018-02-14 12:06:09,025 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:06:09\n",
      "INFO:tensorflow:Saving dict for global step 1098: auc = 0.7877959, global_step = 1098, loss = 0.59375995\n",
      "2018-02-14 12:06:09,027 - tensorflow - INFO - Saving dict for global step 1098: auc = 0.7877959, global_step = 1098, loss = 0.59375995\n",
      "2018-02-14 12:06:09,033 - BestScoreExporter - INFO - bad eval loss: 0.5937599539756775\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2018-02-14 12:06:10,863 - tensorflow - INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "2018-02-14 12:06:11,245 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1098\n",
      "INFO:tensorflow:Saving checkpoints for 1099 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:15,225 - tensorflow - INFO - Saving checkpoints for 1099 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:step = 1099, loss = 0.47767633\n",
      "2018-02-14 12:06:16,563 - tensorflow - INFO - step = 1099, loss = 0.47767633\n",
      "INFO:tensorflow:global_step/sec: 31.884\n",
      "2018-02-14 12:06:19,699 - tensorflow - INFO - global_step/sec: 31.884\n",
      "INFO:tensorflow:step = 1199, loss = 0.6478784 (3.140 sec)\n",
      "2018-02-14 12:06:19,703 - tensorflow - INFO - step = 1199, loss = 0.6478784 (3.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3961\n",
      "2018-02-14 12:06:21,913 - tensorflow - INFO - global_step/sec: 45.3961\n",
      "INFO:tensorflow:step = 1299, loss = 0.4191215 (2.229 sec)\n",
      "2018-02-14 12:06:21,932 - tensorflow - INFO - step = 1299, loss = 0.4191215 (2.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2044\n",
      "2018-02-14 12:06:24,742 - tensorflow - INFO - global_step/sec: 35.2044\n",
      "INFO:tensorflow:step = 1399, loss = 0.44278777 (2.813 sec)\n",
      "2018-02-14 12:06:24,746 - tensorflow - INFO - step = 1399, loss = 0.44278777 (2.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.5597\n",
      "2018-02-14 12:06:28,655 - tensorflow - INFO - global_step/sec: 25.5597\n",
      "INFO:tensorflow:step = 1499, loss = 0.35131794 (3.915 sec)\n",
      "2018-02-14 12:06:28,662 - tensorflow - INFO - step = 1499, loss = 0.35131794 (3.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3197\n",
      "2018-02-14 12:06:31,407 - tensorflow - INFO - global_step/sec: 36.3197\n",
      "INFO:tensorflow:step = 1599, loss = 0.50131756 (2.754 sec)\n",
      "2018-02-14 12:06:31,415 - tensorflow - INFO - step = 1599, loss = 0.50131756 (2.754 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "2018-02-14 12:06:33,399 - tensorflow - INFO - Saving checkpoints for 1647 into D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40266097.\n",
      "2018-02-14 12:06:35,243 - tensorflow - INFO - Loss for final step: 0.40266097.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-14-04:06:35\n",
      "2018-02-14 12:06:35,758 - tensorflow - INFO - Starting evaluation at 2018-02-14-04:06:35\n",
      "INFO:tensorflow:Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1647\n",
      "2018-02-14 12:06:35,945 - tensorflow - INFO - Restoring parameters from D:/Python/notebook/recomm_prod/repo\\foo\\model_1518581106.1947258\\model.ckpt-1647\n",
      "INFO:tensorflow:Evaluation [23/233]\n",
      "2018-02-14 12:06:36,634 - tensorflow - INFO - Evaluation [23/233]\n",
      "INFO:tensorflow:Evaluation [46/233]\n",
      "2018-02-14 12:06:36,818 - tensorflow - INFO - Evaluation [46/233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [69/233]\n",
      "2018-02-14 12:06:37,018 - tensorflow - INFO - Evaluation [69/233]\n",
      "INFO:tensorflow:Evaluation [92/233]\n",
      "2018-02-14 12:06:37,237 - tensorflow - INFO - Evaluation [92/233]\n",
      "INFO:tensorflow:Evaluation [115/233]\n",
      "2018-02-14 12:06:37,477 - tensorflow - INFO - Evaluation [115/233]\n",
      "INFO:tensorflow:Evaluation [138/233]\n",
      "2018-02-14 12:06:37,784 - tensorflow - INFO - Evaluation [138/233]\n",
      "INFO:tensorflow:Evaluation [161/233]\n",
      "2018-02-14 12:06:38,053 - tensorflow - INFO - Evaluation [161/233]\n",
      "INFO:tensorflow:Evaluation [184/233]\n",
      "2018-02-14 12:06:38,327 - tensorflow - INFO - Evaluation [184/233]\n",
      "INFO:tensorflow:Evaluation [207/233]\n",
      "2018-02-14 12:06:38,581 - tensorflow - INFO - Evaluation [207/233]\n",
      "INFO:tensorflow:Evaluation [230/233]\n",
      "2018-02-14 12:06:38,865 - tensorflow - INFO - Evaluation [230/233]\n",
      "INFO:tensorflow:Evaluation [233/233]\n",
      "2018-02-14 12:06:38,893 - tensorflow - INFO - Evaluation [233/233]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-04:06:38\n",
      "2018-02-14 12:06:38,914 - tensorflow - INFO - Finished evaluation at 2018-02-14-04:06:38\n",
      "INFO:tensorflow:Saving dict for global step 1647: auc = 0.75802153, global_step = 1647, loss = 0.65210867\n",
      "2018-02-14 12:06:38,916 - tensorflow - INFO - Saving dict for global step 1647: auc = 0.75802153, global_step = 1647, loss = 0.65210867\n",
      "2018-02-14 12:06:38,920 - BestScoreExporter - INFO - bad eval loss: 0.6521086692810059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_mf_dnn.reco_mf_dnn_est.ModelMfDNN at 0x23ae3759860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "utils = reload('trainer.utils.utils')\n",
    "reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "\n",
    "ctrl.train(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Cmd Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: recomm_movielens_15\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Google\\Cloud SDK\\google-cloud-sdk\\lib\\googlecloudsdk\\core\\util\\files.py:622: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  for chunk in iter(lambda: fp.read(4096), ''):\n",
      "Job [recomm_movielens_15] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe recomm_movielens_15\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs recomm_movielens_15\n"
     ]
    }
   ],
   "source": [
    "!cd D:/Python/notebook/recomm_prod && \\\n",
    "gcloud ml-engine jobs submit training recomm_movielens_15 \\\n",
    "    --job-dir gs://recomm-job/foo/model \\\n",
    "    --runtime-version 1.4 \\\n",
    "    --module-name trainer.ctrl \\\n",
    "    --package-path trainer \\\n",
    "    --region asia-east1 \\\n",
    "    --config config.yaml \\\n",
    "    -- \\\n",
    "    --method train \\\n",
    "    --conf-path gs://recomm-job/foo/data/user_supplied/movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2018-02-22T06:32:40Z'\n",
      "endTime: '2018-02-22T06:33:44Z'\n",
      "jobId: recomm_movielens_15\n",
      "startTime: '2018-02-22T06:33:13Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --method\n",
      "  - train\n",
      "  - --conf-path\n",
      "  - gs://recomm-job/foo/data/user_supplied/movielens.yaml\n",
      "  jobDir: gs://recomm-job/foo/model\n",
      "  packageUris:\n",
      "  - gs://recomm-job/foo/model/packages/0a10348c08fa5c2fcbcd7017d1f01ba18be758689a6f860fc725b64bff1f2955/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.ctrl\n",
      "  pythonVersion: '3.5'\n",
      "  region: asia-east1\n",
      "  runtimeVersion: '1.4'\n",
      "trainingOutput:\n",
      "  consumedMLUnits: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/ml/jobs/recomm_movielens_15?project=training-recommendation-engine\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Frecomm_movielens_15&project=training-recommendation-engine\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs describe recomm_movielens_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build\\lib\n",
      "creating build\\lib\\trainer\n",
      "copying trainer\\ctrl.py -> build\\lib\\trainer\n",
      "copying trainer\\env.py -> build\\lib\\trainer\n",
      "copying trainer\\reco_mf_dnn.py -> build\\lib\\trainer\n",
      "copying trainer\\reco_mf_dnn_est.py -> build\\lib\\trainer\n",
      "copying trainer\\service.py -> build\\lib\\trainer\n",
      "copying trainer\\__init__.py -> build\\lib\\trainer\n",
      "creating build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\flex.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\kkbox_data.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\neg_sampling_data.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\utils.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\utils\\__init__.py -> build\\lib\\trainer\\utils\n",
      "copying trainer\\logging.yaml -> build\\lib\\trainer\n"
     ]
    }
   ],
   "source": [
    "!cd .. && python setup.py build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Python Client API Submit Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Google\\Cloud SDK\\google-cloud-sdk\\lib\\googlecloudsdk\\core\\util\\files.py:622: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\r\n",
      "  for chunk in iter(lambda: fp.read(4096), ''):\r\n",
      "ERROR: (gcloud.ml-engine.jobs.submit.training) Resource in project [training-recommendation-engine] is the subject of a conflict: Field: job.job_id Error: A job with this id already exists.\r\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\r\n",
      "  fieldViolations:\r\n",
      "  - description: A job with this id already exists.\r\n",
      "    field: job.job_id\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "print( ctrl.train_submit(hparam) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25 20:03:52,366 - Loader - INFO - try to parse gs://movielens-foo/user_supplied/movielens.yaml (user supplied) ...\n",
      "2018-02-25 20:04:10,800 - Ctrl - INFO - foo: gen_data take time 0:00:18.854078\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "env = reload('trainer.env')\n",
    "reload('trainer.utils.flex')\n",
    "reload('trainer.service')\n",
    "\n",
    "ctrl = reload('trainer.ctrl').Ctrl.instance\n",
    "hparam = HParams(conf_path='gs://movielens-foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "loader = ctrl.gen_data(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = codecs.open('./1.txt', 'a', 'utf-8')\n",
    "r.stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': [{'id': 'user_id', 'm_dtype': 'catg'},\n",
       "  {'id': 'query_movie_ids',\n",
       "   'is_multi': True,\n",
       "   'm_dtype': 'catg',\n",
       "   'sep': ',',\n",
       "   'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "  {'id': 'genres',\n",
       "   'is_multi': True,\n",
       "   'm_dtype': 'catg',\n",
       "   'sep': '|',\n",
       "   'vocabs_path': 'gs://movielens-foo/user_supplied/genres.vocab'},\n",
       "  {'id': 'avg_rating', 'm_dtype': 'cont'},\n",
       "  {'id': 'year', 'm_dtype': 'cont'},\n",
       "  {'id': 'candidate_movie_id',\n",
       "   'm_dtype': 'catg',\n",
       "   'vocabs_path': 'gs://movielens-foo/user_supplied/item.vocab'},\n",
       "  {'date_format': '%Y-%m-%d %H:%M:%S',\n",
       "   'id': 'timestamp',\n",
       "   'm_dtype': 'datetime'},\n",
       "  {'id': 'rating', 'm_dtype': 'catg'}],\n",
       " 'item': ['genres', 'avg_rating', 'year', 'candidate_movie_id'],\n",
       " 'label': ['rating'],\n",
       " 'override': True,\n",
       " 'project_id': 'foo',\n",
       " 'raw_dir': 'gs://movielens-foo/user_supplied/raws',\n",
       " 'user': ['query_movie_ids']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from io import BytesIO\n",
    "\n",
    "utils = reload('trainer.utils.utils')\n",
    "blob = utils.gcs_blob('gs://movielens-foo/user_supplied/movielens.yaml')\n",
    "yaml.load(blob.download_as_string())\n",
    "# sio = BytesIO(blob.download_as_string())\n",
    "# pd.read_csv(sio, header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改GCS movielens.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "env = reload('trainer.env')\n",
    "bucket = env.bucket('movielens-foo')\n",
    "blob = bucket.get_blob('user_supplied/movielens.yaml')\n",
    "blob.upload_from_file(open('../data/foo/user_supplied/movielens.yaml', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Python API Credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import storage\n",
    "\n",
    "authpath = '../auth.json'\n",
    "project = 'training-recommendation-engine'\n",
    "cred = GoogleCredentials.from_stream(authpath)\n",
    "svc = discovery.build('ml', 'v1', credentials=cred)\n",
    "\n",
    "st_client = storage.Client.from_service_account_json(authpath)\n",
    "bucket = st_client.get_bucket('recomm-job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Blob: movielens-foo, user_supplied/genres.vocab>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client().get_bucket('movielens-foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "\n",
    "blob = bucket.get_blob('user_supplied/raws/merged_movielens.csv')\n",
    "sio = BytesIO()\n",
    "blob.download_to_file(sio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,\"1953,2105,31,1029,1061,1129,1263,1287,1293,1339,1343,1371,1405\",Drama,4.260869565217392,1989.0,1172,2009-12-14 10:53:25,1\\r\\n1,\"1172,2105,31,1029,1061,1129,1263,1287,1293,1339,1343,1371,1405\",Action|Crime|Thriller,4.021739130434782,1971.0,1953,2009-12-14 10:53:11,1\\r\\n1,\"1172,1953,31,1029,1061,1129,1263,1287,1293,1339,1343,1371,1405\",Action|Adventure|Sci-Fi,3.478723404255319,1982.0,2105,2009-12-14 10:52:19,1\\r\\n1,\"1172,1953,2105,1029,1061,1129,1263,1287,1293,1339,1343,1371,1405\",Drama,3.1785714285714284,1995.0,31,2009-12-14 10:52:24,0\\r\\n1,\"1172,1953,2105,31,1061,1129,1263,1287,1293,1339,1343,1371,1405\",Animation|Children|Drama|Musical,3.7023809523809526,1941.0,1029,2009-12-14 10:52:59,0\\r\\n1,\"1172,1953,2105,31,1029,1129,1263,1287,1293,1339,1343,1371,1405\",Thriller,3.5454545454545454,1996.0,1061,2009-12-14 10:53:02,0\\r\\n1,\"1172,1953,2105,31,1029,1061,1263,1287,1293,1339,1343,1371,1405\",Action|Adventure|Sci-Fi|Thriller,3.3125,1981.0,1129,2009-12-14 10:53:05,0\\r\\n1,\"1172,1953,2105,31,1029,1061,1129'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sio.getvalue().decode('utf-8')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required parameter \"body\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-08673c4d57fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# b = a.get(name='projects/{}/jobs/recomm_movielens_13'.format(project))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'projects/{}/jobs/recomm_movielens_13'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\googleapiclient\\discovery.py\u001b[0m in \u001b[0;36mmethod\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m         if name not in _PAGE_TOKEN_NAMES or _findPageTokenName(\n\u001b[0;32m    731\u001b[0m             _methodProperties(methodDesc, schema, 'response')):\n\u001b[1;32m--> 732\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing required parameter \"%s\"'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpattern_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Missing required parameter \"body\""
     ]
    }
   ],
   "source": [
    "# b = a.get(name='projects/{}/jobs/recomm_movielens_13'.format(project))\n",
    "a = svc.projects().jobs()\n",
    "b = a.cancel(name='projects/{}/jobs/recomm_movielens_13'.format(project))\n",
    "b.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc.projects().jobs().get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Restful predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1953,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-12-14 10:53:25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,2105,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>4.021739</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>2009-12-14 10:53:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,31,1029,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>3.478723</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2009-12-14 10:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,1029,1061,1129,1263,1287,1293,1...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2009-12-14 10:52:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172,1953,2105,31,1061,1129,1263,1287,1293,133...</td>\n",
       "      <td>Animation|Children|Drama|Musical</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>2009-12-14 10:52:59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        1  1953,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "1        1  1172,2105,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "2        1  1172,1953,31,1029,1061,1129,1263,1287,1293,133...   \n",
       "3        1  1172,1953,2105,1029,1061,1129,1263,1287,1293,1...   \n",
       "4        1  1172,1953,2105,31,1061,1129,1263,1287,1293,133...   \n",
       "\n",
       "                             genres  avg_rating    year  candidate_movie_id  \\\n",
       "0                             Drama    4.260870  1989.0                1172   \n",
       "1             Action|Crime|Thriller    4.021739  1971.0                1953   \n",
       "2           Action|Adventure|Sci-Fi    3.478723  1982.0                2105   \n",
       "3                             Drama    3.178571  1995.0                  31   \n",
       "4  Animation|Children|Drama|Musical    3.702381  1941.0                1029   \n",
       "\n",
       "             timestamp  rating  \n",
       "0  2009-12-14 10:53:25       1  \n",
       "1  2009-12-14 10:53:11       1  \n",
       "2  2009-12-14 10:52:19       1  \n",
       "3  2009-12-14 10:52:24       0  \n",
       "4  2009-12-14 10:52:59       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flex = reload('utils.flex')\n",
    "with codecs.open('../repo/foo/data/parsed.yaml', 'r', 'utf-8') as r:\n",
    "    schema = flex.Schema.unserialize(r)\n",
    "\n",
    "merged = pd.read_csv('../data/foo/user_supplied/raws/merged_movielens.csv', names=schema.raw_cols)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_movie_ids', 'year', 'candidate_movie_id', 'avg_rating', 'genres', 'user_id'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'user_id': merged.query('user_id == 22').iloc[[0]].user_id.tolist(),\n",
    "    'query_movie_ids': merged.query('user_id == 22').iloc[[0]].query_movie_ids.tolist(),\n",
    "}\n",
    "\n",
    "items = movies.rename(index=str, columns={\"movieId\": \"candidate_movie_id\"})\\\n",
    "              .drop('title', 1) # .to_dict('list')\n",
    "items.loc[:, 'candidate_movie_id'] = items.candidate_movie_id.astype(str)\n",
    "items = items.to_dict('list')\n",
    "data.update(items)\n",
    "with codecs.open('predic.data.json', 'w', 'utf-8') as w:\n",
    "    json.dump(data, w)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training.python.training.hparam import HParams\n",
    "reload('utils')\n",
    "reload('utils.flex')\n",
    "reload('service')\n",
    "ctrl = reload('ctrl').Ctrl.instance\n",
    "\n",
    "hparam = HParams(conf_path='{}/foo/user_supplied/movielens.yaml'.format(datapath))\n",
    "hparam.add_hparam('data', 'predic.data.json')\n",
    "ret = ctrl.predict(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.ml-engine.local.predict) Something has gone really wrong; we can't find a valid Python executable on your PATH.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local predict \\\n",
    "--model-dir D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "--json-instances tmp.20180221155312370200.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\python.exe: can't open file '/cygdrive/d/google-cloud-sdk/lib/gcloud.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local predict \\\n",
    "--model-dir D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "--json-instances tmp.20180221155312370200.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-151bc15fc3a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmulti_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'query_movie_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'genres'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# features = OrderedDict(zip(cols, data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "cols = list(data.keys())\n",
    "multi_cols = ('query_movie_ids', 'genres')\n",
    "\n",
    "def trans(features):\n",
    "    # features = OrderedDict(zip(cols, data))\n",
    "    print( schema.col_states_['query_movie_ids'].transform( features['query_movie_ids'] ) )\n",
    "    # for col in multi_cols:\n",
    "    #     features[col] = tf.string_to_number(tf.string_split(features[col], ',').values, out_type=tf.int32)\n",
    "    return features\n",
    "\n",
    "def add_seq_cols(feat):\n",
    "    for m_col in multi_cols:\n",
    "        name = '{}_len'.format(m_col)\n",
    "        feat[name] = tf.size(feat[m_col])\n",
    "        cols.append(name)\n",
    "    return feat\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.Dataset.from_tensors(data)\n",
    "    dataset = dataset.map(trans, num_parallel_calls=4)\n",
    "    dataset = dataset.map(add_seq_cols, num_parallel_calls=4)\n",
    "    print('cols', cols)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.padded_batch(5, OrderedDict(zip(cols, ([], [], [], [None], [], [None], [], []))))\n",
    "    inputs = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: D:/Python/Anaconda3/envs/py3_5: Is a directory\n",
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:/Python/Anaconda3/envs/py3_5: cannot execute: Is a directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=D:/Python/notebook/recomm_prod/repo/foo/model_1518581106.1947258/export/export_foo/1518581138 \\\n",
    "    --package-path recomm_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir model/census\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n",
      "INFO:tensorflow:Using config: {'_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F871BA2860>, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_is_chief': True, '_model_dir': 'model/census', '_master': '', '_num_ps_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_task_type': 'worker', '_session_config': None, '_num_worker_replicas': 1, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5}\r\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1\r\n",
      "INFO:tensorflow:Saving checkpoints for 2 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:step = 2, loss = 23.01487\r\n",
      "INFO:tensorflow:global_step/sec: 40.1861\r\n",
      "INFO:tensorflow:step = 102, loss = 16.039494 (2.499 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5457\r\n",
      "INFO:tensorflow:step = 202, loss = 18.856514 (2.139 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.3414\r\n",
      "INFO:tensorflow:step = 302, loss = 19.34407 (2.258 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 44.8991\r\n",
      "INFO:tensorflow:step = 402, loss = 14.1604185 (2.224 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.0182\r\n",
      "INFO:tensorflow:step = 502, loss = 32.08555 (2.174 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 48.1635\r\n",
      "INFO:tensorflow:step = 602, loss = 14.999886 (2.076 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 47.889\r\n",
      "INFO:tensorflow:step = 702, loss = 15.736288 (2.089 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 46.5455\r\n",
      "INFO:tensorflow:step = 802, loss = 12.467914 (2.149 sec)\r\n",
      "INFO:tensorflow:global_step/sec: 45.2024\r\n",
      "INFO:tensorflow:step = 902, loss = 16.449446 (2.211 sec)\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into model/census\\model.ckpt.\r\n",
      "INFO:tensorflow:Loss for final step: 16.14082.\r\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-03:12:37\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "INFO:tensorflow:Evaluation [10/100]\r\n",
      "INFO:tensorflow:Evaluation [20/100]\r\n",
      "INFO:tensorflow:Evaluation [30/100]\r\n",
      "INFO:tensorflow:Evaluation [40/100]\r\n",
      "INFO:tensorflow:Evaluation [50/100]\r\n",
      "INFO:tensorflow:Evaluation [60/100]\r\n",
      "INFO:tensorflow:Evaluation [70/100]\r\n",
      "INFO:tensorflow:Evaluation [80/100]\r\n",
      "INFO:tensorflow:Evaluation [90/100]\r\n",
      "INFO:tensorflow:Evaluation [100/100]\r\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-03:12:41\r\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.80275, accuracy_baseline = 0.76325, auc = 0.86648417, auc_precision_recall = 0.64984035, average_loss = 0.5127518, global_step = 1000, label/mean = 0.23675, loss = 20.510073, prediction/mean = 0.26840857\r\n",
      "INFO:tensorflow:Performing the final export in the end of training.\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\r\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'relationship': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'capital_loss': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=string>, 'workclass': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>, 'hours_per_week': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'gender': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital_status': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=string>}\r\n",
      "WARNING:tensorflow:Export includes no default signature!\r\n",
      "INFO:tensorflow:Restoring parameters from model/census\\model.ckpt-1000\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 170, in <module>\r\n",
      "    run_experiment(hparams)\r\n",
      "  File \"D:\\Python\\notebook\\tensorflow_estimator\\census-demo\\trainer\\task.py\", line 56, in run_experiment\r\n",
      "    eval_spec)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 432, in train_and_evaluate\r\n",
      "    executor.run_local()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 618, in run_local\r\n",
      "    metrics = evaluator.evaluate_and_export()\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 789, in evaluate_and_export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 821, in _export_eval_result\r\n",
      "    is_the_final_export=is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 168, in export\r\n",
      "    is_the_final_export)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\exporter.py\", line 115, in export\r\n",
      "    checkpoint_path=checkpoint_path)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 560, in export_savedmodel\r\n",
      "    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 90, in __init__\r\n",
      "    file_io.recursive_create_dir(self._export_dir)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 368, in recursive_create_dir\r\n",
      "    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n",
      "  File \"D:\\Python\\Anaconda3\\envs\\py3_5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n",
      "    c_api.TF_GetCode(self.status.status))\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: model/census\\export\\census\\temp-b'1518145966'; No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/tensorflow_estimator/census-demo\n",
    "TRAIN_FILE=data/adult.data.csv\n",
    "EVAL_FILE=data/adult.test.csv\n",
    "OUTPUT_DIR=model/census\n",
    "python -m trainer.task --train-files $TRAIN_FILE \\\n",
    "                       --eval-files $EVAL_FILE \\\n",
    "                       --job-dir $OUTPUT_DIR \\\n",
    "                       --train-steps 1000 \\\n",
    "                       --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cygdrive/d/google-cloud-sdk/bin/gcloud: line 113: exec: D:\\Python\\Anaconda3\\envs\\py2_7\\python.exe: not found\n",
      "bash: line 10: --: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd D:/Python/notebook/recomm_prod\n",
    "TRAIN_FILE=trainer/data/movielens.tr\n",
    "EVAL_FILE=trainer/data/movielens.vl\n",
    "OUTPUT_DIR=trainer/model/reco_mf_dnn\n",
    "\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --job-dir $OUTPUT_DIR\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_datasets(fpath_ary, schema, n_batch=128, n_epoch=1):\n",
    "    def to_dense(sp):\n",
    "        dense = tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values, '')\n",
    "        return tf.reshape(tf.to_int32(tf.string_to_number(dense)), [-1])\n",
    "\n",
    "    def to_sparse(dense):\n",
    "        idx = tf.where(tf.not_equal(dense, 0))\n",
    "        return tf.SparseTensor(indices=idx, dense_shape=dense.get_shape(), values=tf.gather_nd(dense, idx))\n",
    "\n",
    "    def parse_csv(value):\n",
    "        data = tf.decode_csv(value, record_defaults=defaults)\n",
    "        features = OrderedDict(zip(cols, data))\n",
    "        multi_cols = df_conf.query(\"{} == '{}' and {} == True\".format(schema.M_DTYPE, schema.CATG, schema.IS_MULTI)).id.values\n",
    "        for col in multi_cols:\n",
    "            features[col] = tf.string_split([features[col]], ',')\n",
    "            features[col] = to_dense(features[col])\n",
    "            # features['{}_lens'.format(col)] = tf.size(features[col])\n",
    "        return features \n",
    "    \n",
    "    df_conf = schema.df_conf_.query('{}.notnull()'.format(schema.TYPE))\n",
    "    cols = schema.cols\n",
    "    defaults = []\n",
    "    for _, r in df_conf.iterrows():\n",
    "        if r[schema.M_DTYPE] == schema.CATG:\n",
    "            defaults.append([''] if r[schema.IS_MULTI] else [0])\n",
    "        else:\n",
    "            defaults.append([])\n",
    "    dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "    has_multi = (df_conf[schema.M_DTYPE] == schema.CATG) & (df_conf[schema.IS_MULTI] == True)\n",
    "    if sum(has_multi):\n",
    "        multi_cols = df_conf[has_multi].id.values\n",
    "        dataset = dataset.padded_batch(n_batch, OrderedDict( zip(cols, tuple([None] if e else [] for e in has_multi))) )\n",
    "    else:\n",
    "        dataset = dataset.batch(n_batch)\n",
    "    dataset = dataset.shuffle(n_batch * 10, seed=seed).repeat(n_epoch)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, features.pop(schema.label[0])\n",
    "                                \n",
    "# tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    inputs = make_datasets(['./movielens.tr'], loader.schema, n_batch=30)\n",
    "    query_lens = tf.sequence_mask([1, 2, 3])\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            _, = sess.run([inputs])\n",
    "            # print( sess.run(inputs) )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns with tf.feature_column.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series(minmax_scale(np.random.normal(0, 1, size=1000)))\n",
    "a.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=1000, dtype=tf.int32)\n",
    "    user_id = tf.feature_column.embedding_column(user_id, dimension=8)\n",
    "    avg_rating = tf.feature_column.numeric_column('avg_rating')\n",
    "    columns = [user_id, avg_rating]\n",
    "    \n",
    "    def make_datasets(fpath_ary):\n",
    "        cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "        defaults = [[0], [''], [''], [], [], [0], []]\n",
    "\n",
    "        def parse_csv(value):\n",
    "            data = tf.decode_csv(value, record_defaults=defaults)\n",
    "            features = OrderedDict(zip(cols, data))\n",
    "            # print(features)\n",
    "            return features\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(fpath_ary)\n",
    "        dataset = (dataset.map(parse_csv, num_parallel_calls=4)\n",
    "                          .batch(3)\n",
    "                          # .padded_batch(3, OrderedDict(zip(cols, ([], [None], [None], [], [], [], []))))\n",
    "                          .shuffle(10, seed=seed)\n",
    "                          .repeat(1)\n",
    "                  )\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    inputs = make_datasets(['./te_processed.batch.csv'])\n",
    "    inputs = tf.feature_column.input_layer(inputs, columns)\n",
    "    # features = tf.parse_example(serialized_example, features=tf.feature_column.make_parse_example_spec(columns))\n",
    "    ctx = []\n",
    "    with tf.train.MonitoredTrainingSession() as sess:\n",
    "        while not sess.should_stop():\n",
    "            print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Make Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = ['user_id', 'query_movie_ids', 'genres', 'avg_rating', 'year', 'candidate_movie_id', 'rating']\n",
    "is_multi = [False, True, True, False, False, False, False]\n",
    "pd_dtypes = [int, str, str, float, float, int, float]\n",
    "types = ['int64_list', 'int64_list', 'int64_list', 'float_list', 'float_list', 'int64_list', 'float_list']\n",
    "tf_types = [tf.int64, tf.int64, tf.int64, tf.float32, tf.float32, tf.int64, tf.float32]\n",
    "def persist_example(fpath, tfpath):\n",
    "    with tf.python_io.TFRecordWriter(tfpath) as w:\n",
    "        for chunk in pd.read_csv(fpath, names=cols, dtype=dict(zip(cols, pd_dtypes)), chunksize=1000):\n",
    "            chunk['query_movie_ids'] = chunk.query_movie_ids.map(lambda r: map(int, r.split(',')))\n",
    "            chunk['genres'] = chunk.genres.map(lambda r: map(int, r.split(',')))\n",
    "            \n",
    "            for idx, r in chunk.iterrows():\n",
    "                ex = tf.train.Example()\n",
    "                for multi, col, tpe in zip(is_multi, cols, types):\n",
    "                    val = r[col]\n",
    "                    # ex.features.feature[col].int64_list or float_list or bytes_list\n",
    "                    feat_type = getattr(ex.features.feature[col], tpe)\n",
    "                    # extend function for multivalent columns, otherwise append\n",
    "                    append_or_extend = 'append' if not multi else 'extend'                    \n",
    "                    getattr(feat_type.value, append_or_extend)(val)\n",
    "                w.write(ex.SerializePartialToString())\n",
    "\n",
    "persist_example('./te_processed.csv', './data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_example(ser_example):\n",
    "    # queue = tf.train.string_input_producer([fpath], num_epochs=1)\n",
    "    # _, ser_example = tf.TFRecordReader().read(queue)\n",
    "    # ser_example = tf.train.batch([ser_example], batch_size=10)\n",
    "    ctx_features = {col: tf.FixedLenFeature([], tf_tpe)\n",
    "                    for col, tf_tpe in zip(cols, tf_types) if col not in ('query_movie_ids', 'genres')}\n",
    "    seq_features = {col: tf.FixedLenSequenceFeature([], tf_tpe) \n",
    "                    for col, tf_tpe in [('query_movie_ids', tf.int64), ('genres', tf.int64)]}\n",
    "    context_dict, sequence_dict = tf.parse_single_sequence_example(ser_example, \n",
    "                                                                   context_features=ctx_features, \n",
    "                                                                   sequence_features=seq_features)\n",
    "    # for col, tpe in zip(cols, tf_types):\n",
    "    #     val = feature_dict[col]\n",
    "    #     feature_dict[col] = tf.sparse_to_dense(val.indices, val.dense_shape, val.values, name=col)\n",
    "    feature_dict = {}\n",
    "    feature_dict.update(context_dict)\n",
    "    feature_dict.update(sequence_dict)\n",
    "    ret = OrderedDict()\n",
    "    for c in cols:\n",
    "        ret[c] = feature_dict[c]\n",
    "    return tuple(ret.values())\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.data.TFRecordDataset(['./data.tfrecord'])\n",
    "    dataset = dataset.map(decode_example).padded_batch(10, padded_shapes=([], [None], [None], [], [], [], []))\n",
    "    # dataset = dataset.batch(3)\n",
    "    iters = dataset.make_one_shot_iterator()\n",
    "    r = iters.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        print( sess.run(r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional parse_example\n",
    "1. tf.train.Coordinator + tf.train.start_queue_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import sparse_tensor\n",
    "import re\n",
    "\n",
    "def to_sparse(dense):\n",
    "    idx = tf.where(tf.not_equal(dense, 0))\n",
    "    return tf.SparseTensor(idx, tf.gather_nd(dense, idx), dense.get_shape())\n",
    "\n",
    "def make_example(val):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            'query_movie_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=val)),\n",
    "            'genres': tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n",
    "        }\n",
    "    ))\n",
    "    return example\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    filename = \"tmp.tfrecords\"\n",
    "    if not os.path.exists(filename):\n",
    "        # os.remove(filename)\n",
    "        writer = tf.python_io.TFRecordWriter(filename)\n",
    "        with writer:\n",
    "            for idx, r in teProcessed.head().iterrows():\n",
    "                for col in ('query_movie_ids', 'genres'):\n",
    "                    val = list(map(int, re.split(',\\s*', r[col])))\n",
    "                    ex = make_example(val)\n",
    "                    writer.write(ex.SerializeToString())\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([\"tmp.tfrecords\"], num_epochs=1)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    batch = tf.train.batch(tensors=[serialized_example], batch_size=1)\n",
    "    features = {\n",
    "        'query_movie_ids': tf.VarLenFeature(tf.int64),\n",
    "        'genres': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    data = tf.parse_example(batch, features)\n",
    "    query_movie_ids = data['query_movie_ids']\n",
    "    embbedding = tf.Variable(tf.glorot_uniform_initializer()([9125]), dtype=tf.float32)\n",
    "    print(query_movie_ids.dense_shape)\n",
    "    # r = tf.layers.dense(query_movie_ids, 10)\n",
    "    # emb_query = tf.nn.embedding_lookup_sparse([embbedding], query_movie_ids, None, combiner='sqrtn')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        try:\n",
    "            print(sess.run(data))\n",
    "            pass\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    labels = tf.constant(np.ones([10, 8]))\n",
    "    pred = tf.concat([tf.Variable(tf.ones(shape=[1, 8]), trainable=False), tf.Variable(tf.truncated_normal([9, 8]))], 0)\n",
    "    loss = tf.losses.mean_squared_error(predictions=pred, labels=labels)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(pred.eval())\n",
    "        for i in range(1000):\n",
    "            sess.run([train_op])\n",
    "        print()\n",
    "        print(pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
